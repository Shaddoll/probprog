{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Clustering with LDA and Gaussian LDA\n",
    "*Zijian Chen zc2386*  \n",
    "*Yiran Wang yw3201*  \n",
    "*Columbia University*\n",
    "## 1. Introduction\n",
    "**GOAL**  \n",
    "The goal in modeling a large corpus of discrete documents is to find the latent clustering pattern of those documents as we assume that each document has a probability distribution over certain number of topics, and the words in each documents are drawn from documents' topic distribution. It is hard because we are not sure about the word distribution whether they are discrete or continuous, nor do we have any information about the number of topics in many cases.  \n",
    "**DATA**  \n",
    "To solve topic clustering problem we access [NIPS](https://cs.nyu.edu/~roweis/data.html) corpus data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import edward as ed\n",
    "import glob\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "if '../' not in sys.path:\n",
    "    sys.path.append('../')\n",
    "\n",
    "from models.lda import LDA\n",
    "from models.lda import GaussianLDA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LDA Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Modeling [Blei et al. (2003)](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)\n",
    "LDA assumes the words in every documents are exchangeable, and each of which is a multinomial distribution over vocabularies of size $V$. The likelihood of a corpus $\\mathbf{D}$ contains $D$ documents is given as following:  \n",
    "\\begin{align}\n",
    "\tp(\\mathbf{D}; \\alpha, \\beta)=p(\\theta, \\beta, z, w)&=\\prod_{k=1}^{K} p(\\beta_{k};\\eta)\\prod_{d=1}^{D} p(\\theta_{d} ; \\alpha)(\\prod_{n=1}^{N}p(z_{dn} | \\theta_{d})p(w_{dn} |z_{dn},\\beta_{z_{dn}})\\\\\n",
    "\t&= \\prod_{k=1}^{K} p(\\beta_{k};\\eta)\\prod_{d=1}^{D}p(\\theta_{d} ; \\alpha)(\\prod_{n=1}^{N}\\theta_{z_{dn}}f(w_{dn};\\beta_{z_{dn}}))\n",
    "\\end{align}\n",
    "    \n",
    "**Nodes/Parameter description:** \n",
    "* $\\alpha, \\eta$: dirichlet hyper parameter.  \n",
    "* $\\theta_{d}$: global latent variable, per-doc topic proportions; $\\theta_{d}$ $\\in$ $R^{k}$. $k$ dimensional Dir. random variable $\\theta$ can take values in (k-1)-simplex with $p.d.f$ $p(\\theta | \\alpha)=\\dfrac{\\Gamma(\\sum_{i=1}^{k}\\alpha_{i})}{\\prod_{i=1}^{k}\\Gamma(\\alpha_{i})}\\theta_{1}^{\\alpha_{1}-1} \\cdots \\theta_{k}^{\\alpha_{k}-1} $.\n",
    "* $Z_{dn}$: local latent variables, topic assignment drawn from distribution $\\theta_{d}$, which takes values from {1, 2 .... k}\n",
    "* $\\beta_{z_{dn}}$: global latent variables, topic's distribution over words; per-corpus topic distribution.\n",
    "* $W_{dn}$: observed variables, $(v-1)-$simplex, where $v$ is the vocabulary size. Observed $n^{th}$ word from $d^{th}$ document.\n",
    "\n",
    "As shown in the graph below, the **LDA generative process** is as following:  \n",
    "1. For each topic $k \\in \\{1, 2, ..., K\\}$, draw $\\beta_{k} \\sim Dirichlet(\\eta)$  \n",
    "2. For each document $d \\in \\{1, 2, ...,D\\}$:  \n",
    "    a) Draw $\\theta_{d} \\sim Dirichlet (\\alpha)$  \n",
    "    b) For each word $w_{n}$, $n \\in \\{1, 2, ..., N_{d}\\}$ in document $d$:  \n",
    "    * Draw $z_{dn} \\sim Categorical(\\theta_{d})$.  \n",
    "    * Generate word $w_{dn}\\sim p(w_{dn} | z_{dn}, \\beta_{z_{dn}})$, a categorical probability conditioned on the topic $z_{dn}$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"LDABlei.jpg\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Collapsed Gibbs Sampling Inference for LDA (MCMC) \n",
    "\n",
    "We planned to implement the **Black Box variational inference** method (klqp method) in Edward at the very beginning, however if we include all parameters $\\theta$, $z$, and $\\beta$ for the posterior inference, the result cannot converge. Because klqp in Edward has poor performance at approximating the Dirichlet distribution, especially if we have full-rank covariances for the mixture components. This problem is partially due to the high variance in stochastic optimization if only implementing the vanilla version of BBVI.  \n",
    "In addition, because of the slow convergence of **Gibbs Sampling** inference of LDA, we implemented the **Collapsed Gibbs Sampling** Inference instead [Griffiths et al. (2004).](http://www.pnas.org/content/pnas/101/suppl_1/5228.full.pdf). Since we are only interested in the posterior distribution over the assignment of words in $k$ topics respectively, $p(\\mathbf{z}|\\mathbf{w})$, we can have inexplicit expression of the posterior estimation of $\\theta$ and $\\beta$.   \n",
    "In Monte Carlo Markov Chain, we construct a Markov chain on latent variables which converges to a stationary distribution. This would be used for the estimation of the posterior distribution. In order to draw sample at the time t, we need to construct the complete conditional distribution of latent variables, $p(z_{d,i}|\\mathbf{z}_{d,-i}, \\mathbf{w})$ in this algorithm. Taking the advantage of the conjugate conditional family, the probability distribution can be expressed as: \n",
    "\\begin{align}\n",
    " \tp(z_{d,i}=j|\\mathbf{z}_{d,-i}, \\mathbf{w}) & \\propto \\frac{n_{-i,j}^{(w_{d,i}) }+ \\eta}{n_{-i,j}^{(.)} + V\\eta} \\frac{n_{-i,j}^{(d)}+\\alpha}{n_{-i,.}^{(d)}+k\\alpha}\n",
    "\\end{align}\n",
    "The first fraction express the probability of $w_{d,i}$ under topic $j$, and the second term expresses the probability of topic $j$ document $d_{i}$. The notations are defined as follows:\n",
    "* $n_{-i,j}^{(w_{d,i})}$ is the number of times this $i^{th}$ observed word in document $d$ is assigned to topic $j$ while not including the current assignment $z_{d,i}$.  \n",
    "* $n_{-i,j}^{(.)}$ is the number of times any word is assigned to topic $j$ while not including the current assignment $z_{d,i}$.  \n",
    "* $n_{-i,j}^{(d)}$ is the number of times a word from document $d$ is assigned to topic $j$ while not including the current assignment $z_{d, i}$.\n",
    "* $n_{-i,.}^{(d)}$ is the number words in document $d$ while not including the current assignment $z_{d, i}$.\n",
    "* $V$ is the vocabulary size, and $k$ is the number of topics defined at the very beginning.  \n",
    "\n",
    "With the set of samples drawn after burn-in period, we can evaluate $\\theta$ and $\\beta$ by integrating across the full set of samples as follows:\n",
    " \\begin{align}\n",
    " \t\\hat{\\beta_{j}^{(w)}} &= \\frac{n_{j}^{(w)} + \\eta}{n_{j}^{(.)}+V\\eta}\\\\\n",
    " \t\\hat{\\theta_{j}^{(d)}} &= \\frac{\\theta_{j}^{(d)} + \\alpha}{n_{.}^{(d)}+k\\alpha}\n",
    " \\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Criticism for LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment**  \n",
    "Due to the duration of running time, we run our model of 2-year NIPS documents (year 2012 and year 2011) and for each document, we extracted the abstract part which summarizes the entire document. Furthermore, we did the NLP preprocessing of the input: removed punctuation and the stop words, changed words into lowercase, and stemmed the words with [ntlk](https://www.nltk.org/). We initialize the hyper-parameters $\\alpha = 0.1$ and $\\eta = 0.01$. There are nine major academic subject areas for NIPS: *Algorithm*, *Application*, *Data Competition, Implementation and Software*, *Deep Learning*, *Neuroscience and Cognitive science*, *Optimization*, *Probabilistic Methods*, *Reinforcement Learning & Planning*, and *Theory*, therefore we set $k=9$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of documents, D: 20\n",
      "load../DataPreprocess/nipstxt/nipstoy20/doc_short_wID_12_0024.txtfinished\n",
      "load../DataPreprocess/nipstxt/nipstoy20/doc_short_wID_12_0031.txtfinished\n",
      "load../DataPreprocess/nipstxt/nipstoy20/doc_short_wID_12_0136.txtfinished\n",
      "load../DataPreprocess/nipstxt/nipstoy20/doc_short_wID_12_0122.txtfinished\n",
      "load../DataPreprocess/nipstxt/nipstoy20/doc_short_wID_12_0080.txtfinished\n",
      "load../DataPreprocess/nipstxt/nipstoy20/doc_short_wID_12_0096.txtfinished\n",
      "load../DataPreprocess/nipstxt/nipstoy20/doc_short_wID_12_0108.txtfinished\n",
      "load../DataPreprocess/nipstxt/nipstoy20/doc_short_wID_12_0045.txtfinished\n",
      "load../DataPreprocess/nipstxt/nipstoy20/doc_short_wID_12_0052.txtfinished\n",
      "load../DataPreprocess/nipstxt/nipstoy20/doc_short_wID_12_0103.txtfinished\n",
      "load../DataPreprocess/nipstxt/nipstoy20/doc_short_wID_12_0089.txtfinished\n",
      "load../DataPreprocess/nipstxt/nipstoy20/doc_short_wID_12_0129.txtfinished\n",
      "load../DataPreprocess/nipstxt/nipstoy20/doc_short_wID_12_0115.txtfinished\n",
      "load../DataPreprocess/nipstxt/nipstoy20/doc_short_wID_12_0059.txtfinished\n",
      "load../DataPreprocess/nipstxt/nipstoy20/doc_short_wID_12_0066.txtfinished\n",
      "load../DataPreprocess/nipstxt/nipstoy20/doc_short_wID_12_0073.txtfinished\n",
      "load../DataPreprocess/nipstxt/nipstoy20/doc_short_wID_12_0017.txtfinished\n",
      "load../DataPreprocess/nipstxt/nipstoy20/doc_short_wID_12_0003.txtfinished\n",
      "load../DataPreprocess/nipstxt/nipstoy20/doc_short_wID_12_0010.txtfinished\n",
      "load../DataPreprocess/nipstxt/nipstoy20/doc_short_wID_12_0038.txtfinished\n",
      "vocab size is 632\n",
      "load wordToID finished\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "datafile = \"../DataPreprocess/nipstxt/nipstoy20/doc_short_wID_12_*.txt\"\n",
    "txt_files = glob.glob(datafile)\n",
    "D = len(txt_files)  # number of documents\n",
    "print(\"number of documents, D: {}\".format(D))\n",
    "\n",
    "N = [0] * D  # words per doc\n",
    "K = 9  # number of topics\n",
    "T = 300 # samples drawn in the burnin period\n",
    "S = 200 # samples collected for collapsed gibbs sampling\n",
    "\n",
    "wordIds = [None] * D\n",
    "count = 0  # count number of documents\n",
    "for file in (txt_files):\n",
    "    with open(file, 'rt', encoding=\"ISO-8859-1\") as f:\n",
    "        wordIds[count] = list(map(int, f.readline().split()))\n",
    "        N[count] = len(wordIds[count])\n",
    "        wordIds[count] = np.array(wordIds[count]).astype('int32')\n",
    "    print(\"load\" + file + \"finished\")\n",
    "    count += 1\n",
    "IdtoWord = {}\n",
    "vocab = set()\n",
    "\n",
    "with open(\"../DataPreprocess/wordToID_short_12_20.txt\") as f:\n",
    "    for line in f:\n",
    "        line = line.split()\n",
    "        IdtoWord[int(line[1])] = line[0]\n",
    "        vocab.add(line[0])\n",
    "V = len(vocab)  # vocabulary size21\n",
    "tokens = [None] * V\n",
    "for key in IdtoWord:\n",
    "    tokens[key] = IdtoWord[key]\n",
    "print(\"vocab size is {}\".format(V))\n",
    "print(\"load wordToID finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PMI Evaluation for Top Words**  [Bouma (2009)](https://svn.spraakdata.gu.se/repos/gerlof/pub/www/Docs/npmi-pfd.pdf)  \n",
    "Point-wise mutual information (PMI) is a measure of how much the actual probability of a particular co-occurrence of events $p(x, y)$ differs from what we would expect it to be on the basis of the probabilities of the individual events $p(x)$ and $p(y)$. Also, the calculation based on the independent assumption of $x$ and $y$ is given as:\n",
    "\\begin{align}\n",
    " \tpmi (x, y) = \\ln \\frac{p(x,y)}{p(x)p(y)}\n",
    "\\end{align} \n",
    "The joint probability is estimated by the time we see both words in a 20-length sliding window over the entire corpus. The results are shown here,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model constructed\n",
      "burnin: 0\n",
      "burnin: 1\n",
      "burnin: 2\n",
      "burnin: 3\n",
      "burnin: 4\n",
      "burnin: 5\n",
      "burnin: 6\n",
      "burnin: 7\n",
      "burnin: 8\n",
      "burnin: 9\n",
      "burnin: 10\n",
      "burnin: 11\n",
      "burnin: 12\n",
      "burnin: 13\n",
      "burnin: 14\n",
      "burnin: 15\n",
      "burnin: 16\n",
      "burnin: 17\n",
      "burnin: 18\n",
      "burnin: 19\n",
      "burnin: 20\n",
      "burnin: 21\n",
      "burnin: 22\n",
      "burnin: 23\n",
      "burnin: 24\n",
      "burnin: 25\n",
      "burnin: 26\n",
      "burnin: 27\n",
      "burnin: 28\n",
      "burnin: 29\n",
      "burnin: 30\n",
      "burnin: 31\n",
      "burnin: 32\n",
      "burnin: 33\n",
      "burnin: 34\n",
      "burnin: 35\n",
      "burnin: 36\n",
      "burnin: 37\n",
      "burnin: 38\n",
      "burnin: 39\n",
      "burnin: 40\n",
      "burnin: 41\n",
      "burnin: 42\n",
      "burnin: 43\n",
      "burnin: 44\n",
      "burnin: 45\n",
      "burnin: 46\n",
      "burnin: 47\n",
      "burnin: 48\n",
      "burnin: 49\n",
      "burnin: 50\n",
      "burnin: 51\n",
      "burnin: 52\n",
      "burnin: 53\n",
      "burnin: 54\n",
      "burnin: 55\n",
      "burnin: 56\n",
      "burnin: 57\n",
      "burnin: 58\n",
      "burnin: 59\n",
      "burnin: 60\n",
      "burnin: 61\n",
      "burnin: 62\n",
      "burnin: 63\n",
      "burnin: 64\n",
      "burnin: 65\n",
      "burnin: 66\n",
      "burnin: 67\n",
      "burnin: 68\n",
      "burnin: 69\n",
      "burnin: 70\n",
      "burnin: 71\n",
      "burnin: 72\n",
      "burnin: 73\n",
      "burnin: 74\n",
      "burnin: 75\n",
      "burnin: 76\n",
      "burnin: 77\n",
      "burnin: 78\n",
      "burnin: 79\n",
      "burnin: 80\n",
      "burnin: 81\n",
      "burnin: 82\n",
      "burnin: 83\n",
      "burnin: 84\n",
      "burnin: 85\n",
      "burnin: 86\n",
      "burnin: 87\n",
      "burnin: 88\n",
      "burnin: 89\n",
      "burnin: 90\n",
      "burnin: 91\n",
      "burnin: 92\n",
      "burnin: 93\n",
      "burnin: 94\n",
      "burnin: 95\n",
      "burnin: 96\n",
      "burnin: 97\n",
      "burnin: 98\n",
      "burnin: 99\n",
      "burnin: 100\n",
      "burnin: 101\n",
      "burnin: 102\n",
      "burnin: 103\n",
      "burnin: 104\n",
      "burnin: 105\n",
      "burnin: 106\n",
      "burnin: 107\n",
      "burnin: 108\n",
      "burnin: 109\n",
      "burnin: 110\n",
      "burnin: 111\n",
      "burnin: 112\n",
      "burnin: 113\n",
      "burnin: 114\n",
      "burnin: 115\n",
      "burnin: 116\n",
      "burnin: 117\n",
      "burnin: 118\n",
      "burnin: 119\n",
      "burnin: 120\n",
      "burnin: 121\n",
      "burnin: 122\n",
      "burnin: 123\n",
      "burnin: 124\n",
      "burnin: 125\n",
      "burnin: 126\n",
      "burnin: 127\n",
      "burnin: 128\n",
      "burnin: 129\n",
      "burnin: 130\n",
      "burnin: 131\n",
      "burnin: 132\n",
      "burnin: 133\n",
      "burnin: 134\n",
      "burnin: 135\n",
      "burnin: 136\n",
      "burnin: 137\n",
      "burnin: 138\n",
      "burnin: 139\n",
      "burnin: 140\n",
      "burnin: 141\n",
      "burnin: 142\n",
      "burnin: 143\n",
      "burnin: 144\n",
      "burnin: 145\n",
      "burnin: 146\n",
      "burnin: 147\n",
      "burnin: 148\n",
      "burnin: 149\n",
      "burnin: 150\n",
      "burnin: 151\n",
      "burnin: 152\n",
      "burnin: 153\n",
      "burnin: 154\n",
      "burnin: 155\n",
      "burnin: 156\n",
      "burnin: 157\n",
      "burnin: 158\n",
      "burnin: 159\n",
      "burnin: 160\n",
      "burnin: 161\n",
      "burnin: 162\n",
      "burnin: 163\n",
      "burnin: 164\n",
      "burnin: 165\n",
      "burnin: 166\n",
      "burnin: 167\n",
      "burnin: 168\n",
      "burnin: 169\n",
      "burnin: 170\n",
      "burnin: 171\n",
      "burnin: 172\n",
      "burnin: 173\n",
      "burnin: 174\n",
      "burnin: 175\n",
      "burnin: 176\n",
      "burnin: 177\n",
      "burnin: 178\n",
      "burnin: 179\n",
      "burnin: 180\n",
      "burnin: 181\n",
      "burnin: 182\n",
      "burnin: 183\n",
      "burnin: 184\n",
      "burnin: 185\n",
      "burnin: 186\n",
      "burnin: 187\n",
      "burnin: 188\n",
      "burnin: 189\n",
      "burnin: 190\n",
      "burnin: 191\n",
      "burnin: 192\n",
      "burnin: 193\n",
      "burnin: 194\n",
      "burnin: 195\n",
      "burnin: 196\n",
      "burnin: 197\n",
      "burnin: 198\n",
      "burnin: 199\n",
      "burnin: 200\n",
      "burnin: 201\n",
      "burnin: 202\n",
      "burnin: 203\n",
      "burnin: 204\n",
      "burnin: 205\n",
      "burnin: 206\n",
      "burnin: 207\n",
      "burnin: 208\n",
      "burnin: 209\n",
      "burnin: 210\n",
      "burnin: 211\n",
      "burnin: 212\n",
      "burnin: 213\n",
      "burnin: 214\n",
      "burnin: 215\n",
      "burnin: 216\n",
      "burnin: 217\n",
      "burnin: 218\n",
      "burnin: 219\n",
      "burnin: 220\n",
      "burnin: 221\n",
      "burnin: 222\n",
      "burnin: 223\n",
      "burnin: 224\n",
      "burnin: 225\n",
      "burnin: 226\n",
      "burnin: 227\n",
      "burnin: 228\n",
      "burnin: 229\n",
      "burnin: 230\n",
      "burnin: 231\n",
      "burnin: 232\n",
      "burnin: 233\n",
      "burnin: 234\n",
      "burnin: 235\n",
      "burnin: 236\n",
      "burnin: 237\n",
      "burnin: 238\n",
      "burnin: 239\n",
      "burnin: 240\n",
      "burnin: 241\n",
      "burnin: 242\n",
      "burnin: 243\n",
      "burnin: 244\n",
      "burnin: 245\n",
      "burnin: 246\n",
      "burnin: 247\n",
      "burnin: 248\n",
      "burnin: 249\n",
      "burnin: 250\n",
      "burnin: 251\n",
      "burnin: 252\n",
      "burnin: 253\n",
      "burnin: 254\n",
      "burnin: 255\n",
      "burnin: 256\n",
      "burnin: 257\n",
      "burnin: 258\n",
      "burnin: 259\n",
      "burnin: 260\n",
      "burnin: 261\n",
      "burnin: 262\n",
      "burnin: 263\n",
      "burnin: 264\n",
      "burnin: 265\n",
      "burnin: 266\n",
      "burnin: 267\n",
      "burnin: 268\n",
      "burnin: 269\n",
      "burnin: 270\n",
      "burnin: 271\n",
      "burnin: 272\n",
      "burnin: 273\n",
      "burnin: 274\n",
      "burnin: 275\n",
      "burnin: 276\n",
      "burnin: 277\n",
      "burnin: 278\n",
      "burnin: 279\n",
      "burnin: 280\n",
      "burnin: 281\n",
      "burnin: 282\n",
      "burnin: 283\n",
      "burnin: 284\n",
      "burnin: 285\n",
      "burnin: 286\n",
      "burnin: 287\n",
      "burnin: 288\n",
      "burnin: 289\n",
      "burnin: 290\n",
      "burnin: 291\n",
      "burnin: 292\n",
      "burnin: 293\n",
      "burnin: 294\n",
      "burnin: 295\n",
      "burnin: 296\n",
      "burnin: 297\n",
      "burnin: 298\n",
      "burnin: 299\n",
      "sample: 0\n",
      "sample: 1\n",
      "sample: 2\n",
      "sample: 3\n",
      "sample: 4\n",
      "sample: 5\n",
      "sample: 6\n",
      "sample: 7\n",
      "sample: 8\n",
      "sample: 9\n",
      "sample: 10\n",
      "sample: 11\n",
      "sample: 12\n",
      "sample: 13\n",
      "sample: 14\n",
      "sample: 15\n",
      "sample: 16\n",
      "sample: 17\n",
      "sample: 18\n",
      "sample: 19\n",
      "sample: 20\n",
      "sample: 21\n",
      "sample: 22\n",
      "sample: 23\n",
      "sample: 24\n",
      "sample: 25\n",
      "sample: 26\n",
      "sample: 27\n",
      "sample: 28\n",
      "sample: 29\n",
      "sample: 30\n",
      "sample: 31\n",
      "sample: 32\n",
      "sample: 33\n",
      "sample: 34\n",
      "sample: 35\n",
      "sample: 36\n",
      "sample: 37\n",
      "sample: 38\n",
      "sample: 39\n",
      "sample: 40\n",
      "sample: 41\n",
      "sample: 42\n",
      "sample: 43\n",
      "sample: 44\n",
      "sample: 45\n",
      "sample: 46\n",
      "sample: 47\n",
      "sample: 48\n",
      "sample: 49\n",
      "sample: 50\n",
      "sample: 51\n",
      "sample: 52\n",
      "sample: 53\n",
      "sample: 54\n",
      "sample: 55\n",
      "sample: 56\n",
      "sample: 57\n",
      "sample: 58\n",
      "sample: 59\n",
      "sample: 60\n",
      "sample: 61\n",
      "sample: 62\n",
      "sample: 63\n",
      "sample: 64\n",
      "sample: 65\n",
      "sample: 66\n",
      "sample: 67\n",
      "sample: 68\n",
      "sample: 69\n",
      "sample: 70\n",
      "sample: 71\n",
      "sample: 72\n",
      "sample: 73\n",
      "sample: 74\n",
      "sample: 75\n",
      "sample: 76\n",
      "sample: 77\n",
      "sample: 78\n",
      "sample: 79\n",
      "sample: 80\n",
      "sample: 81\n",
      "sample: 82\n",
      "sample: 83\n",
      "sample: 84\n",
      "sample: 85\n",
      "sample: 86\n",
      "sample: 87\n",
      "sample: 88\n",
      "sample: 89\n",
      "sample: 90\n",
      "sample: 91\n",
      "sample: 92\n",
      "sample: 93\n",
      "sample: 94\n",
      "sample: 95\n",
      "sample: 96\n",
      "sample: 97\n",
      "sample: 98\n",
      "sample: 99\n",
      "sample: 100\n",
      "sample: 101\n",
      "sample: 102\n",
      "sample: 103\n",
      "sample: 104\n",
      "sample: 105\n",
      "sample: 106\n",
      "sample: 107\n",
      "sample: 108\n",
      "sample: 109\n",
      "sample: 110\n",
      "sample: 111\n",
      "sample: 112\n",
      "sample: 113\n",
      "sample: 114\n",
      "sample: 115\n",
      "sample: 116\n",
      "sample: 117\n",
      "sample: 118\n",
      "sample: 119\n",
      "sample: 120\n",
      "sample: 121\n",
      "sample: 122\n",
      "sample: 123\n",
      "sample: 124\n",
      "sample: 125\n",
      "sample: 126\n",
      "sample: 127\n",
      "sample: 128\n",
      "sample: 129\n",
      "sample: 130\n",
      "sample: 131\n",
      "sample: 132\n",
      "sample: 133\n",
      "sample: 134\n",
      "sample: 135\n",
      "sample: 136\n",
      "sample: 137\n",
      "sample: 138\n",
      "sample: 139\n",
      "sample: 140\n",
      "sample: 141\n",
      "sample: 142\n",
      "sample: 143\n",
      "sample: 144\n",
      "sample: 145\n",
      "sample: 146\n",
      "sample: 147\n",
      "sample: 148\n",
      "sample: 149\n",
      "sample: 150\n",
      "sample: 151\n",
      "sample: 152\n",
      "sample: 153\n",
      "sample: 154\n",
      "sample: 155\n",
      "sample: 156\n",
      "sample: 157\n",
      "sample: 158\n",
      "sample: 159\n",
      "sample: 160\n",
      "sample: 161\n",
      "sample: 162\n",
      "sample: 163\n",
      "sample: 164\n",
      "sample: 165\n",
      "sample: 166\n",
      "sample: 167\n",
      "sample: 168\n",
      "sample: 169\n",
      "sample: 170\n",
      "sample: 171\n",
      "sample: 172\n",
      "sample: 173\n",
      "sample: 174\n",
      "sample: 175\n",
      "sample: 176\n",
      "sample: 177\n",
      "sample: 178\n",
      "sample: 179\n",
      "sample: 180\n",
      "sample: 181\n",
      "sample: 182\n",
      "sample: 183\n",
      "sample: 184\n",
      "sample: 185\n",
      "sample: 186\n",
      "sample: 187\n",
      "sample: 188\n",
      "sample: 189\n",
      "sample: 190\n",
      "sample: 191\n",
      "sample: 192\n",
      "sample: 193\n",
      "sample: 194\n",
      "sample: 195\n",
      "sample: 196\n",
      "sample: 197\n",
      "sample: 198\n",
      "sample: 199\n",
      "inference finished\n"
     ]
    }
   ],
   "source": [
    "model = LDA(K, V, D, N)\n",
    "print(\"model constructed\")\n",
    "model.collapsed(wordIds, S, T, tokens)\n",
    "print(\"inference finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "707.7561919689178\n",
      "topic 0\n",
      "network 0.12638986\n",
      "model 0.0884915\n",
      "attractor 0.05059373\n",
      "select 0.03164481\n",
      "output 0.02532846\n",
      "associ 0.02532846\n",
      "signal 0.02532846\n",
      "experi 0.02532846\n",
      "law 0.01901216\n",
      "discuss 0.01901216\n",
      "topic 1\n",
      "gener 0.05228643\n",
      "process 0.04648335\n",
      "find 0.034876928\n",
      "recognit 0.029073752\n",
      "complex 0.029073752\n",
      "experiment 0.029073752\n",
      "base 0.023270622\n",
      "observ 0.023270622\n",
      "imag 0.023270622\n",
      "suggest 0.023270622\n",
      "topic 2\n",
      "languag 0.094329216\n",
      "acquisit 0.047203988\n",
      "task 0.047203988\n",
      "boundari 0.031495456\n",
      "form 0.031495456\n",
      "recurr 0.031495456\n",
      "classif 0.02364116\n",
      "coeffici 0.02364116\n",
      "object 0.02364116\n",
      "exampl 0.02364116\n",
      "topic 3\n",
      "effect 0.058758754\n",
      "respons 0.058758754\n",
      "also 0.044087414\n",
      "stimulu 0.03675175\n",
      "channel 0.029416082\n",
      "differenti 0.029416082\n",
      "given 0.022080416\n",
      "compar 0.022080416\n",
      "formul 0.022080416\n",
      "ground 0.022080416\n",
      "topic 4\n",
      "show 0.094742015\n",
      "result 0.072895296\n",
      "figur 0.072895296\n",
      "ep 0.04376643\n",
      "propos 0.029201943\n",
      "stop 0.029201943\n",
      "simul 0.021919563\n",
      "higher 0.021919563\n",
      "recent 0.021919563\n",
      "evolv 0.021919563\n",
      "topic 5\n",
      "neuron 0.13050124\n",
      "learn 0.09647239\n",
      "synapt 0.04542881\n",
      "connect 0.04542881\n",
      "paper 0.034085754\n",
      "may 0.034085754\n",
      "level 0.028414248\n",
      "input 0.028414248\n",
      "order 0.028414248\n",
      "dendrit 0.017071245\n",
      "topic 6\n",
      "inform 0.06464293\n",
      "spatial 0.05877178\n",
      "system 0.041157752\n",
      "rule 0.041157752\n",
      "cell 0.035286456\n",
      "comput 0.035286456\n",
      "hebbian 0.035286456\n",
      "spike 0.035286456\n",
      "layer 0.023543876\n",
      "coupl 0.023543876\n",
      "topic 7\n",
      "dynam 0.06935977\n",
      "attent 0.06935977\n",
      "visual 0.055501677\n",
      "distribut 0.04857256\n",
      "two 0.041643597\n",
      "synaps 0.034714587\n",
      "node 0.034714587\n",
      "group 0.027785439\n",
      "modul 0.027785439\n",
      "field 0.027785439\n",
      "topic 8\n",
      "abstract 0.10947472\n",
      "use 0.050200537\n",
      "neural 0.036521938\n",
      "differ 0.031962402\n",
      "tempor 0.02740288\n",
      "light 0.02740288\n",
      "provid 0.022843355\n",
      "behavior 0.022843355\n",
      "environ 0.022843355\n",
      "similar 0.022843355\n",
      "83.0\n",
      "topic 0 pmi: 0.071770\n",
      "90.0\n",
      "topic 1 pmi: 0.080255\n",
      "65.0\n",
      "topic 2 pmi: 0.308083\n",
      "75.0\n",
      "topic 3 pmi: 0.425953\n",
      "69.0\n",
      "topic 4 pmi: 0.656589\n",
      "89.0\n",
      "topic 5 pmi: 0.310589\n",
      "82.0\n",
      "topic 6 pmi: 0.163440\n",
      "78.0\n",
      "topic 7 pmi: 0.153564\n",
      "82.0\n",
      "topic 8 pmi: 0.099890\n"
     ]
    }
   ],
   "source": [
    "print(time.time() - t1)\n",
    "model.getTopWords(tokens)\n",
    "comatrix = pickle.load(open(\"../20abstract1yearAll.pickle\", \"rb\"))\n",
    "comatrix += 1\n",
    "model.getPMI(comatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our results: ###\n",
    "<img src=\"LDA2y_results.jpg\" width=\"950\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gaussian LDA Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Modeling  [Das et al. (2015)](http://www.aclweb.org/anthology/P15-1077) \n",
    "Instead of generating words from categorical distribution, Gaussian LDA integrates the word embedding that aims at capturing more semantic regularities in language and generates words from multivariate Gaussian distributions. The observed words input are pre-trained word2vec embedding vectors with fixed dimensions [gensim](https://radimrehurek.com/gensim/models/word2vec.html). Our model used 25 dimensional word embeddings instead of the pre-defined 100 dimension due to the feasibility and running time while implementing on Edward.  \n",
    "The likelihood of a corpus $\\mathbf{D}$ contains $D$ documents is given as following:\n",
    "\\begin{align}\n",
    "p(\\mathbf{D}; \\alpha, \\beta)=\\prod_{k=1}^{K} p(\\Sigma_{k}|\\Psi, v)p(\\mu_{k}|\\mu_{0}, \\Sigma_{k})\\prod_{d=1}^{D} p(\\theta_{d} | \\alpha)(\\prod_{n=1}^{N}p(z_{dn} | \\theta_{d})p(w_{dn} |z_{dn},\\Sigma_{z_{dn}},\\mu_{z_{dn}}))\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nodes Decription**  \n",
    "* $ \\psi, v$: Inverse Wishart distribution hyper parameter\t\n",
    "* $\\mu_{0}$: Gaussian distribution hyper parameter\t\n",
    "* $\\Sigma_{z_{dn}}$: global latent variable, per-topic covariance matrix for Multivariate Gaussian distribution of words.\t\n",
    "* $\\mu_{z_{dn}}$: global latent variable, per-topic mean vector for Multivariate Gaussian distribution of words.  \n",
    "\n",
    "**Gaussian-LDA generative process:** \n",
    "1. For each topic $k \\in \\{1, 2, ..., K\\}$:  \n",
    "    a) Draw topic covariance matrix $\\Sigma_{k} \\sim W(\\Psi, v)$  \n",
    "    b) Draw topic mean $\\mu_{k} \\sim N(\\mu_{0}, I_{k})$  \n",
    "2. For each document $d \\in \\{1, 2, ...,D\\}$ in corpus:  \n",
    "    a) Draw topic distribution $\\theta_{d} \\sim Dirichlet (\\alpha)$  \n",
    "    b) For each word $w_{n}$, $n \\in \\{1, 2, ..., N_{d}\\}$ in document $d$:    \n",
    "    * Draw $z_{dn} \\sim Categorical(\\theta_{d})$.  \n",
    "    * Draw word $w_{dn}\\sim N(\\mu_{z_{dn}}, \\Sigma_{z_{dn}})$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"GaussianLDA.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Black Box Variational Inference for Gaussian LDA  \n",
    "Paper [Das et al. (2015)](http://www.aclweb.org/anthology/P15-1077) provides the Collapsed Gibbs Sampling posterior inference method for Gaussian LDA, however Collapsed Gibbs Sampling does not work with mixture random variables in Edward. With the blessing of the conjugate priors for topic proportions, we can integrate them out during the inference as we got rid of the Dirichlet distribution in klqp implementation in Edward. The variational distribution for $\\mu$ and $\\Sigma$ are that same distribution as the prior while taking the independence assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of documents, D: 150\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0526.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0724.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0687.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0136.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0122.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0491.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0321.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0335.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0484.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0645.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0862.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0731.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0080.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0533.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0519.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0096.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0928.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0914.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0900.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0848.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_1029.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_1001.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_1015.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0108.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0652.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0694.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0680.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0045.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0286.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0279.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0251.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0442.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0456.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0052.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0907.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0244.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0293.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0223.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0237.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0747.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0342.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0356.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0949.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0977.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0963.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0803.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0624.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0157.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0631.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0143.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0209.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0547.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0796.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0782.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0386.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0379.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0393.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0185.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0813.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0754.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0768.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0230.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0568.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0540.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0554.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0024.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0178.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0150.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_1064.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0421.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0435.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0192.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0031.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0970.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0596.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0582.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0216.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0363.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0820.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0834.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0956.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0942.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0017.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0003.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0407.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_1043.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0349.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_1057.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0638.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0610.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0994.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0589.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0199.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0827.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0370.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0761.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0775.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0561.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0575.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0987.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0171.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0617.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0603.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0400.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0414.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0428.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_1050.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0164.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0010.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0038.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0789.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0103.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0659.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0498.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_1022.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_1036.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0328.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0300.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0314.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0738.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0710.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0089.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0512.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0935.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0921.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0869.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0841.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0855.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0666.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_1008.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0470.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0129.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0673.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0115.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0505.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0059.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0717.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0703.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0893.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0449.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0307.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0886.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0879.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0272.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0258.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0066.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0463.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0477.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0073.txtfinished\n",
      "load../DataPreprocess/nips12we25/short_wordembed_0265.txtfinished\n",
      "dimension: 25\n",
      "vocab size is 632\n",
      "load wordId finished\n",
      "load word embeddings finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model constructed\n",
      "2500/2500 [100%] ██████████████████████████████ Elapsed: 848s | Loss: 41379.480\n",
      "inference finished\n"
     ]
    }
   ],
   "source": [
    "t2 = time.time()\n",
    "datafile = \"../DataPreprocess/nips12we25/short_wordembed_*.txt\"\n",
    "txt_files = glob.glob(datafile)\n",
    "D = len(txt_files)  # number of documents\n",
    "print(\"number of documents, D: {}\".format(D))\n",
    "N = [0] * D  # words per doc\n",
    "K = 9  # number of topics\n",
    "T = 2500 # number of iteration\n",
    "S = 5 # # of sample \n",
    "wordIds = [None] * D\n",
    "count = 0  # count number of documents\n",
    "for file in (txt_files):\n",
    "    with open(file, 'rt', encoding=\"ISO-8859-1\") as f:\n",
    "        vec = []\n",
    "        for line in f:\n",
    "            vec.append(list(map(float, line.split())))\n",
    "        N[count] = len(vec)\n",
    "        wordIds[count] = vec\n",
    "    print(\"load\" + file + \"finished\")\n",
    "    count += 1\n",
    "nu = len(wordIds[0][0])\n",
    "print(\"dimension:\", nu)\n",
    "\n",
    "wordToId = dict()\n",
    "tokens = []\n",
    "# with open(\"DataPreprocess/wordToID_short_12.txt\") as f:\n",
    "with open(\"../DataPreprocess/wordToID_short_12_20.txt\") as f:\n",
    "    for line in f:\n",
    "        line = line.split()\n",
    "        wordToId[line[0]] = len(tokens)\n",
    "        tokens.append(line[0])\n",
    "V = len(tokens)  # vocabulary size21\n",
    "print(\"vocab size is {}\".format(V))\n",
    "print(\"load wordId finished\")\n",
    "\n",
    "wordVec = [None] * V\n",
    "# with open(\"DataPreprocess/word_vectors_25.txt\") as f:\n",
    "with open(\"../DataPreprocess/we_12_20.txt\") as f:\n",
    "    for line in f:\n",
    "        line = line.split()\n",
    "        if line[0] in wordToId:\n",
    "            wordVec[wordToId[line[0]]] = list(map(float, line[1:]))\n",
    "print(\"load word embeddings finished\")\n",
    "D = 20\n",
    "\n",
    "model = GaussianLDA(K, D, N, nu)\n",
    "print(\"model constructed\")\n",
    "model.klqp(wordIds, S, T, wordVec)\n",
    "print(\"inference finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Criticism for Gaussian LDA \n",
    "The top words for each topic and corresponding pmi is shown in the following chart. We can see improvement in pmi score as the average pmi changes from 0.203 to 0.600 with more topic interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1267.902379989624\n",
      "topic 0\n",
      "agreement -40.500427\n",
      "fundament -40.536804\n",
      "diffus -40.77791\n",
      "sequenti -40.988068\n",
      "green -41.114006\n",
      "manner -41.33602\n",
      "event -42.39844\n",
      "perceptu -42.421795\n",
      "carri -42.865368\n",
      "excit -42.986515\n",
      "usual -43.21483\n",
      "neither -43.373848\n",
      "psychophys -43.553295\n",
      "come -43.810966\n",
      "establish -43.97314\n",
      "topic 1\n",
      "recent -32.83008\n",
      "first -32.936684\n",
      "demonstr -32.970604\n",
      "procedur -33.01648\n",
      "simpl -33.033684\n",
      "develop -33.06268\n",
      "framework -33.092064\n",
      "simul -33.116978\n",
      "experiment -33.139137\n",
      "context -33.273876\n",
      "provid -33.274765\n",
      "includ -33.295822\n",
      "similar -33.317406\n",
      "theori -33.326797\n",
      "well -33.344704\n",
      "topic 2\n",
      "encod -46.534046\n",
      "global -46.546097\n",
      "locat -46.570942\n",
      "axi -46.5823\n",
      "certain -46.58876\n",
      "strength -46.596672\n",
      "overlap -46.605545\n",
      "top -46.610847\n",
      "sensor -46.64315\n",
      "phase -46.643295\n",
      "except -46.64789\n",
      "region -46.6502\n",
      "synchroni -46.66201\n",
      "boundari -46.67524\n",
      "synapt -46.677597\n",
      "topic 3\n",
      "primari -9.362706\n",
      "distinct -9.379214\n",
      "inter -9.392682\n",
      "intermedi -9.393285\n",
      "agre -9.398555\n",
      "subpopul -9.401191\n",
      "inconsist -9.415439\n",
      "bias -9.458671\n",
      "ignor -9.491414\n",
      "occur -9.503625\n",
      "insight -9.513028\n",
      "concern -9.515548\n",
      "safe -9.522283\n",
      "fractal -9.544412\n",
      "feasibl -9.559073\n",
      "topic 4\n",
      "creat -11.936412\n",
      "initi -11.957513\n",
      "good -12.197694\n",
      "reliabl -12.257327\n",
      "certain -12.262126\n",
      "success -12.264739\n",
      "accur -12.337401\n",
      "behavior -12.370936\n",
      "account -12.531049\n",
      "carri -12.531278\n",
      "incorpor -12.585347\n",
      "substanti -12.649184\n",
      "gain -12.675473\n",
      "field -12.676688\n",
      "choic -12.696483\n",
      "topic 5\n",
      "initi -24.031729\n",
      "associ -24.279465\n",
      "introduc -24.292747\n",
      "basi -24.318186\n",
      "accord -24.43459\n",
      "continu -24.4881\n",
      "addit -24.585564\n",
      "use -24.590855\n",
      "attractor -24.640583\n",
      "tune -24.734232\n",
      "whose -24.790495\n",
      "posit -24.798939\n",
      "gener -24.801132\n",
      "correspond -24.878136\n",
      "determin -24.894054\n",
      "topic 6\n",
      "rel -27.653486\n",
      "mode -27.808111\n",
      "accuraci -27.962265\n",
      "empir -28.213797\n",
      "fast -28.22161\n",
      "neighbor -28.79017\n",
      "net -28.825336\n",
      "approxim -29.23562\n",
      "signific -29.254791\n",
      "expert -29.374928\n",
      "bay -29.378696\n",
      "final -29.482035\n",
      "theoret -29.551981\n",
      "hard -29.707533\n",
      "accur -30.252272\n",
      "topic 7\n",
      "bandwidth -11.944619\n",
      "anatom -11.9543295\n",
      "often -11.978744\n",
      "safe -11.981753\n",
      "argument -11.9840555\n",
      "reliabl -11.9840975\n",
      "environment -11.984099\n",
      "neurosci -11.990916\n",
      "quickli -11.991666\n",
      "learnabl -11.997514\n",
      "success -12.003423\n",
      "maintain -12.003896\n",
      "purpos -12.005591\n",
      "intra -12.010711\n",
      "influenc -12.014045\n",
      "topic 8\n",
      "exist -33.707695\n",
      "mode -33.74877\n",
      "continu -33.7816\n",
      "coeffici -33.844414\n",
      "variat -33.90129\n",
      "case -34.033566\n",
      "bayesian -34.18934\n",
      "updat -34.361626\n",
      "field -34.430393\n",
      "associ -34.4309\n",
      "rule -34.482407\n",
      "imag -34.501026\n",
      "consist -34.67191\n",
      "independ -34.817345\n",
      "appli -35.38096\n",
      "get top words finished\n",
      "1267.9343039989471\n",
      "210.0\n",
      "topic 0 pmi: 1.447072\n",
      "210.0\n",
      "topic 1 pmi: 0.201143\n",
      "210.0\n",
      "topic 2 pmi: 0.572723\n",
      "210.0\n",
      "topic 3 pmi: 1.910330\n",
      "210.0\n",
      "topic 4 pmi: 0.339274\n",
      "210.0\n",
      "topic 5 pmi: 0.022748\n",
      "210.0\n",
      "topic 6 pmi: 0.197845\n",
      "210.0\n",
      "topic 7 pmi: 1.397331\n",
      "210.0\n",
      "topic 8 pmi: -0.283791\n",
      "1268.0404660701752\n"
     ]
    }
   ],
   "source": [
    "print(time.time() - t2)\n",
    "model.getTopWords(wordVec, tokens)\n",
    "print(\"get top words finished\")\n",
    "print(time.time() - t2)\n",
    "comatrix = pickle.load(open(\"../20abstract1yearAll.pickle\", \"rb\"))\n",
    "# comatrix = pickle.load(open(\"DataPreprocess/comatrix1y.pickle\", \"rb\"))\n",
    "comatrix += 1 # normalize\n",
    "model.getPMI(comatrix)\n",
    "print(time.time() - t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our results: ####\n",
    "<img src=\"GLDA50v3.jpg\" width=\"950\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *References*\n",
    "* [Blei, D. M., Ng, A. Y., and Jordan, M. I. (2003).](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf) Latent dirichlet allocation. Journal of machine Learning research, 3(Jan):993–1022. \n",
    "* [Bouma, G. (2009).](https://svn.spraakdata.gu.se/repos/gerlof/pub/www/Docs/npmi-pfd.pdf) Normalized (pointwise) mutual information in collocation extraction. Proceedings of GSCL, 31-40.\n",
    "* [Das, R., Zaheer, M., and Dyer, C. (2015).](http://www.aclweb.org/anthology/P15-1077) Gaussian lda for topic models with word embeddings. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), volume 1, pages 795–804.\n",
    "* [Griffiths, T. L., & Steyvers, M. (2004).](http://www.pnas.org/content/pnas/101/suppl_1/5228.full.pdf) Finding scientific topics. Proceedings of the National academy of Sciences, 101(suppl 1), 5228-5235.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probrequire",
   "language": "python",
   "name": "pprequire"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

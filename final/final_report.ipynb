{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Clustering with LDA and Gaussian LDA\n",
    "*Zijian Chen zc2386*  \n",
    "*Yiran Wang yw3201*  \n",
    "*Columbia University*\n",
    "## 1. Introduction\n",
    "**GOAL**  \n",
    "The goal in modeling a large corpus of discrete documents is to find the latent clustering pattern of those documents as we assume that each document has a probability distribution over certain number of topics, and the words in each documents are drawn from documentâ€™s topic distribution. It is hard because we are not sure about the word distribution whether they are discrete or continuous, nor do we have any information about the number of topics in many cases.  \n",
    "**DATA**  \n",
    "To solve topic clustering problem we access NIPS corpus data NIPS12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import edward as ed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LDA Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Modeling\n",
    "LDA assumes the words in every documents are exchangeable, and each of which is a multinomial distribution over vocabularies of size $V$. The likelihood of a corpus $\\mathbf{D}$ contains $D$ documents is given as following:  \n",
    "\\begin{align}\n",
    "\tp(\\mathbf{D}; \\alpha, \\beta)=p(\\theta, \\beta, z, w)&=\\prod_{k=1}^{K} p(\\beta_{k};\\eta)\\prod_{d=1}^{D} p(\\theta_{d} ; \\alpha)(\\prod_{n=1}^{N}p(z_{dn} | \\theta_{d})p(w_{dn} |z_{dn},\\beta_{z_{dn}})\\\\\n",
    "\t&= \\prod_{k=1}^{K} p(\\beta_{k};\\eta)\\prod_{d=1}^{D}p(\\theta_{d} ; \\alpha)(\\prod_{n=1}^{N}\\theta_{z_{dn}}f(w_{dn};b_{z_{dn}}))\n",
    "\\end{align}\n",
    "\n",
    "**LDA generative process: ** \n",
    "1. For each topic $k \\in \\{1, 2, ..., K\\}$, draw $\\beta_{k} \\sim Dirichlet(\\eta)$  \n",
    "2. For each document $d \\in \\{1, 2, ...,D\\}$:  \n",
    "    a) Draw $\\theta_{d} \\sim Dirichlet (\\alpha)$  \n",
    "    b) For each word $w_{n}$, $n \\in \\{1, 2, ..., N_{d}\\}$ in document $d$:  \n",
    "    * Draw $z_{dn} \\sim Categorical(\\theta_{d})$.  \n",
    "    * Generaste word $w_{dn}\\sim p(w_{dn} | z_{dn}, \\beta_{z_{dn}})$, a categorical probability conditioned on the topic $z_{dn}$.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"LDABlei.jpg\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Collapsed Gibbs Sampling Inference for LDA  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Criticism for LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gaussian LDA Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Black Box Variational Inference Guassian LDA  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probrequire",
   "language": "python",
   "name": "pprequire"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

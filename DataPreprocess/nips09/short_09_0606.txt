abstract present algorithm fast stochast gradient descent use nonlinear adapt momentum scheme optim late time converg rate algorithm make effect use curvatur inform requir storag comput deliv converg rate close theoret optimum demonstr techniqu linear larg nonlinear backprop network improv stochast search learn algorithm perform gradient descent cost function formul either stochast line batch form stochast version take form cot current weight estim pt learn rate minu instantan gradient estim xt input time one obtain correspond batch mode learn rule take constant averag stochast learn provid sever advantag batch learn larg dataset batch averag expens comput stochast learn elimin averag stochast updat regard noisi estim batch updat intrins nois reduc likelihood becom trap poor local optima
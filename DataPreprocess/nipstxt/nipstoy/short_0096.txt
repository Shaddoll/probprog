abstract paper revisits classical neuroscience paradigm hebbian learning find necessary requirement effective associative memory learning efcacies incoming synapses uncorrelated requirement difficult achieve robust manner hebbian synaptic learning since depends network level information effective learning yet obtained neuronal process maintains zero sum incoming synaptic efcacies normalization drastically improves memory capacity associative networks essentially bounded capacity one linearly scales network size also enables effective storage patterns heterogeneous coding levels single network neuronal normalization successfully carried activity dependent homeostasis neuron synaptic efcacies recently observed cortical tissue thus findings strongly suggest effective associative learning hebbian synapses alone biologically implausible hebbian synapses must continuously remodeled neuronally driven regulatory processes brain introduction synapse specific changes synaptic efcacies carried long term potentiation ltp depression ltd thought underlie cortical self organization learning brain accordance hebbian paradigm ltp ltd modify synaptic efcacies function firing pre post synaptic neurons paper revisits hebbian paradigm showing synaptic learning alone cannot provide effective associative learning biologically plausible manner must complemented neuronally driven synaptic remodeling importance neuronally driven normalization processes already demonstrated context self organization cortical maps continuous unsupervised learning principal component analysis networks scenarios normalization necessary prevent excessive growth synapeffective learning requires neuronal remodeling hebbian synapses tic efficacies occurs learning neuronal activity strongly coupled contradistinction paper focuses associative memory learning excessive synaptic runaway growth mild shows even simple learning paradigm normalization processes essential moreover numerous normalization procedures prevent synaptic runaway analysis shows specific neuronally driven correction procedure preserves total sum synaptic efiqcacies leads effective associative memory storage effective synaptic learning rules study computational aspects associative learning model lowactivity associative memory network binary firing neurons uncorret lated memory patterns coding level fraction firing neurons stored neurons network ith neuron updates state time xt sign fi input field postsynaptic potential firing threshold synaptic weight wij jth presynaptic ith postsynaptic neurons determined general additive synaptic learning rule depending neurons activity stored memory patterns presynaptic wij postsynaptic synaptic learning matrix governs incremental modifications synapse function firing presynaptic column postsynaptic row neurons conventional biological terms denotes increment following long term potentiation ltp event denotes heterosynaptic long term depression ltd homosynaptic ltd event parameters define four dimensional space linear additive hebbian learning rules reside however order visualize space one may represent hebbian learning rules reduced two dimensional space utilizing scaling invariance constraint requirement synaptic matrix zero mean otherwise synaptic values diverge noise overshadows signal term retrieval possible figure plots memory capacity network function two free parameters reveals considerable memory storage may obtained along essentially one dimensional curve naturally raising possibility identifying additional constraint relations constraint revealed signal noise analysis neuronal input field fi retrieval signal noise fi fi var fi var wij npcov wij wik vp ov chechik meilijson ruppin averages taken ensemble stored memory patterns ee lo ha beta beta figure memory capacity neurons network different values obtained computer simulations capacity defined maximal number memories retrieved overlap bigger presented degraded input cue overlap overlap serves measure retrieval acuity defined mn xj memory capacity effective learning rules peak values ridge figure displayed tracing projection coordinate optimal learning rule marked arrow performs slightly better effective learning rules evident equation already pointed postsynaptic covariance cov determining covariance incoming synapses postsynaptic neuron positive network memory capacity bounded scale network size postsynaptic covariance non negative effective learning rules obtain linear scaling memory capacity function network size require vanishing postsynaptic covariance intuitively synaptic weights correlated adding new synapse contributes little new information thus limiting number beneficial synapses help neuron estimate whether fire figure lb depicts memory capacity effective synaptic learning rules lie essentially one dimensional ridge observed figure shows effective rules slightly inferior optimal synaptic learning rule calculated previously maximizes memory capacity vanishing covariance constraint effective learning rules implies new requirement concerning balance synaptic depression facilitation thus effective memory storage requires delicate balance ltp xp heterosynaptic depression strongly dependent coding level global property network thus difficult see effective rules implemented synaptic level moreover shown figure hebbian learning rules lack robustness small perturbations effective rules may result large decrease memory capacity effective learning requires neuronal remodeling hebbian synapses furthermore problems cannot circumvented introducing nonlinear hebbian learning rule form wij even nonlinear function covariance cov na na remains positive cov positive observations show effective associative learning hebbian rules alone implausible biological standpoint requiring locality information effective learning via neuronal weight correction results show order obtain effective memory storage postsynaptic covariance must kept negligible may effective storage take place brain hebbian learning proceed show neuronally driven procedure essentially similar assumed occur self organization maintain vanishing covariance turn ineffective hebbian synapses effective ones enables brain utilize ineglcient learning rules use local information still attain effective learning capabilities solution emerges rewriting signal noise equation eq noise signal nvar wij pvar jn wij showing post synaptic covariance greatly diminished variance sum incoming synapses vanishing thus propose following neuronal weight correction procedure learning whenever synapse modified postsynaptic neuron additively modifies synapses maintain sum eglcacies baseline zero level wij wij wij vj neuronal weight correction additive performed either several memories stored done prescriptive learning storge memory pattern developmental learning models interestingly joint operation weight correction linear hebbian learning rule equivalent storage set memory patterns another hebbian learning rule prove new rule zero covariance learning matrix reemphasized matrix right applied synaptic level emergent result operation neuronal mechanism matrix left used mathematical tool analyze network performance thus using neuronal mechanism maintains sum incoming synapses fixed enables level effective performance would achieved using zero covariance hebbian learning rule without need know memories coding level chechik meilijson ruppin demonstrate beneficiary effects neuronal weight correction first applied common realization hebb rule inhibition added obtain zero mean input field otherwise capacity vanishes yielding even though learning rule zero mean synaptic matrix postsynaptic covariance non zero thus still ineffective rule applying neuronal weight correction learning rule results synaptic matrix identical one generated rule without neuronal weight correction zero mean zero postsynaptic covariance figure plots memory capacity obtained zero mean hebb rule neuronal weight correction function network size applying neuronal weight correction originally bounded capacity turns scale linearly network size ineffective learning rule variable coding level network size optimal learning rule neuronal correction network size figure network memory capacity function network size original zero mean learning rule bounded memory capacity capacity becomes linear network size learning rule coupled neuronal weight correction lines plot analytical results squares designate simulation results even optimal learning rule becomes ineffective stored patterns variable coding levels coding levels normally distributed neuronal weight correction provides successful memory storage patterns results obtained computer simulations effectiveness learning rule depends coding level stored patterns learning rules turn ineffective coding levels stored patterns heterogeneous figure compares memory capacity network uses optimal learning rule coding level actually stores memory patterns coding levels normally distributed around application neuronal weight correction provides effective storage patterns optimal learning rule neuronal regulation implements weight correction like previous normalization procedures proposed neuronal algorithm relies availability explicit information total sum synaptic efficacies neuronal level however explicit information synaptic sum may available several mechanisms conservation total synaptic strength proposed see review focus one mechanism neutonal regulation nr total synaptic sum regulated indirectly estimating neuron average postsynaptic potential nr slow process continuously modifying synaptic efficacies maintain homeostasis effective learning requires neuronal remodeling hebbian synapses neuronal activity activity dependent scaling excitatory synapses acts maintain homeostasis neuronal firing already observed cortical tissues studied operation nr driven correction compared additive neuronal weight correction excitatory inhibitory network figure plots memory capacity networks storing memories according hebb rule showing nr approximates additive neuronal weight correction succeeds obtaining linear growth memory capacity eoriginal hebb rule neuronal weight correction network size figure applying nr achieves linear scaling memory capacity slightly inferior capacity compared obtained neuronal weight correction memory patterns stored according hebb rule wij summary paper analyzed hebbian learning rules associative memory network models identified essential requirement effective memory storage vanishing postsynaptic covariance show constraint depends coding level stored memory patterns thus requiring use network level information synaptic level moreover stored memory patterns heterogeneous single learning rule effectively store patterns show applying neuronally driven mechanism preserves total synaptic sum zeroes catastrophic covariance provides effective learning even ineffective synaptic learning rules resulting improvement memory capacity drastic learning rules yielding bounded capacity transformed learning rules yielding linear memory capacity function network size finally normalization mechanism carried neuronal regulation nr mechanism recently identified mammalian cortical cultures characterization effective synaptic learning rules reopens discussion computational role heterosynaptic homosynaptic depression previous studies shown long term synaptic depression necessary prevent saturation synaptic values maintain zero mean synaptic efflcacies study shows effective learning requires proper heterosynaptic depression obtained regardless homosynaptic depression magnitude terms potentiation depression used context cautiously interpreted apparent changes synaptic efficacies measured ltd ltp experiments may involve two kinds processes synaptic driven processes changing synapses according covariance pre post synaptic neurons neuronally driven processes operating zero covariance incoming synapses neuron processes may experimentally segregated operate different time scales relative weights experimentally tested chechik meilijson ruppin several forms synaptic constraints suggested improve stability hebbian learning analysis shows effective memory storage requires sum synaptic strengths must preserved thus predicting specific form normalization occurs brain utilization simple mccullough pitts model studied enabled us gain analytical insight phenomena hand recent findings neuronal weight normalization spiking models lead us believe results also extent spiking neurons networks neuronal weight correction qualitatively improves ability neuron correctly discriminate large number input patterns thus enhances computational power single neuron likely play fundamental computational role variety brain functions perceptual processing associative learning references miller mackay role constraints hebbian learning neural computation von der malsburg self organization orientation sensitive cells striate cortex kybernetik erkki oja simplified neuron model principal component analyzer journal mathematical biology grinstein massica ruppin synaptic runaway associative networks pathogenesis schizophrenia neural computation dayan willshaw optimizing synaptic learning rules linear associative memories biol cyber palm sommer associative data storage retrielval neural networks domani vanhemmen eds schulten editors models neural networks iii association generalization represantation pages springer tsodyks associative memory neural networks hebbian learning rule modern physics letters miller synaptic economics competition cooperation synaptic plasticity neuron turrigano leslie desai nelson activity dependent scaling quantal amplitude neocoritcal pyramidal neurons nature sejnowski statistical constraints synaptic plasticity theo biol willshaw dayan optimal plasticity matrix memories goes must come neural computation bear abraham long term depression hippocampus annu rev neurosci kempter gerstner van hemmen hebbian learning spiking neurons phys rev
abstract recent theories suggest language acquisition assisted evolution languages towards forms easily learnable paper evolve combinatorial languages learned recurrent neural network quickly relatively examples additionally evolve languages generalization different worlds generalization specific examples find languages evolved facilitate different forms impressive generalization minimally biased general purpose learner results provide empirical support theory language well language environment learner plays substantial role learning far language acquisition language acquisition device introduction factors language learnability exploring issues language learnability special abilities humans learn complex languages much emphasized one dominant theory based innate domain specific learning mechanisms specifically tuned learning human languages argued without strong constraints learning mechanism complex syntax language could learned sparse data child observes recent theories challenge claim emphasize interaction learner environment addition two theories proposal rather language savvy infants languages adapt human learners ones survive infant friendly languages date relatively empirical studies explored adaptation language facilitates learning hare elman demonstrated evolving learnable languages classes past tense forms could evolve simulated generations response changes frequency verbs using neural networks kirby showed using symbolic system compositional languages likely emerge learning constrained limited set examples batali evolved recurrent networks communicate simple structured concepts argument humans general purpose learners rather current research questions require exploring nature extent biases learners bring language learning ways languages exploit biases previous theories suggesting many aspects language unlearnable without strong biases gradually breaking new aspects language shown learnable much weaker biases studies include investigation languages may exploit biases subtle attention memory limitations children complementary study shown general purpose learners evolve biases form initial starting weights facilitate learning family recursive languages paper present empirical paradigm continuing exploration factors contribute language learnability paradigm propose necessitates evolution languages comprising recursive sentences symbolic strings languages whose sentences cannot conveyed without combinatorial composition symbols drawn finite alphabet paradigm based specific natural language rather simplest task could find illustrate point languages compositional structure evolved learnable sentences simplicity communication task allows us analyze language generalizability highlight nature generalization properties start evolution recursive language learned easily five sentences minimally biased learner address issues robust learning evolved languages showing different languages support generalization different ways also address factor scant regard paid namely languages may evolve learners also easily generalizable specific set concepts seems almost axiomatic learning paradigms sample randomly training domain may human languages learnable random sentences easily generalizable examples child likely exposed environment third series simulations test whether language adapt learnable core set concepts paradigm exploring language learnability consider simple language task two recurrent neural networks try communicate concept represented point unit interval symbolic channel encoder network sends sequence symbols thresholded outputs concept decoder network receives processes back concept framework described greater detail communication successful decoder output approximate encoder input concepts architecture encoder recurrent network one input unit five output units recurrent connections output hidden units back hidden units encoder produces sequence five symbols states output units taken followed symbol concept taken encode value network tonkes blair wiles cai xa iaiiiii aia ix bcaea ec babbabba baboa bcbca ii iiii iti iti iti ce bc bc caebaecbaebcabaecbaebc figure hierarchical decomposition language produced encoder first symbols produced appearing near root tree ordering leaves tree represent input space smaller inputs encoded sentences left examples used train best decoder found evolution highlighted decoder must generalize branches order learn task decoder must generalize systematically novel states tree including generalizing symbols different positions sequence figure shows sequence states successful decoder presented sequence inputs step output units network assume one eleven states zero output greater denoted saturation two highest activations remainder denoted zero output produced propagation halted otherwise propagation continues five steps output units assume zero state decoder recurrent network input units single output recurrent hidden layer former work shown due conflicting constraints encoder decoder easier decoder process strings reverse order produced encoder consequently input decoder taken reverse output decoder except remains last symbol clarity strings written order produced encoder input pattern presented decoder matches output encoder either two units active none network trained backpropagation time produce desired value presentation final symbol sequence simple hill climbing evolutionary strategy two stage evaluation function used evolve initially random encoder one produces language random decoder learn easily examples evaluation encoder mutated current champion addition gaussian noise weights performed two criteria mutated network must produce greater variety sequences range inputs decoder initially small random weights trained mutated encoder output must yield lower sum squared error across entire range inputs champion mutant encoder paired single decoder initially random weights mutant encoder decoder pair successful champion mutant becomes champion process repeated since encoder input space continuous impossible examine entirety input range approximated uniformly distributed examples final output hill climber language generated best encoder found evolving learnable languages evolving easily learnable language humans learn sparse data first series simulations test whether compositional language evolved learners reliably effectively learn five examples five training examples seems unreasonable expect decoder would learn task task intentionally hard language restricted sequences discrete symbols must describe continuous space note simple linear interpolation possible due symbolic alphabet languages recursive solutions possible unable learned unbiased learner decoder minimally biased learner simulations showed performed much better arguments based learnability theory would predict ten languages evolved hill climbing algorithm outlined generations language new random decoders trained conditions evolution five examples epochs ten runs used encoders decoders five hidden units evolved languages learnable decoders minimum maximum mean learner said effectively learned language sum squared error across points space less encoders employed average sentences minimum maximum communicate points training examples decoder sampled randomly hence decoders faced difficult generalization tasks difficulty task demonstrated language analyzed figures evolved languages contained similar compositional structure language described figures inherent biases decoder although minimal clearly sufficient learning compositional structure evolving languages particular generalization first series simulations demonstrate find languages minimally biased learner generalize examples next simulations consider whether languages evolved facilitate specific forms generalization users section considered case decoder required output encoder input setup yields approximation line figure compositional structure evolved languages allows decoder generalize unseen regions space following series simulations consider relationship structure language way decoder required generalize association studied altering desired relationship encoder input decoder output two sets ten languages evolved one set requiring identity section using function resembling series five steps random heights xj random step conditions section exception training examples used hill climber ran generations completion evolution decoders trained final languages conditions one generation represents creation variable mutated encoder subsequent training decoder language said reliably learnable least random decoders able effectively learn within epochs xj provides index array based magnitude tonkes blair les figure decoder output seeing first symbols message language figure axis encoder input axis decoder output point sequence five points decoder trained shown crosses graph first symbol decoder outputs one five values second symbol outputs possible subsequent symbols string specify finer gradations output note output constructed monotonically symbol providing closer approximation target function rather recursively approximating linear target final position sequence structure inherent sequences allows system generalize parts space never seen note generalization based interpolation symbol values rather compositional structure well two others sine function cubic function results show languages evolved enhance generalization preferentially one world another average languages performed far better tested world evolved worlds languages evolved identity mapping average learned decoders trained identity task compared random step case languages evolved random step task learned decoders trained random step task trained identity task decoders generally performed poorly cubic function decoder learned sine task either set evolved languages second series simulations show manner decoder generalizes restricted task section rather languages evolve facilitate generalization decoder different ways aided minimal biases evolving learnable languages generalization core concepts former simulations randomly selected concepts used train decoders cases pathological distribution points made learning extremely difficult contrast seems likely human children learn language based common set semantically constrained core concepts mom want milk etc third series simulations tested whether selecting fortuitous set training concepts could positive affect success evolved language simulations alternative generalization functions section indicated decoders difficulty generalizing sine function even encoders evolved specifically sine task best systems random decoders successfully learned evolved new language specifically chosen set points generalization sine function one hundred decoders trained resulting language using either set points random set networks trained fixed set learned tasked compared networks trained random sets language evolves communicate restricted set concepts particularly unusual simulation shows surprising result language evolve generalize specific core concepts whole rccursive language particular way case sine function discussion first series simulations show compositional language learned five strings recurrent network generalization performance included correct decoding novel branches symbols novel positions figure second series simulations highlight language evolved facilitate different forms generalization decoder final simulation demonstrates languages also tailored generalize specific set examples three series simulations modify language environment decoder three different ways relationship utterances meaning type generalization required decoder particular utterances meanings learner exposed case language environment learner sculpted exploit minimal biases present learner taking approach similar giving learner additional bias form initial weights also likely effective purpose simulations investigate strongly external factors could assist simplifying learning conclusions key understanding language learnability lie richly social context language training incredibly prescient guesses young language learners rather lies process seems otherwise far remote microcosm toddlers caretakers language change although rate social evolutionary change learning structure appears unchanging compared time takes child develop language abilities process crucial understanding child learn language surface appears impossibly complex poorly taught tonkes blair wiles paper studied ways languages adapt learners running simulations language evolution process contribute additional components list aspects language learned minimally biased general purpose learners namely recursive structure learned examples languages evolve facilitate generalization particular way evolve easily learnable common sentences simulations paper enhancement language learnability achieved changes learner environment without resorting adding biases language acquisition device acknowledgements work supported apa bradley tonkes uq postdoctoral fellowship alan blair arc grant janet wiles references chomsky language mind harcourt brace new york elman bates johnson karmiloff smith parisi plunkeft rethinking innateness connectionist perspective development mit press boston deacon symbolic species co evolution language brain norton company new york kirby fitness selective adaptation language hurford knight studdert kennedy editors approaches evolution language cambridge university press cambridge christiansen language organism implications evolution acquisition language unpublished manuscript february hare elman learning morphological change cognition kirby syntax without natural selection compositionality emerges vocabulary population learners knight hurford studdertkennedy editors evolutionary emergence language social function origins linguistic form cambridge university press cambridge batall computational simulations emergence grammar hurford knight studdert kennedy editors approaches evolution language pages cambridge university press cambridge elman learning development neural networks importance staxting small cognition batall innate biases critical periods combining evolution learning acquisition syntax brooks maes editors proceedings fourth artificial life workshop pages mit press tonkes blair wiles paradox neural encoders decoders talk backwards mckay yao newton kim furuhashi editors simulated evolution learning volume lecture notes artificial intelligence springer
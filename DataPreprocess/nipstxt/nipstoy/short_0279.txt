abstract gaussian mixtures called radial basis function networks density estimation provide natural counterpart sigmoidal neural networks function fitting approximation cases possible give simple expressions iterative improvement performance components network introduced one time particular mixture density estimation show component mixture estimated maximum likelihood iterative likelihood improvement introduce achieves log likelihood within order log likelihood achievable convex combination consequences approximation estimation using kullback leibler risk also given minimum description length principle selects optimal number components minimizes risk bound
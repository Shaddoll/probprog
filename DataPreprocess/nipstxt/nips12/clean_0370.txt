abstract recently sample complexity bounds derived problems involving linear functions neural networks support vector machines paper extend theoretical results area deriving dimensional independent covering number bounds regularized linear functions certain regularization conditions show bounds lead class new methods training linear classifiers similar theoretical advantages support vector machine furthermore also present theoretical analysis new methods asymptotic statistical point view technique provides better description large sample behaviors algorithms introduction paper interested generalization performance linear classifiers obtained certain algorithms computational learning theory point view performance measurements sample complexity bounds described quantity called covering number measures size parametric function family two class classification problem covering number bounded combinatorial quantity called vc dimension following work researchers found combinatorial quantities dimensions useful bounding covering numbers consequently concept vc dimension generalized deal general problems example recently vapnik introduced concept support vector machine successful applied many real problems method achieves good generalization restricting norm weights separating hyperplane similar technique investigated bartlett author studied performance neural networks norm weights bounded idea also applied explain effectiveness boosting algorithm paper extend results emphasize importance dimension independence specifically consider following form regularization method emphasis classification problems widely studied regression problems statistics convergence regularized linear functions numerical mathematics inf ex yl inf ex zy expectation distribution binary label data vector apply formulation purpose training linear classifiers choose decreasing function choose function penalizes large lirn oo appropriately chosen positive parameter balance two terms paper organized follows section briefly review concept covering numbers well main results related analyzing performance learning algorithms section introduce regularization idea main goal construct regularization conditions dimension independent bounds covering numbers obtained section extends results previous section nonlinear compositions linear functions section give asymptotic formula generalization performance learning algorithm used analyze instance svm due space limitation present main results discuss implications detailed derivations found covering numbers formulate learning problem find parameter random observations minimize risk given loss function observations zx independently drawn fixed unknown distribution want find minimizes expected loss risk dp natural method solving using limited number observations empirical risk minimization erm method cf simply choose parameter minimizes observed risk denote parameter obtained way erm convergence behavior method analyzed using vc theoretical point view relies uniform convergence empirical risk uniform law large numbers sups ir bound obtained quantities measure size glivenko cantelli class finite number indices family size measured simply cardinality general function families well known quantity measure degree uniform convergence covering number dated back kolmogrov idea discretize depend data parameter space values approximated ai shall describe simplified version relevant purposes definition let metric space metric given norm observations vectors parameterized covering number norm denoted minimum number collection vectors vt va vi ip also denote alp maxx alp xp note definition jensen inequality always assume metric zx explicitly specified otherwise following theorem due pollard zhang theorem lid distribution supir exp sup inf independently drawn constants theorem improved certain problems see related results however yield similar bounds result relevant paper lemma norm covering number replaced cx norm covering number latter bounded scale sensitive combinatorial dimension bounded norm covering number covering number depend results replace theorem yield better estimates certain circumstances since bartlett lemma binary loss functions shall give generalization comparable theorem theorem let fx two functions lyx implies fa rs reference separating function sup fx exl afoo exp note extreme case choice achieves perfect generalization assume choices always satisfy condition ex better bounds obtained using refined version chernoffbound covering number bounds linear systems section present new bounds covering numbers following form real valued loss functions ziwi shall see later bounds relevant convergence properties note order apply theorem since therefore sufficient estimate clear finite restrictions imposed therefore following assume bounded study conditions llwllq log independent weakly dependent first result generalizes theorem bartlett original results oo related technique also appeared proof uses lemma attributed maurey cf theorem llllp andllvollq oo bound covering number depends logarithmically already quite weak compared linear dependency standard situation however bound theorem tight oo example following theorem improves bound technique proof relies svd decomposition matrices improves similar result logarithmic factor convergence regularized linear functions theorem ifllill iill log log next theorem shows norm covering number also ndependent dimension theorem let llillp bandllwllq log ab one consequence theorem potentially refined explanation tbr boosting algorithm boosting algorithm analyzed using technique related results essentially rely theorem oo unfommately bound contains logarithmic dependency general case seem fully explain fact many cases performance boosting algorithm keeps improving increases however seemingly mysterious behavior might better understood theorem assumption data restricted simply cr norm bounded example contribution wrong predictions bounded constant grow slowly increases regard th norm bounded oo case theorem implies dimensional independent generalization want apply theorem necessary obtain bounds fbr infinity norm covering numbers following theorem gives bounds using result online learning theorem ill lip oo log log ab case ofp oo entropy condition used obtain dimensional independent covering number bounds definition let vector positive entries iill case call distribution vector let zi vector length define weighted relative entropy respect theorem given distribution vector ii entr assume non negative entries ac log ab log foo theorems section combined theorem form complex covering number bounds nonlinear compositions linear functions zhang nonlinear extensions consider following system observation parameter assume nonlinear function bounded total variation definition function said satisj lipschitz condition parameter ifv yl definition total variation function defined tv sup also denote tv oo tv theorem ifl ct wth tv lipschitz parameter assume also dimensional vector ilwllq vq en tv log metric defined gx cll example consider classification hyperplane wt set indicator function let fo wt another loss function fo instead using erm estimating parameter minimizes risk consider scheme minimize empirical risk associated assumption constraint ilwl denote estimated parameter follows covering number bounds theorem probability least ab nab ezi wnt inf ezfo apply slight generalization theorem covering number bound theorem probability least exi ln lnn bounds given paper applied show appropriate regularization conditions assumptions data methods based lead generalization performances form symbol independent used indicate hidden constant may include polynomial dependency log also important note certain cases appear small influence convergence constant demonstrated example next section convergence regularized linear functions asymptotic analysis convergence results previous sections form vc style convergence probability combinatorial flavor however problems differentiable function families involving vector parametem often convenient derive precise asymptotic results using differential structure assume parameter vector smooth function let denote optimal parameter denote derivative respect ct denote assume certain regularity conditions asymptotic expected generalization error given erm tr lu generally evaluation function vh erm tr veh hessian matrix note approach assumes optimal solution unique results exact asymptotically provide better bounds standard pac analysis example would like study form support vector machine consider tz kc discontinuity derivative asymptotic formula may hold however make assumption smoothness distribution expectation derivative still smooth case smoothness crucial furthermore separate report shall illustrate similar small sample bounds without assumption smoothness distribution obtained using techniques related asymptotic analysis consider optimal parameter let tz note xa es sz sz assume tz positive semi definite matrix follows ez tr ii sup ilzll consider obtained observations minimizing empirical risk associated loss function ae inaf sup asymptotically let scheme becomes optimal separating hyperplane asymptotic bound better typical pac bounds fixed note although bound obtained example similar mistake bound perceptron online update algorithm may practice obtain much better estimates plugging empirical data zhang
abstract examine psychophysical law describes influence stimulus context perception according law choice probability ratios factorize components independently controlled stimulus context argued pattern results incompatible feedback models perception paper examine claim using neural network models defined via stochastic differential equations show law related condition named channel separability little existence feedback connections essence channels separable converge response units without direct lateral connections channels sensors directly contaminated external inputs channels implications analysis cognitive computational neurosicence discussed introduction examine psychophysical law named morton massaro law implications connectionist models perception neural information processing example type experiments covered morton massaro law consider experiment massaro cohen subjects identify synthetic consonant sounds presented context phonemes two response alternatives seven stimulus conditions four context conditions response alternatives stimuli synthetic sounds generated varying onset frequency third formant followed vowel stimuli placed four different context consonants morton massaro independently showed remarkable range experiments type influence stimulus context response probabilities accounted factorized version luce strength model luce ls evs vc random variables representing stimulus context subject response set stimulus context response al movellan mcclelland ternatives represents support stimulus response support context response assuming strength parameter exactly zero equivalent kis says response probability ratios factorize two components one affected stimulus unaffected context one affected context unaffected stimulus diffusion models perception massaro conjectured morton massaro law may incompatible feedback models perception conjecture based idea networks feedback connections stimulus effect context units context effect stimulus units making impossible factorize influence information sources paper analyze conjecture show surprisin gly morton massaro law little existence feedback lateral connections ground analysis continuous stochastic versions recurrent neural networks call models diffusion neural networks stochastic diffusion processes defined adding brownian motion standard recurrent neural network dynamics diffusion networks defined following stochastic differential equation dyi dt rr dbi random variable representing internal potential time th unit yn represents external input consists stimulus context hi brownian motion acts stochastic driving term constant rr known dispersion controls amount noise injected onto unit function known dr determines average instantaneous change activation borrowed standard recurrent neural network literature change modulated matrix connections units matrix controls influence external inputs onto unit ri ri ri positive function named capacitance controlling speed processing ewi jzj evi kxk zj yj yj wi element connection matrix weight unit unit vi element matrix logistic activation function terms gain parameters control sharpness activation functions large values activation function unit converges step function variable zj represents short time mean firing rate activation unit analysis grounded discrete time networks binary states see mcclelland information factorization scaled range intuition equation achieved thinking limit discrete time difference equation case yi av ni ni independent standard gaussian random variables fixed state time two forces controlling change activation drift deterministic dispersion stochastic results distribution states time goes zero solution difference equation converges diffusion process defined paper focus behavior diffusion networks stochastic equilibrium assume network given enough time approximate stochastic equilibrium response sampled channel separability section show morton massaro related architectural constraint named channel separability nothing existence feedback connections order define channel separability useful characterize function different units using following categories response specification units unit response specification unit state units network fixed changing state unit affects probability distribution overt responses stimulus units unit belongs stimulus channel response unit state response units fixed probability distribution activations unit affected stimulus context units unit belongs context channel response unit states response units fixed probability distribution activations unit affected context given definitions say network separable stimulus context channels stimulus context units disjoint unit simultaneously belongs stimulus context channels essence channels structurally separable converge response units without direct lateral connections channels sensors directly contaminated external inputs channels see figure rest paper show diffusion network structurally separable morton massaro law approximated arbitrary precision regardless existence feedback connections simplicity examine case weight matrix symmetric case state associated goodness function greatly simplifies analysis later section discuss results generalize non symmetric case let represent internal potential diffusion network let zi otiyi represent firing rates corresponding let represent components units stimulus channel context channel response specification module let vector representing input let components external stimulus context let ot oil otn fixed gain vector za random vector representing firing rates time network gain vector let limt oo represent firing rates stochastic equilibrium movellan shown weights symmetric ni dx equilibrium probability density follows exp cr ga zs zrlxs xc pz ix xc xc movellan mcclelland onse sp icatlon units stimulus context iays rdays stimulus context smsors sensors timuim input context figure network separable context stimulus processing channels stimulus sensor stimulus relay units make stimulus channel units context sensor context channel units make context channel units note modules empty except response module ka xs xc exp ga xs xc dz ix ix ix sc zi ai log zi log zi log zi log without loss generality hereafter set direct connections stimulus context units terms goodness function occur jointly goodness separated three additive terms depend third term depends response units zr zr zc inf ormat facto rization ws submatrix connecting stimulus response units similar notation used submatrices follows write ratio joint probability density two states follows pz lx zs zc zr xs xc exp zs zc pz lx exp wh factorizes desired get probability densities response units integrate states units pz ix zr xs xc pz ix zc dz dz ging terms pz lx also factorizes left mapping continuous states response units discrete externm responses tition space response specification units screte regions probability response becomes integral probability density region corresponding response problem integral probability densities necessarily factorize even though densities factorize every point fortunately two import es law holds le good approximation first response ons small thus approximate inte density point times volume ratio integrms appro mated ratio probability densities individum states second applies models like mcclell rumelhm interactive activation model response sociated distinct response unit models typicmly negative connections mongst response units equilibrium one unit tends active others inactive common response policy picks response corresponding tive unit show policy approximate morton law bitrary level precision gain meter response units incre ed let represent joint state network let first components states response specification units let two dimensionm vectors representing states response specification units let sets external responses two responses corners lim lim pz lx zx pz lx lx regions space mapping two distinct investigate convergence probability ratio let response regions collapse lim fr pz lx ulx du fr pzxlx du lim er dzs dzc er dzs dz movellan mcclelland table predictions morton massaro law left side versus diffusion network square brackets subject massaro cohen experiment prediction diffusion network based random samples context stimulus note since soi ai ir soi ff follows lim eh zi ej dz dz eh zc zl ei dz dz easy show ratio factorizes moreover let lim za since gain response units increases sot decreases fast corners thus cx random variable za converges distribution discrete random variable mass corner hypercube factorized probability ratios expressed since indexing response units arbitrary argument applies responses discussion analysis establishes diffusion networks morton massaro law incompatible presence feedback lateral connections surprisingly even though diffusion networks feedback connections stimulus context units interdependent still possible factorize effect stimulus context response probabilities analysis shows morton massaro arbitrarily approximated sharpness response units increased practice found good approximations relatively small values sharpness parameter see table example analysis assumed weights symmetric mathematical analysis general case non symmetric weights difficult information factorization however useful approximations exist movellan mcclelland showing noise parameter relatively small activation function approximately linear symmetric weights needed exhibit morton massaro law analysis presented potential applications investigate models perception functional architecture brain example interactive activation model word perception separable architecture thus diffusion versions adhere morton massaro law analysis also points potential applications computational neuroscience would interest study whether morton massaro holds level neural responses example may excite neuron two different sources information observe short term average response combination stimuli observed distribution responses exhibits morton massaro law would consistent existence separable channels converging neuron otherwise would indicate channels two input areas response may structurally separable
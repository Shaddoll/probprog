abstract paper consider problem active learning trigonometric polynomial networks give necessary sufficient condition sample points provide optimal generalization capability analyzing condition functional analytic point view clarify mechanism achieving optimal generalization capability also show set training examples satisfying condition provide optimal generalization also reduces computational complexity memory required calculation learning results finally examples sample points satisfying condition given computer simulations performed demonstrate effectiveness proposed active learning method introduction supervised learning obtaining underlying rule training examples formulated function approximation problem sample points actively designed learning performed efficiently paper discuss problem designing sample points referred active learning optimal generalization active learning classified two categories depending optimality one global optimal set training examples optimal fedorov greedy optimal next training example sample optimal step mackay cohn fukumizu sugiyama ogawa paper focus global optimal case give new active learning method trigonometric polynomial networks proposed method employ approximations derivation provides exactly optimal generalization capability moreover proposed method reduces computational complexity memory required calculation learning results finally effectiveness proposed method demonstrated computer simulations http ogawa www cs titech ac jp sugi training data selection optimal generalization formulation supervised learning section supervised learning problem formulated functional analytic point view see ogawa learning criterion model described supervised learning inverse problem let us consider problem obtaining optimal approximation target function variables set training examples training examples made sample points xm subset ldimensional euclidean space corresponding sample values ym xm ym ym xm ym degraded zero mean additive noise nm let mdimensional vectors whose th elements nm ym respectively called sample value vector paper target function assumed belong reproducing kernel hilbert space aronszajn unknown estimated model selection methods sugiyama ogawa let reproducing kernel function pm defined pm xm value sample point xm expressed xm stands inner product reason called sampling function let operator defined era em th vector called standard basis stands neumann schatten product called sampling operator relationship expressed af let us denote mapping learning result fo xy called learning operator supervised learning problem reformulated inverse problem obtaining providing best approximation certain learning criterion learning criterion model mentioned function approximation performed basis learning criterion purpose learning minimize generalization error learning result measured jg enllf fl en denotes ensemble average noise paper adopt projection learning learning criterion let adjoint operator range orthogonal projection operator onto respectively projection learning defined follows xfor fixed hilbert space hx fixed hilbert space neumann schatten product operator hx defined using nx sugiyama ogawa definition projection learning ogawa operator called projection learning operator minimizes functional jp ix en ii xn constraint xa well known eq decomposed bias variance jg ip enllxnll eq implies projection learning criterion reduces bias certain level minimizes variance let us consider following function space definition trigonometric polynomial space let let nl positive integer function space called trigonometric polynomial space order spanned exp int nl nl nl defined inner product defined dimension trigonometric polynomial space order ri ni reproducing kernel space expressed sin ran nt active learning trigonometric polynomial space problem active learning find set xm sample points providing optimal generalization capability section give optimal solution active learning problem trigonometric polynomial space let moore penrose generalized inverse following proposition holds proposition noise covariance matrix given projection learning operator expressed note sampling operator uniquely determined xm see eq eq bias learning result becomes zero iv iv stands null space operator reason operator called moore penrose generalized inverse operator satisfies axa xax ax ax xa xa training data selection optimal generalization figure mechanism noise suppression theorem set xm sample points satisfies mi xaf xnl consider case set xm sample points satisfies af case eq reduced ag enlln nll equivalent noise variance consequently problem active learning becomes problem finding set sample points minimizing eq constraint af first derive condition optimal generalization terms sampling operator theorem assume noise covariance matrix given cr jg eq minimized constraint iv denotes identity operator case minimum value tz tz dimension eq implies forms pseudo orthonormal basis ogawa extension orthonormal bases following lemma gives interpretation theorem lemma set sample points satisfies eq holds xaf ilafl llfll ilxull ull eqs imply becomes isometry becomes partial isometry initial space respectively let us decompose noise nl nl sample value vector rewritten af nl follows eq signal component af transformed original function eq suppresses magnitude noise nl completely removes sugiyama ogawa xl theorem theorem figure two examples sample points condition holds noise analysis summarized fig note theorem interpretation valid hilbert spaces constant theorem given necessary sufficient condition minimize jg terms sampling operator give two examples sample points xm condition holds focus case dimension input simplicity however following results easily scaled case theorem let constant determined eq holds dimension let arbitrary set xm sample points theorem let kl positive integer constant determined xm eq holds let arbitrary set sample points mod theorem means sample points fixed intervals domain sample values gathered point see fig contrast theorem means sample points fixed intervals domain sample values gathered times point see fig discuss calculation methods projection learning result fo let hm th column vector dimensional matrix aa general sample points projection learning result fo calculated fo hm use optimal sample points satisfying condition following theorems hold theorem lated eq holds projection learning result fo calcum fo ymcm training data selection optimal generalization theorem sample points determined following theorem projection learning result fo calculated eq coefficient obtained inner product hm contrast replaced ym eq implies moore penrose generalized inverse aa required calculating fo property quite useful number training examples large since calculation moore penrose generalized inverse high dimensional matrices sometimes unstable eq number basis functions reduced coefficient bp obtained pp pp mean sample values xp general sample points computational complexity memory required calculating fo eq contrast theorem states set sample points satisfies eq computational complexity memory reduced hence theorem theorem provide optimal generalization also reduce computational complexity memory moreover determine sample points following theorem calculate learning result theorem computational complexity memory reduced extremely efficient since depend number training examples results shown tab simulations section effectiveness proposed active learning method demonstrated computer simulations let trigonometric polynomial space order noise covariance matrix let us consider following three sampling schemes optimal sampling training examples gathered following theorem experimental design eq cohn adopted active learning criterion value criterion evaluated reference points sampling location determined multi point search candidates passive learning training examples given unilaterally fig shows relation number training examples generalization error horizontal vertical axes display number training examples generalization error jg measured eq respectively solid line shows sampling scheme dashed dotted lines denote averages trials sampling schemes respectively number training examples generalization error sampling scheme generalization errors sampling schemes respectively graph illustrates proposed sampling scheme gives much better generalization capability sampling schemes especially number training examples large conclusion proposed new active learning method trigonometric polynomial space proposed method provides exactly optimal generalization capability sugiyama ogawa table computational complexity memory required projection optimal sampling learning computational calculation complexity methods memory eq theorem theorem dimension number training examples positive integer figure relation number training examples generalization error time reduces computational complexity memory required calculation learning results mechanism achieving optimal generalization clarified functional analytic point view
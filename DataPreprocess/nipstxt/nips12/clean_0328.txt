abstract calculate lower bounds size sigmoidal neural networks approximate continuous functions particular show approximation polynomials network size grow log degree polynomials bound valid input dimension independently number variables result obtained introducing new method employing upper bounds vapnik chervonenkis dimension proving lower bounds size networks approximate continuous functions introduction sigmoidal neural networks known universal approximators one theoretical results frequently cited justify use sigmoidal neural networks applications statement one refers fact sigmoidal neural networks shown able approximate continuous function arbitrarily well numerous results literature established variants universal approximation property considering distinct function classes approximated network architectures using different types neural activation functions respect various approximation criteria see instance see particular scarselli tsoi recent survey references results many others referenced constructive merely existence proofs provide upper bounds network size asserting good approximation possible sufficiently many network nodes available however partial answer question mainly arises practical applications given function many network nodes needed approximate much attention focused establishing lower bounds network size particular approximation functions reals far computation binary valued complexity approximating continuous functions neural networks functions sigmoidal networks concerned output value network thresholded yield results direction specific boolean function koiran showed networks using standard sigmoid activation function must size fl number inputs measuring network size count input nodes follows maass established larger lower bound constructing binary valued function showing standard sigmoidal networks require fl many network nodes computing function first work complexity sigmoidal networks approximating continuous functions due dasgupta schnitger showed standard sigmoid network nodes replaced types activation functions without increasing size network polynomial yields indirect lower bounds size sigmoidal networks terms network types dasgupta schnitger also claimed size bound sigmoidal networks layers approximating function sin ax paper consider problem using standard sigmoid neural networks approximation polynomials show least fl log network nodes required approximate polynomials degree small error norm bound valid arbitrary input dimension depend number variables lower bounds also obtained results binary valued functions mentioned interpolating corresponding functions polynomials however requires growing input dimension yield lower bound terms degree bound established holds networks number layers far know first lower bound result approximation polynomials computational point view simple class functions computed using basic operations addition multiplication polynomials also play important role approximation theory since dense class continuous functions approximation results neural networks rely approximability polynomials sigmoidal networks see obtain result introducing new method employs upper bounds vapnik chervonenkis dimension neural networks establish lower bounds network size first use vapnik chervonenkis dimension obtain lower bound due koiran calculated mentioned bound size sigmoidal networks boolean function koiran method developed extended maass using similar argument another combinatorial dimension papers derived lower bounds computation binary valued functions koiran inputs maass inputs present new technique show lower bounds obtained networks approximate continuous functions rests two fundamental results vapnik chervonenkis dimension neural networks one hand use constructions provided koiran sontag build networks large vapnik chervonenkis dimension consist gates compute certain arithmetic functions hand follow lines reasoning karpinski macintyre derive upper bound vapnikchervonenkis dimension networks estimates khovanskil result due warren following section give definitions sigmoidal networks vapnikchervonenkis dimension present lower bound result function approximation finally conclude discussion open questions schmitt sigmoidal neural networks vc dimension briefly recall definitions sigmoidal neural network vapnikchervonenkis dimension see consider feedforward neural networks certain number input nodes one output node nodes input nodes called computation nodes associated real number threshold edge labelled real number called weight computation network takes place follows input values assigned input nodes computation node applies standard sigmoid sum wxxx wrxr xx xr values computed node predecessors wr weights corresponding edges threshold output value network defined value computed output node common approximation results means neural networks assume output node linear gate outputs sum wxxx wrxr clearly computing functions finite sets output range output node may apply standard sigmoid well since sigmoidal function consider refer networks sigmoidal neural networks sigmoidal functions general need satisfy much weaker assumptions definition naturally generalizes networks employing types gates make use linear multiplication division gates vapnik chervonenkis dimension combinatorial dimension function class defined follows dichotomy set partition two disjoint subsets sx sx given set functions mapping dichotomy sx say induces dichotomy sx sx say shatters induces dichotomies vapnikchervonenkis vc dimension denoted vcdim defined largest number set elements shattered refer vc dimension neural network given terms feedforward architecture directed acyclic graph vc dimension class functions obtained assigning real numbers programmable parameters general weights thresholds network subset thereof assume output value network thresholded obtain binary values lower bounds network size present lower bound size sigmoidal networks required approximation polynomials first give brief outline proof idea define sequence univariate polynomials means show construct neural architectures af consist various types gates linear multiplication division gates particular gates compute polynomials architecture single weight programmable parameter weights thresholds fixed demonstrate assuming gates computing polynomials approximated sigmoidal neural networks sufficiently well architecture shatter certain set assigning suitable values programmable weight final step reason along lines karpinski macintyre obtain via khovanski estimates warren result upper bound vc dimension afn terms number computation nodes note cannot directly apply theorem since deal division gates comparing bound cardinality shattered set able complexity approximating continuous functions neural networks figure network values assigned input nodes xx xa respectively weight programmable parameter network conclude lower bound number computation nodes thus networks approximate polynomials let sequence polynomials inductively defined clearly uniquely defines every readily seen degree main lower bound result made precise following statement theorem sigmoidal neural networks approximate polynomials interval error lo norm must least computation nodes proof neural architecture af constructed follows network four input nodes xx xa figure shows network input values assigned input nodes order xa xx one weight consider programmable parameter afn associated edge outgoing input node denoted computation nodes partitioned six levels indicated boxes figure level network let us first assume sake simplicity computations real numbers exact three levels labeled input nodes one output node compute called projections ya levels labeled ps px one input node output nodes level ps receives constant input thus value parameter network define output values level denotes input value level pa value equal xx otherwise observe calculated schmitt therefore computations level implemented using gates computing function pn show afn shatter set cardinality let shown lemma exists pq andpq implies dichotomy every pk pj pi pk pj pi sx note pj pi na value computed given input values therefore choosing suitable value parameter afn network induce dichotomy words shattered shown lemma architecture weights chosen function fn computed network satisfies lim fn yx yn ya moreover architecture consists computation nodes linear multiplication division gates note size depend therefore choosing sufficiently small implement projections networks computation nodes resulting network still shatters computation nodes implementing three levels labeled ii level number computation nodes computing respectively assume computation nodes pn replaced sigmoidal networks inputs parameter values defined resulting network computes functions note computation nodes pn programmable parameters estimate size according theorem karpinski macintyre sigmoidal neural network programmable parameters computation nodes vc dimension ml generalize result slightly able apply readily seen proof theorem result also holds network additionally contains linear multiplication gates division gates derive bound taking account gate computing division say introduce defining equality new variable see proceed thus network programmable parameters computation nodes linear multiplication division sigmoidal gates vc dimension ml particular number computation nodes vc dimension hand shown shatter set cardinality since sigmoidal networks computing functions pn since number linear multiplication division gates bounded value single network computing pn must size least yields lower bound size sigmoidal network computing thus far assumed polynomials pn computed exactly sincb polynomials continuous functions since require calculated finite set input values resulting parameter values chosen shatter approximation polynomials sufficient straightforward analysis based fact output value network tolerance close shows pn approximated error complexity approximating continuous functions neural networks im norm resulting network still shatters set completes proof theorem statement previous theorem restricted approximation polynomials input domain however result immediately generalizes arbitrary interval moreover remains valid multivariate polynomials arbitrary input dimension corollary approximation polynomials degree sigmoidal neural networks approximation error loo norm requires networks size log holds polynomials number variables conclusions open questions established lower bounds size sigmoidal networks approximation continuous functions particular concrete class polynomials calculated lower bound terms degree polynomials main result already holds approximation univariate polynomials intuitively approximation multivariate polynomials seems become harder dimension increases therefore would interesting lower bounds terms degree input dimension result approximation error degree coupled naturally one would expect number nodes grow fixed function error decreases present know lower bound aimed calculating constants bounds practical applications values indispensable refining method using tighter results straightforward obtain numbers expect better lower bounds obtained considering networks restricted depth establish result introduced new method deriving lower bounds network sizes one main arguments use functions approximated construct networks large vc dimension method seems suitable obtain bounds also approximation types functions long computationally powerful enough moreover method could adapted obtain lower bounds also networks using activation functions general sigmoidal functions ridge functions radial basis functions may lead new separation results approximation capabilities different types neural networks order accomplished however essential requirement small upper bounds calculated vc dimension networks acknowledgments thank hans simon helpful discussions work supported part esprit working group neural computational learning ii neurocolt
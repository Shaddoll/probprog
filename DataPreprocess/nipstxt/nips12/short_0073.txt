abstract paper question necessity levels expert guided abstraction learning hard statistically neutral classification tasks focus two tasks date calculation parity claimed require intermediate levels abstraction must defined human expert challenge claim demonstrating empirically single hidden layer bp som network learn tasks without guidance moreover analyze network solution parity task show solution makes use elegant intermediary checksum computation introduction breaking complex task many smaller simpler subtasks facilitates solution task decomposition proved successful technique developing algorithms building theories cognition study modeling human problem solving process newell simon employed protocol analysis determine subtasks human subjects employ solving complex task even nowadays many cognitive scientists take task decomposition necessity explicit levels abstraction fundamental property human problem solving dennis norris modeling study problem solving capacity autistic savants case point study norris focuses date calculation task calculate day week given date fell autistic savants reported perform flawlessly attempt train multi layer neural network task norris failed get satisfactory level generalization performance decomposing task three sub tasks training separate networks sub tasks date calculation task could learned norris concluded date calculation task solvable learnable decomposed intermediary steps using human assistance date calculation task hard task inductive learning algorithms statistically neutral task conditional output probabilities input feature chance values solving task implies decomposing possible subtasks statistically neutral suggested decomposition date calculation task known date involves explicit assistance weijters bosch postma mfn som class elements class elements unlabelled element figure example bp som network human supervisor paper challenges decomposition assumption showing date calculation task learned single step appropriately constrained single hidden layer neural network addition another statistically neutral task called parity task given length bit string calculate whether number even odd investigated experimental study dehaene bossini giraux claimed humans decompose parity task first counting input string perform even odd decision study parity shown learnable network single hidden layer bp som give brief characterization functioning bp som details refer aim bp som learning algorithm establish cooperation bp learning som learning order find adequately constrained hidden layer representations learning classification tasks achieve aim traditional mfn architecture combined soms hidden layer mfn associated one som see figure training weights men corresponding som trained hidden unit activation patterns number training cycles bp som learning som develops twodimensional representation translated classification information som element provided class label one output classes task example let bp som network displayed figure trained classification task maps instances either output class three types elements distinguished som elements labelled class elements labelled class unlabelled elements winning class could found two dimensional representation som used addition standard bp learning rule classification reliability information soms included updating connection weights mfn error hidden layer vector accumulation error computed bp learning rule som error som error difference hidden unit activation vector vector best matching element associated class som important effect including som information error signals clusters hidden unit activation vectors instances associated class tend become increasingly similar top effect individual hidden unit activations tend become streamlined often end activations near one limited number discrete values learning statistically neutral tasks without expert guidance date calculation task first statistically neutral calculation task consider date calculation task determining day week given date fell instance october fell friday solving task requires algorithmic approach typically hard human calculators requires one intermediate steps generally assumed identity intermediate steps follows algorithmic solution although variations exist steps reportedly used human experts show explicit abstraction needed reviewing case necessity human assistance learning task date calculation expert based abstraction norris attempted model autistic savant date calculators using multi layer feedforward network mfn back propagation learning rule intended build model mimicking behavior autistic savant without need either develop arithmetical skills encode explicit knowledge regularities structure dates standard multilayer network trained backpropagation able solve date calculation task although network able learn examples used training manage generalize novel date day combinations second attempt norris split date calculation task three simpler subtasks networks using three stage learning strategy norris obtained nearly perfect performance training material performance test material errors almost exclusively made dates falling january february leap years concludes observation reason network able learn well human assistance addition norris claims even backpropagation net right number layers would way net distribute learning throughout net layer learned appropriate step computation date calculation without expert based abstraction demonstrate bp som learning rule single hidden layer feedforward network become successful date calculator experiment compares three types learning standard backpropagation learning bp backpropagation learning weight decay bpwd bp som learning norris used bp learning experiment leads overfitting considerably lower generalization accuracy new material compared reproduction accuracy training material bpwd learning included avoid overfitting parameter values bp including number hidden units task optimized performing pilot experiments bp optimal learning rate momentum values respectively bp bpwd bp som trained fixed number cycles rrt early stopping common method prevent overfitting used experiments bp bpwd bp som experiments bp som used interval dates used norris training test dates ranged january december generated two training sets consisting randomly selected instances one fifth dates also generated two corresponding test sets two validation sets instances new dates within year period experiments training set test set validation set weij ters bosch postma table average generalization performances plus standard deviation averaged ten experiments terms incorrectly processed training test instances bp bpwd bp som trained date calculation task parity task bp incorrect bpwd incorrect bp som incorrect task train test train test train test date calc parity empty intersections partitioned input three fields representing day month units month units year units output represented units one day week mfn contained one hidden layer hidden units bp hidden units bpwd bp som som bp som network contained elements three learning types tested two different data sets five runs different random weight initializations performed set yielding ten runs per learning type averaged classification errors test material reported table table follows average classification error bp high test instances bp yields classification error classification error bp training instances compared classification error bp classification errors training test material bpwd bp som much lower however bpwd generalization performance test material considerably worse performance training material clear indication overfitting note passing results bpwd contrast norris claim bp unable learn date calculation task decomposed subtasks inclusion weight decay bp sufficient good approximation performance results norris decomposed network results table also show performance bp som test material significantly better bpwd bp som learned date calculation task level well beyond average human date calculators reported norris contrast norris pre structured network bp som rely expert based levels abstraction learning date calculation task parity task parity problem starting xor problem parity continues relevant topic agenda many neural network machine learning researchers definition simple determine whether odd even number length bit string established state art algorithms backpropagation cannot learn even small backpropagation fails algorithms unable generalize learning instances parity task unseen new instances task date calculation due statistical neutrality task solution problem must lie comprehensive overview input values intermediary step odd even decision made indeed humans appear follow strategy learning statistically neutral tasks without expert guidance bp bpwd bp som figure graphic representation som associated bp trained mfn left bpwd trained mfn middle som associated bp som network right trained parity task analogous study date calculation task presented section apply bp bpwd bp som parity task selected training set contained different instances selected random set possible bit strings test set validation set contained new instances hidden layer mfn three algorithms contained hidden units som bp som contained elements algorithms run different random weight initializations table displays classification errors training instances test instances analysis results shows bp som performs significantly better bp bpwd test material respectively average error made bp som stems single experiment ten performing chance level remaining nine yielding error bp som able learn parity task quite accurately bp bpwd fail relatively consistent findings additional analysis investigated differences hidden unit activations training three learning algorithms visualize differences representations developed hidden layers mfns trained bp bpwd bp som also trained soms hidden layer activities trained bp bpwd networks left part figure visualizes class labelling som attached bp trained mfn training middle part visualizes som bpwd trained mfn right part displays som bp som network training material som bp som network much organized clustered soms corresponding bp trained bpwd trained mfns reliability values elements three soms represented width black white squares seen overall reliability degree clusteredness som bp som network considerably higher som bp trained bpwd trained mfns parity learned given hardness task supposed necessity expert guidance given bp som success learning parity contrast relevant analyze solution found bp som learning process subsection provide analysis show trained network performs elegant checksum calculation hidden layer intermediary step elements soms bp som networks trained parity task either prototype training instances labeled class weij ters bosch postma table list training instances parity task associated som elements trained bp som network som class even reliability inl inlo inll checksum som class odd reliability ins checksum som class even reliability checksum ii ii prototype instances non empty elements black white squares right part figure thus seen containers homogeneouslylabeled subsets training set fully reliable elements first step analysis consists collecting training non empty som element training instances clustered som element illustration table lists training instances clustered som elements coordinates first sight common property instances associated som element class belong instances som element even instances som element odd instances som element even second step analysis focuses sign weights connections input hidden units surprisingly find connections individual input unit hidden units sign input unit therefore labeled sign marker displayed bottom table allows clusteripg som become interpretable weights input unit units hidden layer negative weights input unit units hidden layer positive hidden layer information gathered checksum computed som element contains instances add identical checksum already seen using sign information rather specific weights instance instances clustered som element lead checksum sum taken product input values weight signs analogously instances cluster count instances cluster zero regularity present instances som elements sum bp som solution parity task interpreted transform hidden layer mapping different approximately discrete checksums either class even odd learning statistically neutral tasks without expert guidance conclusions performed two learning experiments bp sam learning algorithm trained date calculation task parity task tasks hard learn statistically neutral learned adequately without expert guidance bp sam learning algorithm effect san part bp sam adequately constrained hidden layer vectors reliable clustering vectors san streamlined hidden unit activations clearly contributes success results experiments date calculation task conclude norris claim without human assistance backpropagation net would never learn date calculation task inaccurate bp weight decay performs norris target level accuracy bp sam performs even better apparently bpsan able distribute learning throughout net two parts network input layer hidden layer hidden layer output layer perform mapping appropriate intermediary step parity experiment exemplified discovered intermediary step quite elegant consists computation checksum via connection weights input hidden layers unfortunately similar elegant simplicity found connection weights san clustering date calculation task future research aimed developing generic analyses trained bp son networks automatically discovered intermediary steps may made understandably explicit references newell simon human problem solving engelwood cliffs prentice hall norris build connectionist idiot savant cognition hill investigation calendar calculating idiot savant american journal psychiatry dehaene bossini giraux mental representation parity numerical magnitude journal experimental psychology general weijters van den bosch van den herik behavioural aspects combining backpropagation learning self organizing maps connection science rumelhart hinton williams learning internal representations error propagation rumelhart mcclelland eds parallel distributed processing explorations microstructure cognition volume foundations pp cambridge mit press kohonen elf organisation associative memory berlin springer verlag hinton learning distributed representations concepts proceedings eighth annual conference cognitive science society hillsdale erlbaum prechelt proben set neural network benchmark problems benchmarking rules technical report fakult fiir informatik universtrait karlsruhe germany quinlan programs machine learning san mateo ca morgan kaufmann thornton parity problem go away mccalla ed proceeding ai toronto canada pp berlin springer verlag
abstract replac commonli use gaussian nois model nonlinear regress flexibl nois model base student tdistribut degre freedom distribut chosen special case either gaussian distribut cauchi distribut realiz latter commonli use robust regress sinc distribut interpret infinit mixtur gaussian paramet hyperparamet degre freedom distribut learn data base em leam algorithm show model use distribut lead improv predictor real world data set particular outlier present distribut superior gaussian nois model effect adapt degre freedom system learn distinguish outlier non outlier especi onlin learn task one interest avoid inappropri weight chang due measur outlier maintain stabl onlin learn capabl show experiment use distribut nois model lead stabl onlin learn algorithm outperform state art onlin learn method like extend kalman filter algorithm introduct commonli use assumpt nonlinear regress target disturb independ addit gaussian nois although one deriv gaussian nois assumpt base maximum entropi approach main reason assumpt practic gaussian nois assumpt maximum likelihood paramet estim simpli found minim squar error despit common use far clear gaussian nois assumpt good choic mani practic problem reason approach therefor would nois distribut contain gaussian special case tunabl paramet allow flexibl distribut paper use student distribut nois model contain two free paramet degre freedom width paramet nice featur distribut degre freedom approach infin recov gaussian nois model obtain distribut heavi tail gaussian distribut includ cauchi nois model latter mckinsey compani inc briegel tresp commonli use robust regress first goal paper investig addit free paramet lead better gener perform real world data set compar gaussian nois assumpt oc common reason research depart gaussian nois assumpt presenc outlier outlier error occur low probabl gener data gener process subject identif gener problem mayb even one outlier high leverag suffici throw standard gaussian error estim complet track rousseeuw leroy second set experi therefor compar gener perform affect outlier gaussian nois assumpt distribut assumpt deal outlier often critic import onlin learn task onlin learn great interest mani applic exhibit non stationari behavior like track signal imag process navig fault detect see instanc nip sequenti learn workshop one interest avoid inappropri weight chanc due measur outlier maintain stabl onlin learn capabl outlier might result highli fluctuat weight possibl even instabl estim neural network weight vector onlin use gaussian error assumpt state art onlin algorithm like extend kalman filter instanc known nonrobust outlier meinhold singpurwalla sinc base gaussian output error assumpt paper organ follow section adopt probabilist view outlier detect take heavi tail observ error densiti student distribut deriv infinit mixtur gaussian approach work use multi layer perceptron mlp nonlinear model section deriv em algorithm estim mlp weight vector hyperparamet offiin employ state space represent model mlp weight evolut time extend batch algorithm section onlin learn case section applic comput effici fisher score algorithm lead posterior mode weight updat onlin em type algorithm approxim maximum likelihood ml estim hyperparamet last two section section section present experi conclus respect densiti robust error densiti assum nonlinear regress model th data point noisi target yt gener yt xt xt dimension known input vector wt denot neural network model character weight vector wt case multi layer perceptron mlp offiin case weight vector wt assum fix unknown constant vector wt furthermor assum vt uncorrel nois densiti pvt offiin case assum pvt independ pv follow assum student densiti degre freedom zlae cr immedi app ent recov heavi tail cauchi densiti obviou obtain gaussi densiti deriv em le ing rule next section import note denstiy ought infinit mixtur gaussian fore robust neural network regress offiin onlin learn boaton hous data addit outlier number outller figur left function gaussian densiti dash densiti degre freedom right mse boston hous data test set addit ouflier dash line show result use gaussian error measur continu line show result use student distribut error measur zl student densiti degre freedom width paramet zl gaussian densiti center varianc chi squar distribut degre freedom evalu compar differ nois model use evalu function defin huber logp oz neg score function nois densiti case sampl pfunction reflect influenc singl measur result estim assum gaussian measur error pv zlo deriv mean izl singl outlier infinit leverag estim contrast construct robust estim west state larg outlier influenc estim izl figur left show differ student distribut seen degre freedom determin much weight outlier obtain influenc regress particular finit influenc outlier izl approach zero robust offlin regress state equat densiti thought gener infinit mixtur gaussian maximum likelihood adapt paramet hyperparamet therefor perform use em algorithm lang et al th sampl complet data point would consist tripl xt yt ut first two known miss step estim everi data point index ld ld utlyt xt expect valu unknown ut given avail data xt yt yt xt ld old step weight hyperparamet optim use new argm eat ut xt briegel tresp new argm ax tv log tlog dg ld og wi digamma function dg oz note step onedimension nonlinem optim problem also note step weight mlp reduc weight least squme regress problem outlier tend weight except cours gaussi case wi te obtain equal weight robust onlin regress robust onlin regress assum model equat still valid chang time wt particular assum wt follow first order random walk normal distribut increment wtlwt wt qt wo normal distribut center ao covari clearli due nonlinear natur due fact nois process non gaussian fulli bayesian onlin algorithm linear case gaussian nois realiz use kalman filter clearli infeas hand consid data xt yt neg log posterior logp wrid paramet sequenc wr wo wt normal constant logp wr elogpv yt xt wt wo ao wo ao wt qt wt use appropri cost function deriv posterior mode estim weight sequenc two differ present last section first wt allow chang time second penalti term stem prior transit densiti includ penalti term penal rough weight sequenc lead smooth weight estim suitabl way determin stationari point logp wtit posterior mode estim wt appli fisher score current estim get better estim ew ld rf unknown weight sequenc wt solut pld ld neg score function wt gp wtjt owt expect inform matrix logp wtit owtow appli idea given fahrmeir kaufmann robust neural network regress tum solv comput invers expect inform matrix perform robust neural network regress offiin onlin learn choleski decomposit one forward backward pass set data note expect inform matrix posit definit block tridiagon matrix forward backward step iter obtain posterior mode estim wt onlin posterior mode smooth interest smooth backward filter step fisher score step appli sequenti posterior mode smoother time step map llt togeth step one predictor writ wt llt reason start valu obtain posterior mode smoother wt map time one reduc comput load limit backward pass slide time window last rt time step reason non stationari environ onlin purpos furthermor use underli assumpt case new measur yt chang estim drastic singl fisher score step often suffic obtain new posterior mode estim time result singl fisher score step algorithm lookback paramet rt fact one addit line code involv simpl matrix manipul compar onlin kalman smooth given pseudo code detail algorithm full descript found briegel tresp onlin singl fisher score step algorithm pseudo code repeat follow four step evalu step onepredictor writ perform forward recurs rt new data point yt arriv evalu corrector step totl perform backward smooth recurs ws lit ft adapt paramet distribut appli result fahrmeir kilnstier nonlinear assumpt use onlin em type algorithm approxim maximum likelihood estim hyperparamet vt assum scale factor cr degre freedom vt fix quantiti certain time window length vt deriv onlin em updat equat treat weight sequenc wt togeth mix variabl ut miss linear taylor seri expans oft ws fisher score solut walt approxim posterior expect posterior mode wdt posterior covari cov curvatur edt dt lt step somewhat lengthi deriv result approxim maximum likelihood updat rule similar given section detail onlin em type algorithm found briegel tresp experi experi real world data set first experi test studentt distribut use error measur real world data set train studentt distribut use degre freedom width paramet cr adapt use em updat rule section experi repeat time differ divis train test data comparison train neural network minim squar error cost function includ optim weight decay term test data set evalu perform use squar error cost function tabl provid experiment paramet give test set perform base repetit experi addit explain varianc defin percent mspe mspe mspe mean squar predict error use distribut mspe mean squar predict error use gaussian error measur furthermor suppli standard briegel tresp tabl experiment paramet test set perform real world data set data set input hidden train test add exp var std boston hous sunspot fraser river error base experi three experi network optim distribut nois model better network optim use gaussian nois model experi improv signific base pair test signific level result show clearli addit free paramet student distribut lead overfit use sensibl way system valu influenc extrem target valu figur show normal probabl plot clearli visibl deriv gaussian distribut extrem target valu also like remark appli preselect process choos particular data set indic non gaussian nois seem rule rather except real world data set madeel afro atmfxj gausam denati md obablti fmmf vor oo ooo figur normal probabl plot three train data set learn gaussian error measur dash line show expect normal probabl plot show clearli residu follow heavi tail distribut normal distribut experi outlier second experi want test approach deal outlier artifici ad data set start boston hous data set divid train test data randomli select subset train data set ad target uniformli gener real number interv figur right show mean squar error test set differ percentag ad outlier error bar deriv repetit experi differ divis train test set appar approach use distribut consist better network train base gaussian nois assumpt experi onlin learn third experi examin use distribut onlin learn data gener nonlinear map bsin first second third fourth set data point respect gaussian nois varianc ad train mlp hidden unit use first experi compar perform ekf algorithm singl fisher score step algorithm figur left show algorithm converg faster correct map also handl transit model paramet much better eke second experi probabl outlier uniformli drawn interv ad target figur middl show singl fisher score step algorithm use robust neural network regress offlin onlin learn distribut consist better algorithm use gaussian nois model ekf two plot right figur compar nonlinear map learn time step respect ek gf eio gf rs if mal ing aft mal ing aft oo ixx ixx figur left middl onlin mse set train data left compar extend kalman filter ekf dash singl fisher score step algorithm rt gf continu addit gaussian nois second figur show ekf dash dot fisher score gaussian error nois gf dash distribut error nois tf continu respect data addit outlier right true map continu ekf learn map dash dot tf map dash data set addit outlier conclus introduc student distribut replac standard gaussian nois assumpt nonlinear regress learn base em algorithm estim scale paramet degre freedom distribut result show use student distribut nois model lead better test error use gaussian nois assumpt real world data set result seem indic non gaussian nois rule rather except extrem target valu gener weight deal outlier particularli import onlin task outlier lead instabl adapt process introduc new onlin learn algorithm use distribut lead better stabl result compar extend kalman filter
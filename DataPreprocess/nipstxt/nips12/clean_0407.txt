abstract replace commonly used gaussian noise model nonlinear regression flexible noise model based student tdistribution degrees freedom distribution chosen special cases either gaussian distribution cauchy distribution realized latter commonly used robust regression since distribution interpreted infinite mixture gaussians parameters hyperparameters degrees freedom distribution learned data based em leaming algorithm show modeling using distribution leads improved predictors real world data sets particular outliers present distribution superior gaussian noise model effect adapting degrees freedom system learn distinguish outliers non outliers especially online learning tasks one interested avoiding inappropriate weight changes due measurement outliers maintain stable online learning capability show experimentally using distribution noise model leads stable online learning algorithms outperforms state art online learning methods like extended kalman filter algorithm introduction commonly used assumption nonlinear regression targets disturbed independent additive gaussian noise although one derive gaussian noise assumption based maximum entropy approach main reason assumption practicability gaussian noise assumption maximum likelihood parameter estimate simply found minimization squared error despite common use far clear gaussian noise assumption good choice many practical problems reasonable approach therefore would noise distribution contains gaussian special case tunable parameter allows flexible distributions paper use student distribution noise model contains two free parameters degrees freedom width parameter nice feature distribution degrees freedom approach infinity recover gaussian noise model obtain distributions heavy tailed gaussian distribution including cauchy noise model latter mckinsey company inc briegel tresp commonly used robust regression first goal paper investigate additional free parameters lead better generalization performance real world data sets compared gaussian noise assumption oc common reason researchers depart gaussian noise assumption presence outliers outliers errors occur low probability generated data generation process subject identification general problem maybe even one outliers high leverage sufficient throw standard gaussian error estimators completely track rousseeuw leroy second set experiments therefore compare generalization performance affected outliers gaussian noise assumption distribution assumption dealing outliers often critical importance online learning tasks online learning great interest many applications exhibiting non stationary behavior like tracking signal image processing navigation fault detection see instance nips sequential learning workshop one interested avoiding inappropriate weight chances due measurement outliers maintain stable online learning capability outliers might result highly fluctuating weights possible even instability estimating neural network weight vector online using gaussian error assumption state art online algorithms like extended kalman filter instance known nonrobust outliers meinhold singpurwalla since based gaussian output error assumption paper organized follows section adopt probabilistic view outlier detection taking heavy tailed observation error density student distribution derived infinite mixture gaussians approach work use multi layer perceptron mlp nonlinear model section derive em algorithm estimating mlp weight vector hyperparameters offiine employing state space representation model mlp weight evolution time extend batch algorithm section online learning case section application computationally efficient fisher scoring algorithm leads posterior mode weight updates online em type algorithm approximate maximum likelihood ml estimation hyperparameters last two sections section section present experiments conclusions respectively density robust error density assume nonlinear regression model th data point noisy target yt generated yt xt xt dimensional known input vector wt denotes neural network model characterized weight vector wt case multi layer perceptron mlp offiine case weight vector wt assumed fixed unknown constant vector wt furthermore assume vt uncorrelated noise density pvt offiine case assume pvt independent pv following assume student density degrees freedom zlae cr immediately app ent recover heavy tailed cauchy density obvious obtain gaussi density derivation em le ing rules next section important note denstiy ought infinite mixture gaussians fore robust neural network regression offiine online learning boaton housing data additive outliers number outller figure left functions gaussian density dashed densities degrees freedom right mse boston housing data test set additive oufliers dashed line shows results using gaussian error measure continuous line shows results using student distribution error measure zl student density degrees freedom width parameter zl gaussian density center variance chi square distribution degrees freedom evaluated compare different noise models useful evaluate function defined huber logp oz negative score function noise density case samples pfunction reflects influence single measurement resulting estimator assuming gaussian measurement errors pv zlo derive means izl single outlier infinite leverage estimator contrast constructing robust estimators west states large outliers influence estimator izl figure left shows different student distribution seen degrees freedom determine much weight outliers obtain influencing regression particular finite influence outliers izl approaches zero robust offline regression stated equation density thought generated infinite mixture gaussians maximum likelihood adaptation parameters hyperparameters therefore performed using em algorithm lange et al th sample complete data point would consist triple xt yt ut first two known missing step estimate every data point indexed ld ld utlyt xt expected value unknown ut given available data xt yt yt xt ld old step weights hyperparameters optimized using new argm eat ut xt briegel tresp new argm ax tv log tlog dg ld og wi digamma function dg oz note step onedimensional nonlinem optimization problem also note steps weights mlp reduce weighted least squmes regression problem outliers tend weighted exception course gaussi case wi te obtain equal weight robust online regression robust online regression assume model equation still valid change time wt particular assume wt follows first order random walk normally distributed increments wtlwt wt qt wo normally distributed center ao covariance clearly due nonlinear nature due fact noise process non gaussian fully bayesian online algorithm linear case gaussian noise realized using kalman filter clearly infeasible hand consider data xt yt negative log posterior logp wrid parameter sequence wr wo wt normalizing constant logp wr elogpv yt xt wt wo ao wo ao wt qt wt used appropriate cost function derive posterior mode estimate weight sequence two differences presentation last section first wt allowed change time second penalty terms stemming prior transition density included penalty terms penalizing roughness weight sequence leading smooth weight estimates suitable way determine stationary point logp wtit posterior mode estimate wt apply fisher scoring current estimate get better estimate ew ld rf unknown weight sequence wt solution pld ld negative score function wt gp wtjt owt expected information matrix logp wtit owtow applying ideas given fahrmeir kaufmann robust neural network regression tums solving compute inverse expected information matrix performed robust neural network regression offiine online learning cholesky decomposition one forward backward pass set data note expected information matrix positive definite block tridiagonal matrix forward backward steps iterated obtain posterior mode estimate wt online posterior mode smoothing interest smooth backwards filter step fisher scoring steps applied sequentially posterior mode smoother time step map llt together step one predictor writ wt llt reasonable starting value obtaining posterior mode smoother wt map time one reduce computational load limiting backward pass sliding time window last rt time steps reasonable non stationary environments online purposes furthermore use underlying assumption cases new measurement yt change estimates drastically single fisher scoring step often suffices obtain new posterior mode estimate time resulting single fisher scoring step algorithm lookback parameter rt fact one additional line code involving simple matrix manipulations compared online kalman smoothing given pseudo code details algorithm full description found briegel tresp online single fisher scoring step algorithm pseudo code repeat following four steps evaluate step onepredictor writ perform forward recursions rt new data point yt arrives evaluate corrector step totl perform backward smoothing recursions ws lit ft adaptation parameters distribution apply results fahrmeir kilnstier nonlinear assumptions use online em type algorithm approximate maximum likelihood estimation hyperparameters vt assume scale factors cr degrees freedom vt fixed quantities certain time window length vt deriving online em update equations treat weight sequence wt together mixing variables ut missing linear taylor series expansion oft ws fisher scoring solutions walt approximating posterior expectations posterior modes wdt posterior covariances cov curvatures edt dt lt step somewhat lengthy derivation results approximate maximum likelihood update rules similar given section details online em type algorithm found briegel tresp experiments experiment real world data sets first experiment tested studentt distribution useful error measure real world data sets training studentt distribution used degrees freedom width parameter cr adapted using em update rules section experiment repeated times different divisions training test data comparison trained neural networks minimize squared error cost function including optimized weight decay term test data set evaluated performance using squared error cost function table provides experimental parameters gives test set performance based repetitions experiments additional explained variance defined percent mspe mspe mspe mean squared prediction error using distribution mspe mean squared prediction error using gaussian error measure furthermore supply standard briegel tresp table experimental parameters test set performance real world data sets data set inputs hidden training test add exp var std boston housing sunspot fraser river error based experiments three experiments networks optimized distribution noise model better networks optimized using gaussian noise model experiments improvements significant based paired test significance level results show clearly additional free parameter student distribution lead overfitting used sensible way system value influence extreme target values figure shows normal probability plots clearly visible derivation gaussian distribution extreme target values also like remark apply preselection process choosing particular data sets indicates non gaussian noise seems rule rather exception real world data sets madeels afro atmfxj gausam denaty md obablty fmmf vor oo ooo figure normal probability plots three training data sets learning gaussian error measure dashed line show expected normal probabilities plots show clearly residuals follow heavy tailed distribution normal distribution experiment outliers second experiment wanted test approach deals outliers artificially added data set started boston housing data set divided training test data randomly selected subset training data set added targets uniformly generated real number interval figure right shows mean squared error test set different percentages added outliers error bars derived repetitions experiment different divisions training test set apparent approach using distribution consistently better network trained based gaussian noise assumption experiment online learning third experiment examined use distribution online learning data generated nonlinear map bsin first second third fourth set data points respectively gaussian noise variance added training mlp hidden units used first experiment compare performance ekf algorithm single fisher scoring step algorithm figure left shows algorithm converges faster correct map also handles transition model parameter much better eke second experiment probability outliers uniformly drawn interval added targets figure middle shows single fisher scoring step algorithm using robust neural network regression offline online learning distribution consistently better algorithm using gaussian noise model ekf two plots right figure compare nonlinear maps learned time steps respectively ek gfs eio gfs rs ifs mal ing afte mal ing afte oo ixx ixx figure left middle online mse sets training data left compare extended kalman filtering ekf dashed single fisher scoring step algorithm rt gfs continuous additive gaussian noise second figure shows ekf dashed dotted fisher scoring gaussian error noise gfs dashed distributed error noise tfs continuous respectively data additive outliers right true map continuous ekf learned map dashed dotted tfs map dashed data sets additive outliers conclusions introduced student distribution replace standard gaussian noise assumption nonlinear regression learning based em algorithm estimates scaling parameters degrees freedom distribution results show using student distribution noise model leads better test errors using gaussian noise assumption real world data set result seems indicate non gaussian noise rule rather exception extreme target values general weighted dealing outliers particularly important online tasks outliers lead instability adaptation process introduced new online learning algorithm using distribution leads better stable results compared extended kalman filter
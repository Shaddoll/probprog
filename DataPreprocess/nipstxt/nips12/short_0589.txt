abstract paper describes bidirectional recurrent mixture density networks model multi modal distributions type xtly xtlxl xt without explicit assumptions use context expressions occur frequently pattern recognition problems sequential data example speech recognition experiments show proposed generative models give higher likelihood test data compared traditional modeling approach indicating summarize statistical properties data better introduction many problems engineering interest formulated sequential data problems abstract sense supervised learning sequential data input vector dimensionality sequence xl xt xt living space mapped output vector dimensionality target sequence tl tt tt space often embodies correlations neighboring vectors xt xt tt tt general number training data sequence pairs input target used estimate parameters given model structure whose performance evaluated another set test data pairs many applications problem becomes predict best sequence given arbitrary input sequence best meaning sequence minimizes error using suitable metric yet defined making use theory pattern recognition problem often simplified treating sequence one pattern makes possible express objective sequence prediction well known expression arg maxy yix input sequence valid output sequence predicted sequence highest probability sample sequence training target data denoted output sequence general denoted live output space simplify notation random variables values denoted different symbols means schuster among possible sequences training sequence prediction system corresponds estimating distribution yix number samples includes defining appropriate model representing distribution estimating pa rameters training data maximized practice model consists several modules responsible different part yix testing usage trained system recognition given input sequence corresponds principally evaluation possible output sequences find best one procedure called search efficient implementation important many applications order build model predict sequences necessary decompose sequences modules responsible smaller parts build often used approach decomposition generative prior model part using bia aib bia arg maxp arg maxp xiy arg nax hp lx xt yr generative part prior part many applications approximated simpler expressions example first order markov model arg nax xtlyt ytlyt making simplifying approximations example every output yt depends previous output yt previous outputs ytlyl yt ytlyt inputs assumed statistically independent time xtlx xt xtly likelihood input vector xt given complete output sequence assumed depend output found ones xt xt yt assuming output sequences categorical sequences consisting symbols approximation derived expressions basis many applications example using gaussian mixture distributions model xt yt occuring symbols approach used sophisticated form stateof art speech recognition systems focus paper present models generative part need less assumptions ideally means able model directly expressions form xtlx xt possibly multi modal distribution vector conditioned previous vectors xt xt xl complete sequence shown next section athere distinction made probability mass density usually denoted respectively quantity model categorical probability mass assumed continuous probability density assumed bidireca onal recurrent mixture density networks mixture density recurrent neural networks assume want model continuous vector sequence conditioned sequence categorical variables shown figure one approach assume vector sequence modeled uni modal gaussian distribution constant variance making uni modal regression problem many practical examples assumption hold requiring complex output distribution model multi modal data one example attempt model sounds phoneroes based data multiple speakers certain phoneme sound completely different depending phonetic environment speaker using single gaussian constant variance would lead crude averaging examples traditional approach build generative models symbol separately suggested conventional gaussian mixtures used model observed input vectors parameters distribution means covariances mixture weights general change temporal position vector model within given state segment symbol bad representation data areas shown means bi modal looking distribution indicated two shown variances state used model speech procedure often used cope problem increase number symbols grouping often appearing symbol sub strings new symbol subdividing original symbol number states kkkeeeeeeeeeeiiiiiiiiiiikkkooooooooo kkkeeeeeeeeeeiiiiiiiiiiikkkoooooooooo figure conventional gaussian mixtures left mixture density brnns right multi modal regression another alternative explored parameters gaussian mixture distribution modeling continuous targets predicted one bidirectional recurrent neural network extended model mixture densities conditioned complete vector sequence shown right side figure another extension section architecture allows estimation time varying mixture densities conditioned hypothesized output sequence continuous vector sequence model exactly generative term without explicit approximations use context basics non recurrent mixture density networks mlp type found extension uni modal multi modal regression somewhat involved straightforward two interesting cases radial covariance matrix diagonal covariance matrix per mixture component trained gradientdescent procedures regular uni modal regression nns suitable equations calculate error back propagated found two cases mentioned derivation simple case conventional recurrent neural networks rnns model expressions form xtlyl yt distribution vector given input vector plus past input vectors bidirectional recurrent neural networks brnns simple schuster extension conventional rnns extension allows one model expressions form xtly distribution vector given input vector plus past following input vectors mixture density extension brnns liere two types extensions brnns mixture density networks considered extension model expressions type xtly multi modal distribution continuous vector conditioned vector sequence labeled mixture density brnn type ii extension model expressions type xtlx xt probability distribution continuous vector conditioned vector sequence previous context time xl xe xt architecture labeled mixture density brnn type ii first extension conventional uni modal regression brnns mixture density networks particularly difficult compared non recurrent implementation changes model multi modal distributions completely independent structural changes made form brnn second extension involves structural change basic brnn structure incorporate additional inputs shown figure neighboring xt xt incorporated adding additional set weights feed hidden forward states extended inputs targets outputs time step includes xt directly xt xt indirectly hidden forward neurons architecture allows one estimate generative term without making explicit assumptions since information xt conditioned theoretically available forward states backward states figure brnn mixture density extension type ii inputs striped outputs black hidden neurons grey additional inputs dark grey note without backward states additional inputs structure conventional rnn unfolded time different non recurrent mixture density networks extended brnns predict parameters gaussian mixture distribution conditioned vector sequence rather single vector time position one parameter set means variances actually standard variations mixture weights conditioned brnn type xt brnn type ii bidirectional recurrent mixture density networks experiments results goal experiments show proposed models suitable model speech data traditional approaches rely fewer assumptions speech data used observation vector sequences representing original waveform compressed form vector mapped exactly one phonemes three approaches compared allow estimation likelihood xly various degrees approximations conventional gaussian mixture model xiy ltt xtlyt according likelihood phoneme class vector approximated conventional gaussian mixture distribution separate mixture model built estimate xly possible categorical states case two assumptions necessary variance radial covariance matrix diagonal single variance vector components chosen match conditions brnn cases number parameters complete model km several models different complexity trained table mixture density brnn lp xt one mixture density brnn type number mixture components radial covariance matrix output distribution approach trained presenting complete sample sequences note type possible context dependencies assumption automatically taken care probability conditioned complete sequences sequence contains information neighboring phonemes also position frame within phoneme conventional systems modeled crudely introducing certain number states per phoneme number outputs network depends number mixture components total number parameters adjusted changing number hidden forward backward state neurons set mixture density brnn ii xiy one mixture density brnn type ii number mixture components radial covariance matrix trained conditions note case assumptions taken care exactly expressions required form modeled mixture density brnn type ii experiments recommended training test data timit speech database used experiments timit database comes hand aligned phonetic transcriptions utterances transformed sequences categorical class numbers training test vec number possible categorical classes number phonemes categorical data input data brnns represented dimensional vectors kth component one others zero feature extraction waveforms resulted vector sequences xl model done speech recognition systems variances normalized respect training data radial variance mixture component model reasonable choice schuster three model types trained conventional gaussian mixture model also mixture components number resulting parameters used rough complexity measure models shown table states triphone models clustered table number parameters different types models mixture mono mono tri brnn brnn ii components state state state training conventional approach using mixtures gaussians done using em algorithm classes samples reduced reach stationary point likelihood training brnns types must done using gradient descent algorithm modified version rprop used detail described measure used comparing tested approaches log likelihood training test data given models built training data absence search algorithm perform recognition valid measure evaluate models since maximizing log likelihood training data objective model types note given alignment vectors phoneme classes test data used calculating log likelihood test data theie search best alignment results figure shows average log likelihoods depending number mixture components tested approaches training upper line test data lower line baseline state monophones give lowest likelihood state monophones slightly better larger gap training test data likelihood comparison training data system distinct triphones states trained also note system lot parameters brnn systems see table compared results traditional gaussian mixture systems show models become better building detailed models different phonetic context using states context classes mixture density brnn type gives higher likelihood traditional gaussian mixture models expected brnn type models contrast traditional gaussian mixture models able include possible phonetic context effects removing assumption frame certain phoneme surrounded frames phonemes theoretically restriction range contextual influence mixture density brnn type ii addition removes independence assumption gives significant higher likelihood models note difference likelihood training test data model small indicating useful model underlying distribution data bidirectional recurrent mixture density networks mono state train mono lstate test mono state trajn mono state test tri state train brnn trajn brni test brnn train brnl test number gaussian mixture components figure mixture density brnns multi modal regression results conclusions mixture density brnns allow one model probabilistic expressions frequently occurring sequence processing problems less assumptions traditionally necessary shown model statistical properties speech data better traditional approach using gaussian mixture models making mixture density brnns approximations potential candidates improved speech recognition coding synthesis many issues covered paper space limitations detailed description models found references bishop mixture density networks technical report ncrg neural computing research group aston university birmingham england bishop neural networks pattern recognition clarendon press oxford england linguistic data consortium timit acoustic phonetic continuous speech corpus http morph ldc upenn edu catalog ldc html riedmiller braun direct adaptive method faster back propagation learning rprop algorithm proceedings ieee international conierence neural networks pages schuster paliwal bidirectional recurrent neural networks ieee transactions neural networks schuster supervised learning tom sequential data applications speech recognition phd thesis nara institute science technology nara japan http isw aist nara ac jp shikano lab database library paper dt ps gz young review large vocabulary speech recognition ieee signal processing magazine
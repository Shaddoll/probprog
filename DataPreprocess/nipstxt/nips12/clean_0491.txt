abstract latent variable generative model finite noise used describe several different algorithms independent components analysis ica particular fixed point ica algorithm shown equivalent expectation maximization algorithm maximum likelihood certain constraints allowing conditions global convergence elucidated algorithms also explained generic behavior near singular point size optimal generarive bases vanishes expansion likelihood singular point indicates role higher order correlations determining features discovered ica application convergence algorithms demonstrated simple illustrative example introduction independent components analysis ica generated much recent theoretical practical interest successes number different signal processing problems ica attempts decompose observed data components statistically independent possible viewed nonlinear generalization principal components analysis pca applications ica include blind separation audio signals beamforming radio sources discovery features biomedical traces also number approaches deriving algorithms ica fundamentally consider problem recovering independent source signals observations xi wijsj wij mixing matrix number sources greater dimcnsionality observations thus columns represent different independent features present observed data bell scjnowski formulated infomax algorithm ica maximizing mutual information data nonlinearly transformed version data lee rokni sornpolinsky covariant version algorithm uses natural gradient mutual information iteratively update estimate demixing matrix terms estimated components ix nonlinearity differentiates features learned infomax ica algorithm found conventional pca fortunately exact form nonlinearity used eq crucial success algorithm long preserves sub gaussian super gaussian nature sources another approach ica due hyvarinen oja derived maximizing objective functions motivated projection pursuit fixed point ica algorithm attempts self consistently solve extremum nonlinear objective function simplest formulation considers single source mixing matrix single vector constrained unit length iw assuming data first preprocessed whitened fixed point ica algorithm iteratively updates estimate follows wtx nonlinear function ag constant given integral gaussian ag drle gt rl fixed point algorithm extended arbitrary number sources using eq serial deflation scheme alternatively columns mixing matrix updated simultaneously orthogonalizing matrix zg wrz xow assumption observed data match underlying ica model ws shown fixed point algorithm converges locally correct solution least quadratic convergence however global convergence generic fixed point ica algorithm uncertain contrast gradient based infomax algorithm whose convergence guaranteed long sufficiently small step size chosen paper first review latent variable generative model framework independent components analysis consider generarive model presence finite noise show fixed point ica algorithm related expectationmaximization algorithm maximum likelihood allows us elucidate conditions fixed point algorithm guaranteed globally converge assuming data indeed generated independent components derive optimal parameters convergence also investigate optimal size ica mixing matrix varies function added noise demonstrate presence singular point expanding likelihood singular point behavior ica algorithms related higher order statistics present data finally illustrate application convergence ica algorithms artificial data generative model convenient method interpreting different ica algorithms terms hidden latent variable generarive model shown fig hidden variables ica algorithms higher order statistics hidden variables weights noi wse visible variables figure generative model ica algorithms hidden variables cr additive gaussian noise terms ws cr visible variables correspond different independent components assumed factorized non gaussian prior probability distribution sj hidden variables instantiated visible variables generated via linear mapping generarive weights ii exp variance gaussian noise added visible variables probability data given model calculated integrating possible values hidden variables ds ls ws limit added noise vanishes previously shown maximizing likelihood eq equivalent infomax algorithm eq following analysis consider situation iance noise nonzero expectation maximization assume data initially preprocessed spherized xixj ij unfortunately finite noise arbitrary prior deriving learning rule closed form analytically intractable however becomes possible derive simple expectation maximization em learning rule constraint wo worwo implies orthogonal length individual columns indeed data obeys ica model ws shown optimal must satisfy orthogonality condition assuming constraint eq arbitrary data posterior distribution slx becomes conveniently factorized vtx jsj slx exp sj ff sj lee rokni sornpolinsky step factorized form allows expectation function ds wtx analytically evaluated expectation used step find new estimate xg wrx asw symmetric matrix lagrange multipliers constrain new orthogonal eq easily solved taking reduced singular value decomposition rectangular matrix udv xg wtx utu vv diagonal mr mr matrix solution em estimate mixing matrix given uv udu specific example consider following prior binary hidden variables case expectation ds slx tanh wrx cr em update rule given orthogonalizing matrix xtanh wtx fixed point ica besides presence linear term agw eq em update rule looks much like fixed point ica algorithm turns without linear term convergence naive em algorithm much slower eq show possible interpret role linear term fixed point ica algorithm within framework generatire model suppose distribution observed data actually mixture isotropic distribution non isotropic distribution pd ozpo isotropic part break rotational symmetry affect choice directions learned basis thus efficient apply learning algorithm non isotropic portion distribution oc pd po rather whole observed distribution pd applying em pl results correction term arising subtracted isotropic distribution correction em update becomes xg wtx gw equivalent fixed point ica algorithm unfortunately clear compute appropriate value use fitting data taking small value result learning rule similar naive em update rule implies algorithm guaranteed monotonically converge albeit slowly local maximum likelihood hand choosing large value result subtracted probability density negative everywhere case algorithm converge slowly local minimum likelihood fixed point algorithm operates intermediate regime algorithm likely converge rapidly however also situation subtracted density could positive negative regions algorithm longer guaranteed converge ica algorithms higher order statistics noise figure size optimal generative bases function added noise showing singular point behavior around optimal value order determine optimal value ct make assumption observed data obeys ica model note statistics sources data need match assumed prior distribution sources generarive model eq assumption related mixture assumption eq easy show fixed point algorithm analyzing behavior algorithm vicinity fixed point simple expression emerges change deviations fixed point single iteration eq xo averaging true source distribution assumed simplicity identical sources us algorithm converges rapidly one chooses local convergence cubic eq one show condition stability fixed point given ac thus stability criterion eq equivalent sg cubic nonlinearity implies algorithm find true independent features source distribution positive kurtosis singular point expansion let us consider optimal size weights varies function noise parameter cr small weights approximately described infomax algorithm eq lengths columns unity order match covariance data large cr however optimal size weights small covariance noise already larger data fact factor analysis special case generative model eq shown weights exactly zero thus size optimal generarive weights varies shown qualitatively fig certain critical noise value weights exactly equal lee rokni sornpolinsky iteration figure convergence modified em algorithm function tanh nonlinearity likelihood ln cosh wtx plotted function iteration number optimal basis plotted two dimensional data distribution likelihood maximized top minimized bottom zero critical value weights become nonzero expand likelihood generafive model vicinity singular point expansion well behaved size generative weights acts small perturbative parameter expansion log likelihood model around singular value tr ww kurt sm xixjxt vimwjml vkmwlm ijklm given kurt sm represents kurtosis prior distribution hidden variables note expansion valid symmetric prior differs expansions assume small deviations gaussian prior eq shows importance fourth order cumulant observed data breaking rotational degeneracy weights generic behavior ica manifest optimizing cumulant term eq depends crucially sign kurtosis used prior example artificial data illustration convergence algorithm eq consider simple two dimensional uniform distribution xl vf xl otherwise tanh nonlinearity fig shows overall likelihood converges different values parameter algorithm iterated algorithm converges maximum likelihood fastest convergence opt however algorithm converges minimum likelihood intermediate value likelihood converge fluctuating wildly maximum minimum likelihood solutions maximum ica algorithms higher order statistics likelihood solution shows basis vectors aligned sides square distribution whereas minimum likelihood solution basis aligned diagonals solutions also understood maximizing minimizing kurtosis terms eq discussion utility latent variable generative model demonstrated deriving algorithms ica constraining generarive weights orthogonal em algorithm analytically obtained interpreting data fitted mixture isotropic non isotropic parts simple correction em algorithm derived certain conditions modified algorithm equivalent fixed point ica algorithm converges much rapidly naive em algorithm optimal parameter convergence derived assuming data consistent ica generative model also exists critical value noise parameter generarive model controlled expansion likelihood possible expansion makes clear role higher order statistics determining generic behavior different ica algorithms acknowledge support bell laboratories lucent technologies us israel binational science foundation israel science foundation also thank hagai attias simon haykin juha karhunen te lee erkki oja sebastian seung boris shraiman oren shriki helpful discussions
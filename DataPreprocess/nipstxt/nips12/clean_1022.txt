abstract propose new approach problem searching space stochastic controllers markov decision process mdp partially observable markov decision process pomdp following several authors approach based searching parameterized families policies example via gradient descent optimize solution quality however rather trying estimate values derivatives policy directly indirectly using estimates probability densities policy induces states different points time enables algorithms exploit many techniques efficient robust approximate density propagation stochastic systems show techniques applied deterministic propagation schemes mdp dynamics given explicitly compact form stochastic propagation schemes access generative model simulator mdp present empirical results variants complex problems introduction recent years growing interest algorithms approximate planning exponentially even infinitely large markov decision processes mdps partially observable mdps pomdps large domains value functions sometimes complicated difficult approximate even though may simple compactly representable policies perform well observation led particular interest direct policy search methods attempt choose good policy restricted class ii policies setting ii ro class policies smoothly parameterized value ro differentiable gradient ascent methods may used find locally optimal however estimating values ro associated gradient often far trivial one simple method estimating ro value involves executing one monte carlo trajectories using ro taking average empirical return cleverer algorithms executing single trajectories also allow gradient estimates methods become standard approach policy search sometimes work fairly well paper propose somewhat different approach value gradient estimation problem rather estimating quantities directly estimate probability density states system induced ro different points time time slice policy search via density estimation densities completely determine value policy density estimation easy problem utilize existing approaches density propagation allow users specify prior knowledge densities also shown theoretically empirically provide robust estimates time slice densities show direct policy search implemented using approach two different settings planning problem first access explicit model system dynamics allowing us provide explicit algebraic operator implements approximate density propagation process second access generative model dynamics allows us sample provide explicit representation next state distributions show techniques combined gradient ascent order perform policy search somewhat subtle argument case sampling based approach also present empirical results variants complex domains problem description markov decision process mdp tuple possibly infinite set states start state finite set actions reward function trl transition model gives probability landing state upon taking action state stochastic policy map aa probability taking action state many ways defining policy quality value horizon discount factor finite horizon discounted value function vt defined vt ea es infinite state space summation replaced integral define several optimality criteria finite horizon total reward horizon vy vy infinite horizon discounted reward discount limr vt infinite horizon average reward limt vt assume limit exists fix optimality criterion goal find policy high value discussed assume restricted set ii policies wish select good ii assume ii set policies parameterized continuously differentiable simple example may one dimensional state two action mdp sigmoidal probability choosing action ao state ro ao exp note framework also encompasses cases family ii consists policies depend certain aspects state particular pomdps restrict attention policies depend observables restriction results subclass stochastic memory free policies introducing artificial memory bits process state also define stochastic limited memory policies value ro specified find best policy ii search maximizes compute approximate many algorithms used find local maximum nemer mead simplex search confused simplex algorithm linear programs require ability evaluate function optimized point compute estimate gradient respect also use variety deterministic stochastic gradient ascent methods write rewards rather assume single start state rather initial state distribution simplify exposition several minor extensions trivial ng parr koller densities value functions optimization algorithms require method computing sometimes also gradient many real life mdps however exactly completely infeasible due large even infinite number states consider approach estimating quantities based density based reformulation value function expression policy induces probability distribution states time letting initial distribution giving probability define time slice distributions via recurrence easy verify standard notions value defined earlier reformulated terms vr dot product operation equivalently expectation respect somewhat subtly case infinite horizon average reward vg limiting distribution one exists reformulation gives us alternative approach evaluating value policy ro first compute time slice densities use compute value unfortunately modification resolve difficulty representing computing probability densities large infinite spaces often easier representing computing value functions however several results indicate representing computing high quality approximate densities may often quite feasible general approach approximate density propagation algorithm using time slice distributions restricted family example continuous spaces might set multivariate gaussians approximate propagation algorithm modifies equation maintain time slice densities precisely policy ro view defining operator takes one distribution returns another current policy roo rewrite cases closed approximate density propagation algorithms use alternative operator properties also hopefully close use denote approximation denote selected carefully often case close indeed standard contraction analysis stochastic processes used show proposition assume exists constant cases might arbitrarily small case proposition meaningless however many systems reasonable independent furthermore empirical results also show approximate density propagation often track exact time slice distributions quite accurately approximate tracking applied planning task given optimality criterion expressed define approximation replacing accuracy guarantees approximate tracking induce comparable guarantees value approximation guarantees performance policy found optimizing also possible proposition assume fixed ivt policy search via density estimation proposition let argmaxo viol argmaxo maxo iv differentiating approximate densities section discuss two different techniques maintaining approximate density using approximate propagation operator show combined gradient ascent perform policy search general assume family distributions parameterized example set dimensional multivariate gaussians diagonal covariance matrices would dimensional vector specifying mean vector covariance matrix diagonal consider task gradient ascent space policies using optimality criterion say rt differentiating relative get rr avoid introducing new notation also use denote aset sociated vector parameters parameters function hence internal gradient term represented jacobian matrix entries representing derivative parameter relative parameter oj gradient computed using simple recurrence based chain rule derivatives oo first summand jacobian derivative transition operator relative policy parameters second product two terms derivative relative distribution parameters result previous step recurrence deterministic density propagation consider transition operator simplicity omit dependence idea approach try get close possible subject constraint specifically define projection operator takes distribution returns distribution closest sense define order ensure gradient descent applies setting need ensure differentiable functions clearly many instantiations idea assumption holds provide two examples consider continuous state process nonlinear dynamics mixture conditional linear gaussians define set multivariate gaussians operator takes distribution mixture gaussians computes mean covariance matrix easily computed parameters using simple differentiable algebraic operations different example algorithm approximate density propagation dynamic bayesian networks dbns dbn structured representation stochastic process exploits conditional independence properties distribution allow compact representation dbn state space defined set possible assignments set random variables xn transition model described using bayesian network fragment nodes xi node xi represents represents nodes xi network forced roots parents associated conditional probability distributions node associated conditional probability distribution cpd specifies parents transition probability xt defined ng parr koller parents dbns support compact representation complex transition models mdps extend dbn encode behavior mdp stochastic policy introducing new random variable representing action taken current time parents variables state action allowed depend cpd may compactly represented function approximation distribution actions defined different contexts discrete dbns number states grows exponentially number state variables making explicit representation joint distribution impractical algorithm defines set distributions defined compactly set marginals smaller clusters variables simplest example set distributions independent parameters defining distribution parameters multinomials projection operator simply marginalizes distributions onto individual variables differentiable one useful corollary analysis decay rate structured often much higher decay rate multiple applications converge rapidly stationary distribution property useful approximating optimize relative stochastic density propagation many settings assumption direct access strong weaker assumption access generative model black box generate samples appropriate distribution generate samples case use different approximation scheme based operator stochastic operator takes distribution generates number random state samples si si action generate sample transition distribution si sample si ai assigned weight wi ro ai si compensate fact actions would selected ro equal probability resulting set samples weighted wis given input statistical density estimator uses estimate new density assume density estimation procedure differentiable function weights often reasonable assumption clearly used compute thereby approximate ro value however gradient computation far trivial particular compute derivative must consider behavior perturbed one say applied originally case entirely different set samples would probably generated possibly leading different density hard see one could differentiate result perturbation propose alternative solution based importance sampling rather change samples modify weights reflect change probability would generated specifically fitting define sample si ai weight compute derivatives respect parameters required let vector parameters using chain rule first term derivative estimated density relative sample weights matrix second derivative weights relative parameter vector jacobian easily computed policy search via density estimation actual avg cost estimatedavg cost function evaluations figure driving task dbn model policy search optimization results experimental results tested approach two different domains first average reward dbn mdp problem shown figure task find policy changing lanes driving moderately busy two lane highway slow lane fast lane model based bat dbn result separate effort build good model driver behavior simplicity assume car speed controlled automatically concerned choosing lateral action change lane drive straight observables shown figure lclr rclr clearance next car lane close medium far agent pays cost step blocked meaning driving close car front pays penalty per step staying fast lane policies specified action probabilities possible observation combinations since reasonably small number parameters used simplex search algorithm described earlier optimize process mixed quite quickly fairly good approximation used fully factored representation joint distribution except single cluster three observables evaluations averages monte carlo trials steps figure shows estimated actual average rewards policy parameters evolved time algorithm improved quickly converging natural policy car generally staying slow lane switching fast lane necessary overtake second experiment used bicycle simulator actions corresponding leaning leftycenter right applying negative zero positive torque handlebar six dimensional state used includes variables bicycle tilt angle orientation handlebar angle bicycle tilt exceeds falls enters absorbing state used policy search following space selected twelve simple manually chosen fine tuned features state actions chosen softmax probability taking action ai exp wi exp problem comes generative model complicated nonlinear noisy bicycle dynamics used stochastic density propagation version algorithm stochastic gradient ascent distribution mixture singleton point consisting absorbing state multivariate gaussian ng parr koller first task domain balance reliably bicycle using horizon discount si samples per density propagation step quickly achieved next trying learn ride goal radius away also succeeded finding policies reliably formal evaluation difficult sufficiently hard problem even finding solution considered success also slight parameter sensitivity best results obtained picked fit care using part data earlier less successful trials representative fairly good rider state distribution using algorithm able obtain solutions median riding distances km goal significantly better results obtained learning rather planning setting using value function approximation solution reported much larger riding distances goal km single best ever trial km conclusions presented two new variants algorithms performing direct policy search deterministic stochastic density propagation settings empirical results also shown methods working well two large problems acknowledgements warmly thank kevin murphy use help bayes net toolbox jette rand preben alstr use bicycle simulator ng supported berkeley fellowship work koller parr supported aro muri program integrated approach intelligent systems darpa contract daca subcontract iet inc onr contract darpa hpkb program sloan foundation powell foundation
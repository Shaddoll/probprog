abstract present algorithm infers model structure mixture factor analysers using efficient deterministic variational approximation full bayesian integration model parameters procedure automatically determine optimal number components local dimensionality component number factors factor analyser alternatively used infer posterior distributions number components dimensionalities since parameters integrated method prone overfitting using stochastic procedure adding components possible perform variational optimisation incrementally avoid local maxima results show method works well practice correctly infers number dimensionality nontrivial synthetic examples importance sampling variational approximation show obtain unbiased estimates true evidence exact predictive density kl divergence variational posterior true posterior model variational approximations general introduction factor analysis fa method modelling correlations multidimensional data model assumes dimensional data vector generated first linearly transforming dimensional vector unobserved independent zero mean unit variance gaussian sources adding dimensional zeromean gaussian noise vector diagonal covariance matrix ax integrating marginal density gaussian zero mean covariance aa matrix known factor loading matrix given data sample covariance matrix factor analysis finds optimally fit maximum likelihood sense since single factor analyser seen reduced parametrisation full covariance gaussian factor analysis relationship principal components analysis pca mixture models reviewed ghahramani beal mixture factor analysers mfa models density weighted average factor analyser densities yla sl yls vector mixing proportions discrete indicator variable factor loading matrix factor analyser includes mean vector exploiting factor analysis parameterisation covariance matrices mixture factor analysers used fit mixture gaussians correlated high dimensional data without requiring parameters undesirable compromises axis aligned covariance matrices mfa gaussian cluster intrinsic dimensionality ks dimensions allowed vary across clusters consequently mixture factor analysers simultaneously addresses problems clustering local dimensionality reduction multiple identity model becomes mixture probabilistic pcas tractable maximum likelihood procedure fitting mfa mpca models derived expectation maximisation algorithm maximum likelihood ml approach mfa easily get caught local maxima ueda et al provide effective deterministic procedure avoiding local maxima considering splitting factor analyser one part space merging two another part splits merges considered simultaneously number factor analysers stay since adding factor analyser always expected increase training likelihood fundamental problem maximum likelihood approaches fail take account model complexity cost coding model parameters complex models penalised leads overfitting inability determine best model size structure distributions thereof without resorting costly cross validation procedures bayesian approaches overcome problems treating parameters unknown random variables averaging ensemble models define evidence data set yn integrating parameters penalises models degrees freedom since models priori model larger range data sets information inferred data parameters captured posterior distribution oiy rather ml point estimate bayesian theory deals problems overfitting model selection averaging practice often computationally analytically intractable perform required integrals gaussian mixture models markov chain monte carlo mcmc methods developed approximate integrals sampling main criticism mcmc methods slow technically log likelihood bounded constraints put determinant component covariances real ml objective mfa find highest finite local maximum likelihood awe sometimes use refer parameters sometimes unknown quantities parameters hidden variables formally difference two number hidden variables grows whereas number parameters usually variational inference bayesian mixtures factor analysers usually difficult assess convergence furthermore posterior density parameters stored set samples inefficient another approach bayesian integration gaussian mixtures laplace approximation makes local gaussian approximation around maximum posteriori parameter estimate approximations based large data limits poor particularly small data sets principle advantages bayesian integration ml largest local gaussian approximations also poorly suited bounded positive parameters mixing proportions mixture model finally difficult see approach applied online incremental changes model structure paper employ third approach bayesian inference variational approximation form lower bound log evidence using jensen inequality seek maximise maximising equivalent minimising kldivergence oiy tractable used approximation intractable posterior approach draws roots one way deriving mean field approximations physics used recently bayesian inference variational method several advantages mcmc laplace approximations unlike mcmc convergence assessed easily monitoring approximate posterior encoded efficiently unlike laplace approximations form tailored parameter fact optimal form parameter falls optimisation approximation global optimises objective function variational methods generally fast guaranteed increase monotonically transparently incorporates model complexity knowledge one done full bayesian analysis mixtures factor analysers course vis vis mcmc main disadvantage variational approximations guaranteed find exact posterior limit however straightforward application sampling possible take result variational optimisation use sample exact posterior exact predictive density described section remainder paper first describe mixture factor analysers detail section derive variational approximation section show empirically model infer number components intrinsic dimensionalities prone overfitting section finally conclude section model starting evidence bayesian mfa obtained averaging likelihood priors parameters hyperparameters ghahramani beal hyperparameters precision parameters inverse variances columns conditional independence relations variables model shown tation figure figure generative model variational bayesian mixture factor analysers circles denote random variables solid rectangles denote hyperparameters dashed rectangle shows plate repetitions data graphically usual belief network represenwhile arbitrary choices could made priors first line choosing priors conjugate likelihood terms second line greatly simplifies inference interpretability choose rla symmetric dirichlet conjugate multinomial sl prior factor loading matrix plays key role model component mixture gaussian prior vs element vector precision column one precisions outgoing weights factor xt go zero allows model reduce intrinsic dimensionality data warrant added dimension method intrinsic dimensionality reduction used bishop bayesian pca closely related mackay neal method automatic relevance determination ard inputs neural network avoid overfitting important integrate parameters whose cardinality scales model complexity number components dimensionalities therefore also integrate precisions using gamma priors la variational approximation applying jensen inequality repeatedly log evidence lower bound using following factorisation distribution parameters hidden variables given factorisation several additional factorisations fall conditional independencies model resulting variational objective function sn sn xn dxnq xn xnlsn dasq dxnq xnlsn lnp ynlxn sn variational posteriors given appendix derived performing free form extremisation difficult show extrema indeed maxima optimal posteriors conjugate forms priors model hyperparameters govern priors estimated fashion see appendix currently integrate although also done conjugate priors effect pseudo observations variational inference bayesian mixtures factor analysers birth death optimising jr occasionally one finds zero responsibility components result insufficient support local data overcome dimensional complexity prior factor loading matrices components mixture die natural causes longer needed removing redundant components increases component birth happen spontaneously introduce heuristic whenever stabilised pick parent component stochastically probability proportional attempt split two specific contribution jr last bracketed term normalised sn works better cycling components picking random concentrates attempted births components faring poorly parameter distributions two gaussians created split initialised partitioning responsibilities data along direction sampled parent distribution usually causes decrease monitoring future progress reject attempted birth recover although perfectly possible start model many components let die computationally efficient start one component allow spawn necessary exact predictive density true evidence kl importance sampling variational approximation obtain unbiased estimates three important quantities exact predictive density true log evidence kl divergence variational posterior true posterior letting sample oi sample instance mixture factor analysers predictive density given weight predictive densities importance weights wi oi oi easy evaluate results mixture mixtures factor analysers converge exact predictive density yly long wherever true log evidence similarly estimated ln denotes averaging importance samples finally kl divergence given kl oiy ln ln procedure three significant properties first importance weights used estimate three quantities second importance sampling work poorly high dimensions ad hoc proposal distributions variational optimisation used principled manner pick good approximation therefore hopefully good proposal distribution third procedure applied variational approximation detailed exposition found results experiment discovering number components tested model synthetic data generated mixture gaussians points per cluster figure top left variational algorithm little difficulty finding correct number components birth heuristics successful avoiding local maxima finding gaussians repeated splits attempted rejected finding distribution number components using also simple experiment shrinking spiral used dataset data points shrinking spiral another test well algorithm could ghahramani beal figure top exp frames left right data gaussian ellipses accepted births bottom exp shrinking spiral data gaussian ellipses accepted births note number gaussians increases left right points per cluster intrinsic dimensionalities ii ii figure left exp function iteration spiral problem typical run drops constitute component births thick lines accepted attempts thin lines rejected attempts middle exp means factor loading matrices results analogous given bishop bayesian pca right exp table learned number gaussians dimensionalities training set size increases boxes represent model components capture several clusters escape local maxima robust initial conditions figure bottom local maxima pose problem algorithm always found gaussians regardless whether initialised runs took minutes mhz alpha ev processor plot shows compute time spent accepted moves figure left experiment discovering local dimensionalities generated synthetic data set data points gaussians intrinsic dimensionalities embedded dimensions variational bayesian approach correctly inferred number gaussians intrinsic dimensionalities figure middle varied number data points found expected fewer points data could provide evidence many components intrinsic dimensions figure right discussion search model structures mfas computationally intractable factor analyser allowed different intrinsic dimensionalities paper shown variational bayesian approach used efficiently infer model structure avoiding overfitting deficiencies ml approaches one attraction variational method exploited models factorisation assumed inference automatic exact also use get distribution structures desired finally derive var ational inference bayesian mixtures factor analysers generally applicable importance sampler gives us unbiased estimates true evidence exact predictive density kl divergence variational posterior true posterior encouraged results synthetic data applied bayesian mixture factor analysers real world unsupervised digit classification problem report results experiments separate article appendix optimal distributions hyperparameters ls af aq af yt bt wu lnq wu lnl lnp lx diag wu denote normal gamma dirichlet distributions respectively denotes expectation variational posterior digaroma function lnf note optimal distributions block diagonal covariance structure even though matrix cov iance ameters differentiating respect meters precision prior get fixed point equations ln ln simil ly fixed point ameters dirichlet prior wu
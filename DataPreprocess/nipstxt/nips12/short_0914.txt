abstract project pursued paper develop first information geometric principles general method learning similarity text documents individual document modeled memoryless information source based latent class decomposition term document matrix lowdimensional curved multinomial subfamily learned model canonical similarity function known fisher kernel derived approach applied unsupervised supervised learning problems alike particular covers interesting cases labeled unlabeled data available experiments automated indexing text categorization verify advantages proposed method introduction computer based analysis organization large document repositories one today great challenges machine learning key problem quantitative assessment document similarities reliable similarity measure would provide answers questions like similar two text documents documents match given query best time searching huge line hyper text collections like world wide web becomes popular relevance related questions needs emphasized focus work data driven methods learn similarity function based training corpus text documents without requiring domain specific knowledge since assume labels text categories document classes topics etc given stage former definition unsupervised learning problem fact general problem learning object similarities precedes many classical unsupervised learning methods like data clustering already presuppose availability metric similarity function paper develop framework learning similarities text documents first principles try span bridge foundations statistics information geometry real world applications information retrieval text learning namely ad hoc retrieval text categorization although developed general methodology limited text documents sake concreteness restrict attention exclusively domain learning similarity documents latent class decomposition memoryless information sources assume available set documents dl fixed vocabulary words terms wl wm information theoretic perspective document di viewed information source probability distribution word sequences following common practice information retrieval focus restricted case text documents modeled level single word occurrences means adopt bag words view treat documents memoryless information sources modeling assumption document memoryless information source assumption implies document represented multinomial probability distribution wjldi denotes unigram probability generic word occurrence document di wj correspondingly data reduced simple sufficient statistics counts di wj often word wj occurred document di rectangular matrix coefficients di wj also called term document matrix latent class analysis latent class analysis decomposition technique contingency tables cf references therein applied language modeling aggregate markov model information retrieval probabilistic latent semantic analysis latent class analysis unobserved class variable zk zl associated observation word occurrence di wj joint probability distribution mixture model parameterized two equivalent ways wj xk lx wjlx wlx xld latent class model introduces conditional independence assumption namely wj independent conditioned state associated latent variable since cardinality zk typically smaller number documents words collection acts bottleneck variable predicting words conditioned context particular document give reader intuitive understanding latent class decomposition visualized representative subset factors latent class model fitted reuters collection cf section figure intuitively learned parameters seem meaningful represent identifiable topics capture corresponding vocabulary quite well using latent class decomposition model collection memoryless sources implicitly assume overall collection help estimating parameters individual sources assumption validated experiments modeling assumption parameters collection memoryless information sources estimated latent class decomposition parameter estimation latent class model important geometrical interpretation parameters qb wjlz define low dimensional subfamily multinomial family qb rj qb ha multinomials obtained convex combinations set basis vectors qb given parameters extensions general case possible beyond scope paper hofmann government president banks pct union marks gold billion tax chairman debt january air currency steel dlrs budget executive brazil february workers dollar plant year cut chief new rise strike german mining surplus spending officer loans rose airlines bundesbank copper deficit cuts vice dlrs aircraft central tons foreign deficit company bankers december port mark silver current taxes named bank year boeing west metal trade reform board payments fell employees dollars production account billion director billion prices airline dealers ounces reserves trading american trade oil vs areas food house exchange general japan crude cts weather drug reagan futures motors japanese energy net area study president stock chrysler ec petroleum loss normal aids administration options gm states prices mln good product congress index car united bpd shr crop treatment white contracts ford officials barrels qtr damage company secretary market test community barrel revs caused environmental told london cars european exploration profit affected products volcker exchanges motor imports price note people approval reagans figure selected factors factor decomposition reuters collection displayed terms probable words class conditional distribution wjlzk selected states zk exclusion stop words zkl define unique multinomial distribution since defines submanifold multinomial simplex corresponds curved exponential subfamily would like emphasis propose learn parameters within family mixing proportions parameters define subfamily class conditionals standard procedure maximum likelihood estimation latent variable models expectation maximization em algorithm step one computes posterior probabilities latent class variables lz wlzk diz wjlz ldg ez lzz wl wj step formulae written compactly wlz denotes kronecker delta related models demonstrated latent class model viewed probabilistic variant latent semantic analysis dimension reduction technique based singular value decomposition also closely related non negative matrix decomposition discussed uses poisson sampling model motivated imposing non negativity constraints decomposition pca relationship latent class model clustering models like distributional clustering investigated presents yet another approach dimension reduction multinomials based spherical models different type curved exponential subfamilies one presented affine mean value parameterization notice graphical models latent variable general stratified exponential families yet case geometry simpler geometrical view also illustrates well known identifiability problem latent class analysis interested reader referred practical remedy used bayesian approach conjugate dirichlet prior distributions multinomials sake clarity described paper since technical nevertheless rather straightforward learning similarity documents fisher kernel information geometry fisher kernel follow work derive kernel functions hence similarity functions generative data models approach yields uniquely defined intrinsic coordinate invariant kernel called fisher kernel one important implication yardsticks used statistical models carry selection appropriate similarity functions spite purely unsupervised manner fisher kernel learned latter also useful supervised learning provides way take advantage additional unlabeled data important text learning digital document databases world wide web offer huge background text repository starting point partition data log likelihood contributions various documents average log probability document di probability word occurrences di normalized document length given di wjld log wlx xld ida constants negative kullback leibler divergence empirical distribution ts wjldi model distribution represented order derive fisher kernel compute fisher scores di gradient di respect well fisher information parameterization fisher kernel given di dn di lu dn computational considerations computational reasons propose approximate inverse information matrix identity matrix thereby making additional assumptions information orthogonality specifically use variance stabilizing parameterization multinomials square root parameterization yields isometric embedding multinomial families positive part hypersphere parameterization approximation exact multinomial family disregarding normalization constraint conjecture also provide reasonable approximation case subfamily defined latent class model simplifying assumption fisher information square root parameterization approximated identity matrix interpretation results instead going details derivation postponed end section revealing relate results back main problem defining similarity function text documents closer look two contributions resulting different sets parameters contribution stems square root transformed parameters zk simplified version given weighted inner product low dimensional factor representation documents mixing weights ldi part kernel thus computes topical overlap documents thereby able capture synonyms words identical similar meaning well words referring hofmann topic notice required di actually many terms common order get high similarity score contribution due parameters wjlzk different type using approximation fisher matrix arrive inner product ld wald ld wa ld wa wjlz also appealing interpretation essentially computes inner product empirical distributions di scheme popular context information retrieval vector space model however common words contribute explained factor respecfive posterior probabilities overlap allows capture words multiple meanings called polysems example factors displayed figure term president occurs twice president company president us depending document word occurs posterior probability high either one factors typically hence term used different context different meanings generally increase similarity documents distinction absent naive inner product corresponds degenerate case since choice determines coarseness identified topics different resolution levels possibly contribute useful information combined models simple additive combination derived inner products combination scheme experimentally proven effective robust modeling assumption similarities derived latent class decompositions different levels resolution additively combined summary emergence important language phenomena like synonymy polysemy information geometric principles satisfying proves opinion interesting similarity functions rigorously derived without specific domain knowledge based explicitly stated assumptions technical derivation define pj lz wvl di ol di ol di op walz op op jzk op similarly define applying bayes rule substitute di di yields op op op optional approximation step makes sense whenever wj ldi wj ldi notice ignored normalization constraints would yield reactive term constant multinomial experimentally observed deterioration performance making additional simplifications learning similarity docurnents medline cranfield cacm cisi vsm vsm table average precision results vector space baseline method vsm fisher kernel approach vsm standard test collections medline cranfield cacm cisi earn acq money grain crude average improv sub svm svm knn knn sub svm svm knn knn sub svm svm knn knn data svm cv svm knn knn table classification errors nearest neighbors knn svms svm naive kernel fisher kernel derived models frequent categories reuters corpus earn acq monex fx grain crude different subsampling levels experimental results applied proposed method ad hoc information retrieval goal return list documents ranked respect given query obviously involves computing similarities documents queries follow series experiments ones reported kernels di zk di ld di dn wjldi wjld proposed ad hoc manner able obtain rigorous theoretical justification well additional improvements average precision recall values four standard test collections reported table show substantial performance gains achieved help generative model cf details conducted experiments demonstrate utility method supervised learning problems applied text categorization using standard data set evaluation reuters collections news stories tried boost performance two classifiers known highly competitive text categorization nearest neighbor method support vector machines svms linear kernel since particularly interested setting generative model trained larger corpus unlabeled data run experiments classifier trained subsample subsampling factors sx results summarized table free parameters base classifiers optimized extensive simulations held data results indicate hofmann substantial performance gains achieved standard nearest neighbor method subsampling levels svms gain huge subsampled data collections insignificant svms trained data seems indicate generative model provide extra information svm classifier trained data however notice many interesting applications text categorization operate small sample limit lots unlabeled data examples include definition personalized news categories example classification filtering email line topic spotting tracking many conclusion presented approach learn similarity text documents first principles based latent class model able derive similarity function theoretically satisfying intuitively appealing shows substantial performance gains conducted experiments finally made contribution relationship unsupervised supervised learning initiated showing generative models help exploit unlabeled data classification problems references shun ichi amari differential geometrical methods statistics springer verlag berlin new york deerwester dumais furnas landauer harshman indexing latent semantic analysis journal american society information science evans gilula guttman latent class analysis two way contingency tables bayesian methods biometrika geiger heckerman king meek stratified exponential families graphical models model selection technical report msr tr microsoft research gilula haberman canonical analysis contingency tables maximum likelihood journal american statistical association gous exponential spherical subfamily models phd thesis stanford statistics department hofmann probabilistic latent semantic indexing proceedings th international conference research development information retrieval sigir pages hofmann puzicha jordan unsupervised learning dyadic data advances neural information processing systems mit press jaakkola haussler exploiting generative models discriminative classitiers advances neural information processing systems mit press joachims text categorization support vector machines learning many relevant features international conference machine learning ecml kass vos geometrical foundations asymptotic inference wiley new york lee seung learning parts objects non negative matrix factorization nature murray rice differential geometry statistics chapman hall london new york pereira tishby lee distributional clustering english words proceedings cl pages saul pereira aggregate mixed order markov models statistical language processing proceedings nd international conference empirical methods natural language processing
abstract describe method learning overcomplete set basis functions purpose modeling sparse structure images sparsity basis function coefficients modeled mixture gaussians distribution one gaussian captures nonactive coefficients small variance distribution centered zero one gaussians capture active coefficients large variance distribution show prior form exist efficient methods learning basis functions well parameters prior performance algorithm demonstrated number test cases also natural images basis functions learned natural images similar obtained methods sparse form coefficient distribution much better described also since parameters prior adapted data assumption sparse structure images need made priori rather learned data introduction general problem address learning set basis functions representing natural images efficiently previous work using variety optimization schemes established basis functions best code natural images terms sparse independent components resemble gabor like wavelet basis basis functions spatially localized oriented bandpass spatial frequency order tile joint space position orientation spatial frequency manner yields useful image representations also advocated basis set overcomplete number basis functions exceeds dimensionality images coded major challenge learning overcomplete bases though comes fact posterior distribution coefficients must sampled learning posterior sharply peaked sparse prior imposed conventional sampling methods become especially cumbersome olshausen millman one approach dealing problems associated overcomplete codes sparse priors suggested form resulting posterior distribution coefficients averaged many images shown posterior distribution one coefficients overcomplete representation sparse prior imposed learning cauchy distribution overlaid dashed line would seem coefficients fit imposed prior well instead want occupy one two states inactive state coefficient set nearly zero active state coefficient takes significant non zero value along continuum suggests appropriate choice prior one capable capturing two discrete states coeffx tenl vajue figure posterior distribution coefficients cauchy prior overlaid approach modeling form sparse structure uses mixture gaussians prior coefficients set binary ternary state variables determine whether coefficient active inactive state coefficient distribution gaussian distributed variance mean depends state variable important advantage approach regard sampling problems mentioned use gaussian distributions allows analytical solution integrating posterior distribution given setting state variables sampling needs done binary ternary state variables show problem tractable one approach differs taken previously attias use variational methods approximate posterior rather rely sampling adequately characterize posterior distribution coefficients mixture gaussians model image modeled linear superposition basis functions qbi coefficients ai plus gaussian noise ai follows expressed vector matrix notation prior probability distribution coefficients factoffal distribution coefficient ai modeled mixture gaussians distribution either two three gaussians fig set binary ternary state variables si determine gaussian used describe coefficients total prior sets variables form ailsi si learning sparse codes mixture gaussians prior two gaussians binary state variables three gaussians ternary state variables si ai si si figure mixture gaussians prior si determines probability active inactive states ailsi gaussian distribution whose mean variance determined current state si total image probability given ila ii al als aa za za parameters include aa aa diagonal inverse covariance matrix elements aa ii si notations aa used explicitly reflect dependence means variances ai si also diagonal elements asii model illustrated graphically figure ternary figure image model olshausen millman learning objective function learning parameters model average log likelihood logp ii maximizing objective minimize lower bound coding length learning accomplished via gradient ascent objective learning rules parameters aa given si sli li ore takes values binary ternary defined eqs next section note expressions dropped outer brackets averaging images simply reduce clutter thus image must sample posterior sli order collect appropriate statistics needed learning statistics must accumulated many different images parameters updated according rules note approach differs atti attempt sum states use variational approximation approximate posterior instead effectively summing states probable according posterior conjecture scheme work practice posterior significant probability small fraction states well characterized relatively small number samples next present efficient method gibbs sampling posterior sampling inference order sample posterior first cast boltzmann form log logp ila da learning sparse codes mixture gaussians prior log zaa eals log det const eals vvaeals aa ii al aa argmin eals gibbs sampling performed flipping state variables si according si si binary lq eae si ea ternary binary case two alternative states ternary case ae si denotes change due changing si given ae si si asi log aa log ha tiav jiiav alai aa jii ivi asi si aa si vi aai note computations considering change state local involve terms index thus deciding whether change state computed quickly however change state accepted must update using sherman morrison formula kept computation long accepted state changes rare found case sparse distributions gibbs sampling may performed quickly efficiently addition generally sparse matrices system scaled number elements affected flip si relatively order code images model single state coefficients must chosen given image use purpose map estimator argmaxp ali argmaxp sli maximizing posterior distribution accomplished assigning temperature sli cre gradually lowering state changes olshausen llman results test cases first trained algorithm number test cases containing known forms sparse non sparse bi modal structure using critically sampled complete overcomplete basis sets training sets consisted pixel image patches created sparse superposition basis functions sil ai results test cases confirm algorithm capable correctly extracting sparse non sparse structure data shown lack space natural images trained algorithm image patches extracted pre whitened natural images cases basis functions initialized random functions white noise prior initialized gaussian gaussians roughly equal variance shown figure results set basis functions overcomplete two gausian case three gaussian case prior initialized platykurtic three gaussians equal variance offset three different positions thus case sparse form prior emerged completely data resulting priors two coefficients shown figure posterior distribution averaged many images overlaid coefficients posterior distribution matches mixture gaussians prior well others tails appear laplacian form also appears extra complexity offered three gaussians utilized gaussians move center position mean non sparse bimodal prior imposed basis function solution become localized oriented bandpass sparse priors coding efficiency evaluated coding efficiency quantizing coefficients different levels calculating total coefficient entropy function distortion introduced quantization done basis sets containing basis functions high snr overcomplete basis sets yield better coding efficiency despite fact coefficients code however point occurs appears well beyond point errors longer perceptually noticeable around db conclusions shown prior basis functions image model adapted natural images without sparseness imposed model seeks distributions sparse learns appropriate basis functions distribution conjecture small number samples allows posterior sufficiently characterized appears hold cases averages collected gibbs sweeps sweeps initialization algorithm proved capable extracting structure challenging datasets high dimensional spaces overcomplete image codes lowest coding cost high snr levels levels appear higher practically useful hand learning sparse codes mixture gaussians prior snr figure overcomplete set basis functions priors vertical axis log probability learned natural images two priors learned three gaussian mixture using basis functions posterior distribution averaged many coefficients overlaid rate distortion curve comparing coding efficiency different learned basis sets sum marginal entropies likely underestimates true entropy coefficients considerably certainly statistical dependencies among coefficients may still case overcomplete bases show win lower snr dependencies included model coupling term acknowledgments work supported nih grant mh
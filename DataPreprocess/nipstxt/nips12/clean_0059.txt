abstract paper argues two apparently distinct modes generalizing concepts abstracting rules computing similarity exemplars seen special cases general bayesian learning framework bayes explains specific workings two modes rules abstracted similarity measured well generalization appear rule similarity based different situations analysis also suggests rules similarity distinction even computationally fundamental may still useful algorithmic level part principled approximation fully bayesian learning introduction domains ranging reasoning language acquisition broad view emerging cognition hybrid two distinct modes computation one based applying abstract rules based assessing similarity stored exemplars much support view comes study concepts categorization generalizing concepts people judgments often seem reflect rule based similarity based computations different brain systems thought involved case recent psychological models classification typically incorporate combination rule based similarity based modules contrast currently popular modularity position argue rules similarity best seen two ends continuum possible concept representations introduced general theoretical framework account people learn concepts positive examples based principles bayesian inference explore framework provides unifying explanation two apparently distinct modes generalization bayesian framework includes rules similarity special cases also addresses several questions conventional modular accounts people employ particular algorithms selecting rules measuring similarity algorithms opposed others people generalizations appear shift similarity like patterns rule like patterns systematic ways number examples observed increases shifts short paper focuses simple learning game involving number concepts rule like similarity like generalizations clearly emerge judgments human subjects imagine written short computer programs take input natural number return output either yes according whether number tenenbaum satisfies simple concept possible concepts might odd power less simplicity assume numbers consideration learner shown randomly chosen positive examples numbers program says yes must identify numbers program would accept task admittedly artificial nonetheless draws people rich knowledge number remaining amenable theoretical analysis structure meant parallel natural tasks word learning often require meaningful generalizations positive examples concept section presents representative experimental data task section describes bayesian model contrasts predictions models based purely rules similarity section summarizes discusses model applicability domains number concept game eight subjects participated experimental study number concept learning essentially instructions given trial subjects shown one random positive examples concept asked rate probability test numbers would belong concept examples observed denotes set examples observed particular trial number examples trials designed fall one three classes figure presents data two representative trials class bar heights represent average judged probabilities partitular test numbers fall concept given one positive examples marked bars shown test numbers rated subjects missing bars denote zero probability generalization merely missing data class trials subjects saw one example concept minimize bias trials preceded others multiple examples given given one example people gave test numbers fairly similar probabilities acceptance numbers intuitively similar example received slightly higher ratings acceptable acceptable remaining trials presented four examples occured pseudorandom order class ii trials examples consistent simple mathematical rule note obvious rules powers two multiples often way logically implied data multiples five possibility second case even numbers numbers possibilities mention logically possible psychologically implausible candidates powers two except nonetheless subjects overwhelmingly followed none pattern generalization test numbers rated near according whether satisified single intuitively correct rule preferred rules loosely characterized specific rules smallest extension include examples also meet criterion psychological simplicity class iii trials examples satisified simple mathematical rule similar magnitudes generalization followed similarity gradient along dimension magnitude probability ratings fell numbers characteristic distance beyond largest smallest observed examples roughly typical distance neighboring examples logically reason participants could generalized according rules similarity concept learning various complex rules happened pick given examples according different values yet subjects displayed less similarity gradients summarize data generalization single example followed weak similarity gradient based mathematical magnitude properties numbers several examples observed generalization evolved either none pattern determined specific simple rule simple rule applied articulated magnitude based similarity gradient falling characteristic distance roughly equal typical separation neighboring examples similar patterns observed several trials shown including one different value two experiments quite different domains described briefly section bayesian model introduced bayesian framework concept learning context learning axis parallel rectangles multidimensional feature space show framework adapted complex situation learning number concepts explain phenomena rules similarity documented formally observe positive examples concept want compute cix probability new objet belongs given observations inductive leverage provided hypothesis space possible concepts probabilistic model relating hypotheses data hypothesis space elements correspond subsets universe ofobj ts psychologically plausible candidates extensions concepts universe consists numbers hypotheses correspond subsets even numbers numbers etc hypotheses thought terms either rules similarity potential rules abstracted features entering similarity computation bayes distinguish interpretations ause capture fraction hypotheses people might bring task would like objetlye way focus relevant parts people hypothesis space one method additive clustering adclus extracts set features best accounts subjects similarity judgments given set objects features simply correspond subsets objects thus naturally identified hypotheses concept learning applications adclus similarity judgments numbers reveal two kinds subsets numbers sharing common mathematical property consecutive numbers similar magnitude applying adclus full set numbers impractical construct analogous hypothesis space domain based two kinds hypotheses found adclus solution one group hypotheses captures salient mathematical properties odd even square cube prime numbers multiples powers small numbers sets numbers ending digit second group hypotheses representing dimension numerical magnitude includes intervals consecutive numbers endpoints priors likelihoods probabilistic model consists ofa priorp likelihood hypothesis rather assigning prior probabilities hypotheses individually adopted hierarchical approach based intuitive division mathematical properties magnitude intervals fraction total probability allocated mathematical hypotheses group leaving tenenbaum magnitude hypotheses probability distributed uniformly across mathematical hypotheses probability distributed across magnitude intervals function interval size according erlang distribution capture intuition intervals intermediate size likely large small size treated free parameters model likelihood determined assumption randomly sampled positive examples simplest case example assumed independently sampled uniform density concept examples xlh hl vj otherwise hi denotes size subset example ifh denotes even numbers ihl even numbers equation embodies size principle scoring hypotheses smaller hypotheses assign greater likelihood larger hypotheses data assign exponentially greater likelihood number consistent examples increases size principle plays key role learning concepts positive examples see determining appearance rule like similarity like modes generalization given priors likelihoods posterior hix follows directly bayes rule finally compute probability generalization new object averaging predictions hypotheses weighted posterior probabilities ix equation follows conditional independence membership ofy given evaluate equation note el simply ify otherwise model results figure lb shows predictions bayesian model tr model captures main features data including convergence specific rule class ii trials appropriately shaped similarity gradients class iii trials understand transitions graded similarity like none rule like regimes ofgeneral zation arising interaction sizeprinciple equation hypothesis averaging equation hypothesis contributes average equation proportion posterior probability ix degree uncertainty hlx determines whether generalization sharp graded hix spread many distinct hypotheses contribute significantly resulting broad gradient generalization ix concentrated single hypothesis contributes significantly generalization appears none degree uncertainty ix tum consequence size principle given examples consistent one hypothesis significantly smaller next best competitor powers two significantly smaller even numbers smallest hypothesis becomes exponentially likely generalization appears follow specific rule however given one example given several examples consistent many similarly sized hypotheses top candidates similar intervals numbers numbers etc size based likelihood favors smaller hypotheses slightly hlx spread many overlapping hypotheses generalization appears follow gradient similarity bayesian rules similarity concept learning model predicts right shape magnitude based similarity gradients class iii trials accident characteristic distance bayesian generalization gradient varies uncertainty ix interval hypotheses shown covary intuitively relevant factor average separation neighboring examples bayes vs rules similarity alone instructive consider two special cases bayesian model equivalent conventional similarity based rule based algorithms concept learning literature call sim algorithm pioneered also described bayesian approach learning concepts positive negative evidence sim replaces size based likelihood binary likelihood measures whether hypothesis consistent examples ifyj otherwise generalization sim count features shared examples independent frequency features number examples seen figure lc shows sim successfully models generalization single example class fails capture generalization sharpens multiple examples either specific rule class ii magnitude based similarity gradient appropriate characteristic distance class iii call min algorithm preserves size principle replaces step hypothesis averaging maximization cix arg maxh xlh otherwise min perhaps oldest algorithm concept learning maximum likelihood algorithm asymptotically equivalent bayes success finite amounts data depends peaked hix figure min always selects specific consistent rule reasonable hypothesis much probable class ii conservative cases classes iii quantitative terms predictions bayes correlate much highly observed data predictions either sim min sum full bayesian framework explain full range rule like similarity like generalization patterns observed task discussion experiments two domains provide support bayes unifying framework concept learning context multidimensional continuous feature spaces similarity gradients default mode generalization bayes successfully models shape gradients depends distribution number examples sim min bayes also successfully predicts fast similarity gradients converge specific consistent rule convergence quite slow domain hypothesis space consists densely overlapping subsets axisparallel rectangles much like interval hypotheses class iii number tasks another experiment engaged word learning task using photographs real objects stimuli cover story learning new language trial subjects saw either one example novel word toy animal labeled blicket three examples one three different levels specificity subordinate dalmatians labeled three blickets basic dogs superordinate animals asked pick instances concept set test objects containing matches example levels dalmatians dogs animals well many non matching objects figure shows data predictions three models similarity like generalization given one example rapidly converged specific rule three examples observed number task classes ii contrast axis parallel rectangle task class iii num tenenbaum ber tasks similarity like responding still norm three four examples modeling purposes hypothesis space constructed hierarchical clustering subjects similarity judgments augmented priori preference basic level concepts bayesian model successfully predicts rapid convergence similarity gradient minimal rule smallest hypothesis consistent example set significantly smaller next best competitor dogs significantly smaller dogs cats multiples ten vs multiples five bayes fits full data extremely well comparison sim iq successfully accounts trials min trials conclusion bayesian framework able account rule similarity like modes generalization well dynamics transitions modes across several quite different domains concept learning key features bayesian model hypothesis averaging size principle former allows either rule like similarity like behavior depending uncertainty posterior probability latter determines uncertainty function number distribution examples structure leamer hypothesis space sparsely overlapping hypotheses specific hypothesis consistent examples much smaller nearest competitors convergence single rule occurs rapidly exampies densely overlapping hypotheses many consistent hypotheses comparable size convergence single rule occurs much slowly gradient similarity norm examples importantly bayesian framework much obviate distinction rules similarity explain might useful understanding brain figures show special cases bayes corresponding sim min algorithms consistently account distinct complementary regimes generalization sim without size principle works best given one example densely overlappipg hypotheses equation generate large differences likelihood without hypothesis averaging works best given many examples sparsely overlapping hypotheses specific hypothesis dominates sum equation light recent brain imaging studies dissociating rule exemplarbased processing bayesian theory may best thought computational level account concept learning multiple subprocesses perhaps subserving sim implemented distinct neural circuits hope explore possibility future work
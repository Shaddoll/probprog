abstract incorporate prior knowledge construct nonlinear algorithms invariant feature extraction discrimination employing unified framework terms nonlinear variant rayleigh coefficient propose non linear generalizations fisher discriminant oriented pca using support vector kernel functions extensive simulations show utility approach introduction common practice preprocess data extracting linear nonlinear features well known feature extraction technique principal component analysis pca aims find orthonormal ordered basis th direction describes much variance possible maintaining orthogonality directions however since pca linear technique limited capture interesting nonlinear structure data set nonlinear generalizations proposed among kernel pca computes principal components data set mapped nonlinearly high dimensional feature space often one prior information instance might know sample corrupted noise invariances classification change feature extraction concepts known noise transformation invariance certain degree equivalent interpreted causing change feature ought minimized clearly invariance alone sufficient condition good feature could simply take constant function one would like obtain feature invariant possible still covering much information necessary describing particular data considering one linear feature vector restricting first second order statistics data one arrives maximization called rayleigh coefficient wtsw wvs vw invariant feature extraction classification kernel spaces feature vector matrices describing desired undesired properties feature respectively information noise data covariance noise covariance obtain oriented pca leave field data description perform supervised classification common choose separability class centers class variance within class variance case recover well known fisher discriminant ratio maximized cover much information coded avoiding one coded problem known solved analogy pca generalized symmetric eigenproblem vw corresponding biggest eigenvalue paper generalize setting nonlinear one analogy first map data via nonlinear mapping high dimensional feature space optimize avoid working mapped data explicitly might impossible infinite dimensional introduce support vector kernel functions well known kernel trick kernel functions compute dot product feature space formulating algorithms using dot products replace occurrence dot product kernel function possible choices proven useful support vector machines kernel pca gaussian rbf exp llx yll polynomial kernels positive constants respectively remainder paper organized follows next section shows formulate optimization problem induced feature space section considers various ways find fisher discriminant conclude extensive experiments section discussion findings kernelizing rayleigh coefficient optimize kernel feature space need find formulation uses dot products images numerator denominator scalars done independently furthermore matrices basically covariances thus sum outer products images therefore due linear nature every solution written expansion terms mapped training data define common choices let training sample appropriate cl two subclasses ixil get full covariance sb sw operators finite dimensional subspace spanned xi possibly infinite space let span xi span xi symmetric vl lies span ci operates subspace exist expansion maximizes mika riitsch weston sch lkopf smola miiller could used st oriented kernel pca sn could use estimate noise covariance analogous definition mapped patterns sampled assumed noise distribution standard formulation fisher discriminant yielding kernel fisher discriminant kfd given sw mi mi sb afl within class scatter sw sn class scatter sis st mi sample mean patterns class incorporate known invariance oriented kernel pca one could use tangent covariance matrix small local parameter transformation finite difference approximation covariance tangent point details using st oriented kernel pca impose invariance local transformation crucially matrix constructed training patterns therefore argument used find expansion slightly incorrect neverthless assume reasonable approximation describing variance induced multiplying either matrices left right expansion find formulation uses dot products sake brevity give explicit formulation kfd cf details defining ea exi xj write kfd ru ffna dna kk eil il kij xi xj results choices sn cases oriented kernel pca transformation invariance obtained along lines note still maximize rayleigh coefficient however quotient terms expansion coefficients terms potentially infinite dimensionai space furthermore well known solution special eigenproblem direction solved using cholesky factorization projection new pattern onto computed ai xi algorithms estimating covariance matrix rank samples ill posed furthermore performing explicit centering covariance matrix loses one dimension rank even worse kfd matrix rank thus ratio well defined anymore denominator might become zero following propose several ways deal problem kfd furthermore tackle question solve optimization problem kfd efficiently far eigenproblem size becomes large numerically demanding reformulations original problem allow overcome limitations finally describe connection kfd rbf networks invariant feature extraction classification kernel spaces regularization solution subspace noted matrix rank besides numerical problems cause matrix even positive could think imposing regularization control capacity end simply add multiple identity matrix replace nu viewed different ways makes problem feasible numerically stable becomes positive ii seen decreasing bias sample based estimation eigenvalues cf iii imposes regularization lieill favoring solutions small expansion coefficients furthermore one could use regularization type additives penalizing iiw analogy svm adding kernel matrix kij xi xj optimize need solve eigenproblem might intractable large solutions sparse one directly use efficient algorithms like chunking support vector machines cf end might restrict solution lie subspace instead expanding write patterns zi could either subset training patterns estimated clustering algorithm derivation change end matrices another advantage increases rank relative size although still might need regularization quadratic optimization sparsification even full rank maximizing underdetermined optimal multiple thereof since rmc tz rank one thus seek vector anna minimal fixed rtz solution unique find optimal solving quadratic optimization problem min anna subject tz although quadratic optimization problem easier solve eigenproblem appealing interpretation constraint ensures average class distance projected onto direction discrimination constant intra class variance minimized maximize average margin contrarily svm approach optimizes large minimal margin considering able overcome another shortcoming kfd solutions sparse thus evaluating expensive solve add regularizer aiic objective function regularization parameter allowing us adjust degree sparseness connection rbf networks interestingly exists close connection rbf networks kfd add regularization expand training patterns find optimal given symmetric positive matrix kernel elements xi xj label vector rbf network see note written kdk rank yi vector patterns class zero otherwise mika rtitsch weston sch kopf smola andk miiller banana cancer diabetes german heart image ringnorm sonar splice thyroid titanic twonorm waveform rbf ab abr oj svm kfd table com parison kfd single rbf classifier adaboost ab regul ada boost abr svms see text best suit bold face second best italics kernel sample fixed kernel width gives solution mean squared error labels output minimized also case restricted expansions exists connection rbf networks smaller number centers cf experiments kernel fisher discriminant figure shows illustrative comparison features found kfd kernel pca kfd feature discriminates two classes first kernel pca feature picks important nonlinear structure evaluate performance kfd real data sets performed extensive comparison state art classifiers whose details reported compared kernel fisher discriminant support vector machines gaussian kernel adaboost regularized adaboost cf table kfd used regularized within class scatter computed projections onto optimal direction means use classification estimate threshold done trying thresholds two outputs training set selecting median smallest empirical error computing threshold maximizes margin outputs analogy support vector machine deal errors trainig set using svm soft margin approach disadvantage however control regularization constant slack variables results table show average test error standard full rank null space spanned yl null space ly get free fix constraint positive constant also feasible sthe breast cancer domain obtained university medical center inst oncology ljubljana yugoslavia thanks zwitter soklic data data sets used experiments obtained via http www first gmd de raetsch figure comparison feature found kfd left first kernel pca feature right depicted two classes information used kfd dots crosses levels feature value polynomial kernel degree two kfd regularized within class scatter invariant feature extraction classification kernel spaces deviation averages estimation runs different realizations datasets estimate necessary parameters ran fold cross validation first five realizations training sets took model parameters median five estimates see details experimental setup using prior knowledge toy example figure shows comparison kernel pca oriented kernel pca used full covariance noise matrix tangent covariance rotated patterns ii along axis translated patterns toy example shows imposing desired invariance yields meaningful invariant features another experiment incorporated prior knowledge kfd used usps database handwritten digits consists training test patterns dimensional gray scale images digits used regularized within class scatter added multiple tangent covariance nu invariance transformations chosen horizontal vertical translation rotation thickening cf simply averaged matrices corresponding transformation feature extracted using restricted expansion patterns zi first training samples kernel chosen gaussian width optimal svms class trained one kfd classified class rest computed class error winnertakes scheme threshold estimated minimizing empirical risk normalized outputs kfd without invariances achieved test error slightly better plain svm kernel using tangent covariance matrix led slight improvement result significantly better corresponding one kfd attributed fact used expansion coefficients cases tangent covariance matrix however lives slightly different subspace indeed subsequent experiment used vectors obtained clustering larger dataset including virtual examples generated appropriate invariance transformation led comparable svm using prior knowledge best svm result local kernel virtual support vectors conclusion task learning data equivalent prior knowledge invariances specific sources noise case feature extraction seek features sufficiently noise invariant still describing interesting structure oriented pca closely related fisher discriminant use particularly simple features since consider first second order statistics maximizing rayleigh coefficient since linear methods restricted many real world applications used support vector kernel functions obtain nonlinear versions algorithms namely oriented kernel pca kernel fisher discriminant analysis experiments show kernel fisher discriminant competitive figure comparison first features found kernel pca oriented kernel pca see text left right kpca okpca rotation translation invariance gaussian kernel mika riitsch weston scholkopf smola mailer cases even superior state art algorithms tested interestingly svm kfd construct hyperplane sense optimal many cases one given solution kfd superior one svms encouraged preliminary results digit recognition believe reported results improved incorporating different invariances using local kernels future research focus improvements algorithmic complexity new algorithms far larger one svm algorithm connection kfd support vector machines cf acknowledgments work partially supported grants dfg ja ec storm project number carried bs gmd first references bishop neural networks pattern recognition oxford univ press boser guyon vapnik training algorithm optimal margin classifiers haussler editor proc colt pages acm press diamantaras kung principal component neural networks wiley new york fang dawid comparison full bayes bayes least squares criteria normal discrimination chinese journal applied probability statistics freund schapire decision theoretic generalization line learning application boosting eurocolt lncs friedman regularized discriminant analysis journal american statistical association fukunaga introduction statistical pattern recognition academic press san diego nd edition mika riitsch weston schslkopf mfiller fisher discriminant analysis kernels hu larsen wilson douglas editors neural networks signal processing ix pages ieee moody darken fast learning networks locally tuned processing units neural computation riitsch onoda mfiller soft margins adaboost technical report nc tr royal holloway college university london uk saitoh theory reproducing kernels applications longman scientific technical harlow england schslkopf support vector learning oldenbourg verlag schslkopf burges smola editors advances kernel methods support vector learning mit press schslkopf smola mfiller nonlinear component analysis kernel eigenvalue problem neural computation shashua relationship support vector machine classification sparsifted fisher linear discriminant neural processing letters april tong koller bayes optimal hyperplanes maximal margin hyperplanes submitted ijcai workshop support vector machines robotics stanford edu koller
abstract stochastic meta descent smd new technique online adaptation local learning rates arbitrary twice differentiable systems like matrix momentum uses full second order information retaining computational complexity exploiting efficient computation hessian vector products apply smd independent component analysis employ resulting algorithm blind separation time varying mixtures matching individual learning rates rate change source signal mixture coefficients technique capable simultaneously tracking sources move different priori unknown speeds introduction independent component analysis ica methods typically run batch mode order keep stochasticity empirical gradient low often combined global learning rate annealing scheme negotiates tradeoff fast convergence good asymptotic performance time varying mixtures must replaced learning rate adaptation scheme adaptation single global learning rate however facilitates tracking sources whose mixing coefficients change comparable rates resp switch time cases sources move much faster others switch different times individual weights unmixing matrix must adapt different rates order achieve good performance apply stochastic meta descent smd new online adaptation method local learning rates extended bell sejnowski ica algorithm natural gradient kurtosis estimation modifications resulting algorithm capable separating tracking time varying mixture sources whose unknown mixing coefficients change different rates schraudolph giannakopoulos smd algorithm given sequence data points minimize expected value twice differentiable loss function respect parameters stochastic gradient descent denotes component wise multiplication local learning rates fi best adapted exponentiated gradient descent cover wide dynamic range staying strictly positive gt lntyt ln olnty tyt fft exp yt nil global meta learning rate approach rests assumption element fi affects corresponding element considerable variation forms basis local rate adaptation methods found literature order avoid expensive exponentiation weight update typically use linearization valid small lu giving fft max yt constrain multiplier least typically safeguard unreasonably small negative values meta level gradient descent stable must case chosen multiplier fidoes stray far unity conditions find linear approximation quite sufficient definition gradient trace accurately measure effect change local learning rate corresponding weight tempting consider immediate effect change fit declaring independent fit one quickly arrives nyt yt fft however common approach fails take account incremental nature gradient descent change fi affects current update also future ones authors account setting exponential average past gradients found empirically method alineida et al indeed improved approach averaging serves reduce stochasticity product implied average remains one immediate single step effects contrast sutton models long term effect fi future weight updates linear system carrying relevant partials forward time done real time recurrent learning results iterative update rule extended nonlinear systems define online ica local rate adaptation exponential average effect past changes ff current weights ai olnp forgetting factor free parameter algorithm inserting olni xt yt gt gives ht denotes instantaneous hessian time approximation assumes vi ofit ofit signifies certain dependence appropriate choice meta learning rate note efficient algorithm calculate htyt without ever compute store matrix ht shall elaborate technique case independent component analysis meta level conditioning gradient descent ff meta level may course suffer ill conditioning like descent main level meta descent fact squares condition number defined previous gradient exponential average past gradients special measures improve conditioning thus required make meta descent work non trivial systems many researchers use sign function radically normalize update unfortunately nonlinearity preserve zero mean property characterizes stochastic gradients equilibrium particular translate skew equilibrium distribution non zero mean change causes convergence non optimal step sizes renders methods unsuitable online learning notably almeida et al avoid pitfall using running estimate gradient stochastic variance meta normalizer addition modeling long term effect change local learning rate iterative gradient trace serves highly effective conditioner meta descent fixpoint given aht diag ff modified newton step typical values close scales inverse gradient consequently expect product yt well conditioned quantity experiments feedforward multi layer perceptrons confirmed smd require explicit meta level normalization converges faster alternative methods application ica apply smd technique independent component analysis using bell sejnowski algorithm base method goal find unmixing schraudolph giannakopoulos matrix wt scaling permutation provides good linear estimate wt independent sources gt present given mixture signal tthe mixture generated linearly according gt unknown unobservable full rank matrix include well known natural gradient kurtosis estimation modifications basic algorithm well matrix pt local learning rates resulting online update weight matrix wt wt wt pt dr gradient dt given fw ot owt fit tanh wt sign component tanh term depending current kurtosis estimate following pearlmutter define differentiation operator wt og wt rvt describes effect perturbation weights direction vt use efficiently calculate hessian vector product ht vt vec ht vec vt dt vec operator concatenates columns matrix single column vector since linear operator wt vt zvt zvt wt vt tg tanh fft diag tanh vt forth cf starting apply operator obtain ht vt conjunction matrix versions learning rate update pt pt max ao dt vt gradient trace vt vt pt dt kht vt constitutes smd ica algorithm online ic qth local rate adaptation experiment algorithm tested artificial problem sources follow elliptic trajectories according abase sin cos abase normally distributed mixing matrix well whose columns represent axes ellipses sources travel velocities normally distributed around mean one revolution every data samples sources supergaussian ica smd algorithm implemented online access data including line whitening whenever condition number estimated whitening matrix exceeded large threshold set updates disabled prevent algorithm diverging parameters settings results separating sources without ambiguity discarded figure shows performance index lower better zero ideal case along condition number mixing matrix showing algorithm robust temporary confusion separation ordinate represents data samples divided mini batches efficiency figure shows match actual mixing column estimate subspace spanned elliptic trajectory singularity occurring halfway damaging performance globally algorithm remains stable long degenerate inputs handled correctly conclusions smd ica found separating solution find possible simultaneously track ten sources move independently different priori unknown error index cond figure global view quality separation schraudolph giannakopoulos estimation error figure projection column mixing matrix arrows link exact point estimate trajectory proceeds lower right upper left speeds continue tracking extended periods necessary handle momentary singularities online estimation number sources heuristic solution smd adaptation local learning rates facilitate continuous online use ica rapidly changing environments acknowledgments work supported swiss national science foundation grants number
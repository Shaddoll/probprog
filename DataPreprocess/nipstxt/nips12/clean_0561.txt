abstract adaboost ensemble methods successfully applied number classification tasks seemingly defying problems overfitting adaboost performs gradient descent error function respect margin asymptotically concentrating patterns hardest learn noisy problems however disadvantageous indeed theoretical analysis shown margin distribution opposed minimal margin plays crucial role understanding phenomenon loosely speaking outliers tolerated benefit substantially increasing margin remaining points propose new boosting algorithm allows possibility pre specified fraction points lie margin area even wrong side decision boundary introduction boosting related ensemble learning methods recently used great success applications optical character recognition idea large minimum margin explains good generalization performance adaboost low noise regime however adaboost performs worse noisy tasks iris breast cancer benchmark data sets latter tasks large margin training points cannot achieved without adverse effects generalization error experimental observation supported study generalization error ensemble methods bounded sum fraction training points margin smaller value say plus complexity term depending base hypotheses bound capture part going practice nevertheless already conveys message cases pays allow points small margin misclassified leads larger overall margin remaining points cope problem mandatory construct regularized variants adaboost traded number margin errors size margin riitsch sch lkopf smola mgller onoda ands mika goal however far achieved heuristic way introducing regularization parameters immediate interpretation cannot adjusted easily present paper addresses problem two ways primarily makes algorithmic contribution problem constructing regularized boosting algorithms however compared previous efforts parameterizes trade much intuitive way free parameter directly determines fraction margin errors turn also appealing theoretical point view since involves parameter controls quantity plays crucial role generalization error bounds cf also furthermore allows user roughly specify parameter reasonable estimate expected error possibly studies obtained thus reducing training time boosting linear programming solution deriving new algorithm briefly discuss properties solution generated standard adaboost closely related arc gv show relation linear programming lp solution class base hypotheses let gt sequence hypotheses weights satisfying hypotheses gt elements hypotheses class defined base learning algorithm ensemble generates label weighted majority votes sign iillgz order express therefore also margin depend ease notation define yf defined likewise use normalized margin min zi ensemble learning methods find hypotheses gt used combination weights following consider adaboost algorithms including arcing details see main idea adaboost introduce weights wt zi training patterns used control importance single pattern learning new hypothesis repeatedly running base algorithm training patterns difficult learn misclassified repeatedly become important minimization objective adaboost expressed terms margins exp ilc zi every iteration adaboost tries minimize error stepwise maximization margin widely believed adaboost tries maximize smallest margin training set strictly speaking however general proof missing would imply adaboost asymptotically approximates scaling solution following linear programming problem complete hypothesis set cf assuming finite number basis hypotheses maximize subject zi alll rn alll lgi ilalll arc ensemble learning presence outliers since linear program cannot solved exactly infinite hypothesis set general interesting analyze approximation algorithms kind problems breiman proposed modification ariaboost arc gv making possible show asymptotic convergence ot global solution pip theorem breiman choose iteration argmin exp ii zi oft assume base learner always finds hypothesis minimizes weighted training error respect weights lira pip note algorithm derived modified error function gv exp tl zi question one might ask whether use adaboost rather arc gv practice arc gv converge fast enough benefit asymptotic properties conducted experiments investigate question empirically found adaboost problems finding optimal combination plp arc gv convergence depend plp plp adaboost usually converges maximum margin solution slightly faster arc gv observation becomes clear converge iilll bounded value thus asymptotic case cannot reached whereas arc gv optimum always found moreover number iterations necessary converge good solution seems reasonable near optimal solution number iterations rather high implies real world hypothesis sets number iterations needed find almost optimal solution become prohibitive conjecture practice reasonably good approximation optimum provided adaboost arc gv algorithms lp adaboost approach shown noisy problems generalization performance usually good one adaboost theorem cf theorem page fact becomes clear minimum right hand side inequality cf need necessarily achieved maximum margin propose algorithm directly control number margin errors therefore also contribution terms inequality separately cf theorem first consider small hypothesis class end linear program lp adaboost subsection combine algorithm ideas section get new algorithm arc approximates lp solution lp adaboost let us consider case given finite set hypotheses find coefficients combined hypothesis extend lp adaboost algorithm incorporating parameter solve following linear optimization problem ei maximize subject zi alll alll tandl riitsch scholkopf smola miiller onoda ands mika algorithm force margins beyond zero get soft margin classification cf svms regularization constant following proposition shows immediate interpretation proposition rfitsch et al suppose run algorithm given data resulting optimal upper bounds fraction margin errors upper bounds fraction patterns margin larger since slack variables enter cost function linearly absolute size important loosely speaking due fact optimum primal objective function derivatives wrt primal variables matter derivative linear function constant case svms hypotheses thought vectors feature space statement translated precise rule distorting training patterns without changing solution move locally orthogonal separating hyperplane yields desirable robustness property note algorithm essentially depends number outliers size error arc algorithm suppose large finite base hypothesis class difficult solve directly end propose new algorithm arc approximates solution optimal fixed margins zi written argrnax zi oo pe vm max setting pv zi subtracting resulting inequality sides yields zi two substitutions needed transform problem one solved adaboost algorithm particular get rid slack variables absorbing quantities similar zi works follows right hand side objective function cf left hand side term depends nonlinearly defining fig ts ts zi zi substitute respectively obtain new optimization problem note tsv ts zi play role corrected virtual margin obtain nonlinear min max problem maximize subject ts zi rn arc gv solve approximately cf section hence replacing margin equation formulas arc gv cf arc ensemble learning presence outliers obtain new algorithm refer arc state interesting properties arc using theorem bounds generalization error ensemble methods case rp construction number patterns margin smaller cf proposition thus get following simple reformulation bound theorem let distribution let sample examples chosen lid according suppose base hypothesis space vc dimension let probability least random choice training set every function generated arc pv satisfies following bound log minimizing right hand side tradeoff first second term controlling easily interpretable regularization parameter experiments show set toy experiments illustrate general behavior arc base hypothesis class use rbf networks data two class problem generated several gauss blobs cf banana shape dataset bttp www first gmd de data banana btm obtain following results arc leads approximately vrn patterns effectively used training base learner figure left shows fraction patterns high average weights learning process ett wt zi find number latter increases almost linearly follows soft margin patterns pv set weight patterns estimated test error averaged training sets exhibits rather fiat minimum figure lower indicates vsvms corresponding results obtained well behaved parameter sense slight misadjustment harmful arc leads fraction margin errors cf dashed line figure exactly predicted proposition finally good value already inferred prior knowledge expected error setting value similar latter provides good starting point optimization cf theorem note recover bagging algorithm used bootstrap samples weights patterns wt zi also hypothesis weights constant finally present small comparison ten benchmark data sets obtained uci benchmark repository cf http da rst gmd de raetsch data benchraaxks htral analyze performance single rbf networks adaboost arc rbf svms adaboost arc use rbf networks base hypothesis model parameters rbf number centers etc arc svms rr optimized using fold cross validation details experimental setup riitsch schglkopf smola iller onoda ands mika number important patterns number margin errors training error figure toy experiment left graph shows average fraction important patterns av fraction margin errors av training error different values regularization constant arc right graph shows corresponding generalization error cases parameter allows us reduce test errors values much lower hard margin algorithm recover arcgv adaboost get bagging found fig shows generalization error estimates averaging realizations data sets confidence interval results best classifier classifiers significantly worse set bold face test significance used test eight ten data sets arc performs significantly better adaboost clearly shows superior performance arc noisy data sets supports soft margin approach adaboost furthermore find comparable performances arc svms three cases svm performs better two cases arc performs best summarizing adaboost useful low noise cases classes separable arc extends applicability boosting problems difficult separate applied data noisy conclusion analyzed adaboost algorithm found arc gv adaboost efficient approximating solution non linear min max problems huge hypothesis classes parameterized lpreg adaboost algorithm cf introduced new regularization constant controls fraction patterns inside margin area new parameter highly intuitive optimized fixed interval using fact arc gv approximately solve min max problems found formulation arc gv arc implements idea boosting defining appropriate soft margin present paper extends previous work regularizing boosting doom adaboostreg shows utility flexibility soft margin approach adaboost banana cancer diabetes german heart ringnorm sonar thyroid titanic waveform rbf ab arc svm table generalization error estimates confidence intervals best classifiers particular data set marked bold face see text arc ensemble learning presence outliers found empirically generalization performance arc depends slightly choice regularization constant makes model selection via cross validation easier faster future work study detailed regularization properties regularized versions adaboost particular comparison lp support vector machines acknowledgments partial funding dfg grant ja gratefully acknowledged work done bs gmd first
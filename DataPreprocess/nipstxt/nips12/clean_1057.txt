abstract function approximation essential reinforcement learning standard approach approximating value function determining policy far proven theoretically intractable paper explore alternative approach policy explicitly represented function approximator independent value function updated according gradient expected reward respect policy parameters williams reinforce method actor critic methods examples approach main new result show gradient written form suitable estimation experience aided approximate action value advantage function using result prove first time version policy iteration arbitrary differentiable function approximation convergent locally optimal policy large applications reinforcement learning rl require use generalizing function approximators neural networks decision trees instance based methods dominant approach last decade value function approach function approximation effort goes estimating value function action selection policy represented implicitly greedy policy respect estimated values policy selects state action highest estimated value value function approach worked well many applications several limitations first oriented toward finding deterministic policies whereas optimal policy often stochastic selecting different actions specific probabilities see singh jaakkola jordan second arbitrarily small change estimated value action cause selected discontinuous changes identified key obstacle establishing convergence assurances algorithms following value function approach bertsekas tsitsiklis example learning sarsa dynamic programming methods shown unable converge policy simple mdps simple function approximators gordon baird tsitsiklis van roy bertsekas tsitsiklis occur even best approximation found step changing policy whether notion best mean squared error sense slightly different senses residual gradient temporal difference dynamic programming methods paper explore alternative approach function approximation rl sutton mcallester singh mansour rather approximating value function using compute deterministic policy approximate stochastic policy directly using independent function approximator parameters example policy might represented neural network whose input representation state whose output action selection probabilities whose weights policy parameters let denote vector policy parameters performance corresponding policy average reward per step policy gradient approach policy parameters updated approximately proportional gradient op positive definite step size achieved usually assured converge locally optimal policy performance measure unlike value function approach small changes cause small changes policy state visitation distribution paper prove unbiased estimate gradient obtained experience using approximate value function satisfying certain properties williams reinforce algorithm also finds unbiased estimate gradient without assistance learned value function reinforce learns much slowly rl methods using value functions received relatively little attention learning value function using reduce variance gradient estimate appears essential rapid learning jaakkola singh jordan proved result similar special case function approximation corresponding tabular pomdps result strengthens generalizes arbitrary differentiable function approximators konda tsitsiklis prep independently developed simialr result see also baxter bartlett prep marbach tsitsiklis result also suggests way proving convergence wide variety algorithms based actor critic policy iteration architectures barto sutton anderson sutton kimura kobayashi paper take first step direction proving first time version policy iteration general differentiable function approximation convergent locally optimal policy baird moore obtained weaker superficially similar result vaps family methods like policy gradient methods vaps includes separately parameterized policy value functions updated gradient methods however vaps methods climb gradient performance expected long term reward measure combining performance valuefunction accuracy result vaps converge locally optimal policy except case weight put upon value function accuracy case vaps degenerates reinforce similarly gordon fitted value iteration also convergent value based find locally optimal policy policy gradient theorem consider standard reinforcement learning framework see sutton barto learning agent interacts markov decision process mdp state action reward time denoted st rt respectively environment dynamics characterized state transition probabilities pr st st expected rewards rt st vs agent decision making procedure time characterized policy pr alst vs ji parameter vector assume diffentiable respect parameter exists also usually write policy gradient methods rl function approximation function approximation two ways formulating agent objective useful one average reward formulation policies ranked according long term expected reward per step lim rl rn limt pr st stationary distribution states assume exists independent policies average reward formulation value state action pair given policy defined ao vse aea second formulation cover designated start state care long term reward obtained give results apply formulation well definitions lrt lrt st xk discount rate allowed episodic tasks formulation define discounted weighting states encountered starting following tpr st first result concerns gradient performance metric respect policy parameter theorem policy gradient mdp either average reward start state formulations op oo oo proof see appendix way expressing gradient first discussed average reward formulation marbach tsitsiklis based related expression terms state value function due jaakkola singh jordan cao chen extend results start state formulation provide simpler direct proofs williams theory reinforce algorithms also viewed implying event key aspect expressions gradient terms form oa effect policy changes distribution states appear convenient approximating gradient sampling example sampled distribution would unbiased estimate obtained following oe course also normally known must estimated one ap proach use actual returns rt oo oo ek ek vk iwt start state formulation approximation st leads williams episodic reinforce algorithm oc corrects oversampling actions preferred known follow expected value williams policy gradient approximation consider case approximated learned function approximator approximation sufficiently good might hope use place sutton mc lester singh mansour still point roughly direction gradient example jaakkola singh jordan proved special case function approximation arising tabular pomdp one could assure positive inner product gradient sufficient ensure improvement moving direction extend result general function approximation prove equality gradient let fw approximation qx parameter natural rt learn fw following rr updating rule awt fw ao st unbiased st st st tj estimator st perhaps pu process converged local optimum fu ofu ow theorem policy gradient function approximation fw satisfies compatible policy parameterization sense ofu ow op proof combining gives oo tells us error fu orthogonal gradient policy parameterization expression zero subtract policy gradient theorem yield op oo application deriving algorithms advantages given policy parameterization theorem used derive appropriate form value function parameterization example consider policy gibbs distribution linear combination features eb eotq vs tsitsiklis personal communication points linear features given righthand side may way satisfy condition policy gradient methods rl function approximation qbsa dimensional feature vector characterizing state action pair meeting compatibility condition requires ofw qbsb natural parameterization fw words fw must linear features policy except normalized mean zero state algorithms easily derived variety nonlinear policy parameterizations multi layer backpropagation networks careful reader noticed form given requires zero mean state fw sense better think fw approximation advantage function much baird rather qx convergence requirement really fw get relative value actions correct state absolute value variation state state results viewed justification special status advantages target value function approximation rl fact generalized include arbitrary function state added value function approximation example generalized sd fw arbitrary function follows immediately choice affect theorems substantially affect variance gradient estimators issues entirely analogous use reinforcement baselines earlier work williams dayan sutton practice presumably set best available approximation results establish approximation process proceed without affecting expected evolution convergence policy iteration function approximation given theorem prove first time form policy iteration function approximation convergent locally optimal policy theorem policy iteration function approximation let rr fw differentiable function approximators policy value function respectively satisfy compatibility condition max oo aoj oo let step size sequence limk ak ak oo mdp bounded rewards sequence rrk defined rk ok wk rk ok otk dw rk oo converges limk proof theorem assures update direction gradient mdp rewards together assure us bounds oo ooj ao ao sutton mcallester singh mansour also bounded together step size requirements necessary conditions apply proposition page bertsekas tsitsildis assures convergence local optimum acknowledgements authors wish thank martha steenstrup doina precup comments michael kearns insights notion optimal policy function approximation
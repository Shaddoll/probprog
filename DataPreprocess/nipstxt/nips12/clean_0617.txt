abstract introduce novel distributional clustering algorithm maximizes mutual information per cluster data given categories algorithm considered bottom hard version recently introduced information bottleneck method algorithm compared top soft version information bottleneck method relationship hard soft results established demonstrate algorithm newsgroups data set subset two newsgroups achieve compression orders magnitudes loosing original mutual information introduction problem self organization members set based similarity conditional distributions members another set first introduced termed distributional clustering question recently shown special case much fundamental problem features variable relevant prediction another relevance variable general problem shown natural information theoretic formulation find compressed representation variable denoted mutual information high possible constraint mutual information surprisingly variational problem yields exact self consistent equations conditional distributions constrained information optimization problem called information bottleneck method original approach solution resulting equations used already based analogy deterministic annealing approach clustering see top hierarchical algorithm starts single cluster undergoes cascade cluster splits determined stochastically phase transitions soft fuzzy tree clusters paper propose alternative approach information bottleneck slonim tishby problem based greedy bottom merging several advantages top method fully deterministic yielding initially hard clusters desired number clusters gives higher mutual information per cluster deterministic annealing algorithm considered hard zero temperature limit deterministic annealing prescribed number clusters furthermore using bottleneck self consistent equations one soften resulting hard clusters recover deterministic annealing solutions without need identify cluster splits rather tricky main disadvantage method computational since starts limit cluster per member set information bottleneck method mutual information random variables symmetric functional joint distribution log xex yl ylx log objective information bottleneck method extract compact representation variable denoted minimal loss mutual information another relevance variable specifically want find possibly stochastic map lx minimizes lossy coding length via constraint mutual information relevance variable words want find efficient representation variable predictions close possible direct prediction shown introducing positive lagrange multiplier enforce mutual information constraint problem amounts minimization la grangian lx respect lx subject markov condition normalization minimization yields directly following self consistent equations map lx well yls lx exp ylx yl yl ylx lx lx variational principle eq determines also shape annealing process since changing mutual informations ix vary ix normalization function functional cl pllq yp log qp kulback liebler divergence emerges variational principle equations solved iterations proved converge finite value see lagrange multiplier natural interpretation inverse temperature suggests deterministic annealing explore hierarchy solutions approach taken already agglomerative information bottleneck thus optimal curve analogous rate distortion function information theory follows strictly concave curve ix plane called information plane deterministic annealing fixed number clusters follows concave curve well curve suboptimal beyond certain critical value another interpretation bottleneck principle comes relation mutual information bayes classification error error bounded see important information theoretic measure class conditional distributions xlyi called jensen shannon divergence measure plays important role context jensen shannon divergence class distributions pi prior ri defined js pm ripi rih vi hip shannon entropy hip exp logp convexity entropy jensen inequality guarantees non negativity jsdivergence hard clustering limit finite cardinality representation limit eqs induces hard partition disjoint subsets limit member belongs subset yls smallest dkl ylx llp yl probabilistic map obtains limit values paper focus bottom agglomerative algorithm generating good hard partitions denote partition cardinality also zm zl zm case zi say zm optimal partition necessarily unique every partition ztm zm starting trivial partition ix seek sequence merges coarser coarser partitions close possible optimal easy verify limit eqs partition distributions simplified follows let xlzl xi denote specific component cluster partition zm vx otherwise ty xi yy xi using distributions one easily evaluate mutual information zm zm zm zm using eq hard partition hard clustering obtained one apply reverse annealing soften clusters decreasing self consistent equations eqs using procedure fact recover stochastic map lx hard partition without need identify cluster splits demonstrate reverse deterministic annealing procedure last section sionin tishby relation work similar agglomerative procedure without information theoretic framework analysis recently used text categorization newsgroup corpus another approach stems distributional clustering algorithm given clustering dyadic data earlier application mutual information semantic clustering words used agglomerative information bottleneck algorithm algorithm starts trivial partition clusters components component contains exactly one element step merge several components current partition single new component way locally minimizes loss mutual information zm let current partition zm denote new partition merge several components zm obviously let zi zk zm denote set components merged zm new component generated merge evaluate reduction mutual information zm due merge one needs distributions define new partition determined follows every zm probability distributions zlx remains equal distributions zm new component zm define zi zi vy ifx ziforsomel vx lx otherwise easy verify zm indeed valid partition proper probability distributions using notations every merge define additional quantities merge prior distribution defined iik ri prior probability zi merged subset ri information decrease decrease mutual information due single merge zl zm zm information decrease decrease mutual information due single merge ix zl zm zm algorithm greedy procedure step perform best possible merge merge components current partition minimize zk since increase corollary greedy procedure enough check possible merging pairs components current partition another advantage mergi ng pairs way go possible cardinalities possible pairs given partition zm merge find best possible merge one must evaluate reduction information zi zj zm zm every pair zm iyi operations every pair however using proposition know zi zj zi zj jsn zi yizj reduction mutual agglomerat information bottleneck information due merge zi zj evaluated directly looking pair operations reduction factor time complexity every merge input empirical probability matrix ixi iyi output zm partition clusters every initialization construct zi xi ylzi ylxi every zlxj otherwise zn every calculate xd xj jsn ylxd ylxy every di points corresponding couple loop find argmini di severm minima choose bitr ily merge zz yl zz every ze otherwise every update ze new partition clusters update di costs pointers couples contained zz end figure pseudo code algorithm discussion algorithm non parametric simple greedy procedure depends input empirical joint distribution output algorithm hierarchy partitions zm moreover unlike clustering heuristics built measure efficiency even sub optimal solutions namely mutual information zm bounds bayes classification error quality measure obtained partition fraction mutual information zm captures zm found empirically given curve vs curve concave always true decrease mutual information zm zm every step given increase decreasing therefore point becomes relatively high indication reached value meaningful partition clusters slonim tishby merging results substantial loss information thus significant reduction performance clusters features however since computational cost final low part procedure low well complete merging single cluster figure left figure results agglomerative algorithm shown information plane normalized vs normalized ng dataset compared soft version information bottleneck via reverse annealing smooth curves left annealing curve connected starting point dotted line plane hard algorithm clearly inferior soft one right hand side agglomerative algorithm plotted vs cardinality partition three subsets newsgroup dataset compare performance different data cardinalities normalize value thus forcing three curves start end points predictive information newsgroup ng ng similar dichotomy dataset ng much better prediction possible izi expected dichotomies inset presents full curve normalized vs iz ng data comparison plane hard partitions superior soft ones application evaluate ideas algorithm apply several subsets newsgroups dataset collected ken lang using articles evenly distributed among usenet discussion groups see replaced every digit single character another mark non alphanumeric characters following preprocessing first dataset contained strings appeared times data dataset referred ng similarly strings appeared times constitutes ng dataset contains different strings evaluate also dichotomy data used corpus consisting two discussion groups newsgroups similar topics alt atheism talk religion misc using pre processing removing strings occur less times resulting lexicon contained different strings refer dataset ng plot results algorithm three data sets two different planes first normalized information vs size partition number clusters greedy procedure directly tries maximize given seen strong concavity curves figure right indeed procedure able maintain high percentage relevant mutual information original data reducing dimensionality features agglomerative information bottleneck several orders magnitude right hand side figure present comparison efficiency procedure three datasets two class data consisting different strings compressed two orders magnitude clusters almost without loosing mutual information news groups decrease compression three orders magnitude clusters maintains original mutual information similar results even though less striking obtained contain newsgroups ng dataset compressed strings clusters keeping mutual information clusters keeping information compression efficiency obtained ng dataset relationship soft hard clustering demonstrated information plane normalized mutual information values vs plane soft procedure optimal since direct maximization constraining hard partition suboptimal plane confirmed empirically provides excellent starting point reverse annealing figure present results agglomerative procedure ng information plane together reverse annealing different values predicted theory annealing curves merge various critical values globally optimal curve correspond rate distortion function information bottleneck problem reverse annealing heating procedure need identify cluster splits required original annealing cooling procedure seen phase diagram much better recovered procedure suggesting combination agglomerative clustering reverse annealing ultimate algorithm problem
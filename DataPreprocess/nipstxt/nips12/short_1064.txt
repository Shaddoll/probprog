abstract present monte carlo algorithm learning act partially observable markov decision processes pomdps real valued state action spaces approach uses importance sampling representing beliefs monte carlo approximation belief propagation reinforcement learning algorithm value iteration employed learn value functions belief states finally samplebased version nearest neighbor used generalize across states initial empirical results suggest approach works well practical applications
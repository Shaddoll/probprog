abstract nonnegative boltzmann machine nnbm recurrent neural network model describe multimodal nonnegative data application maximum likelihood estimation model gives learning rule analogous binary boltzmann machine examine utility mean field approximation nnbm describe monte carlo sampling techniques used learn parameters reflective slice sampling particularly well suited distribution efficiently implemented sample distribution illustrate learning nnbm translationally invariant distribution well generative model images human faces introduction multivariate gaussian elementary distribution used model generic data represents maximum entropy distribution constraint mean covariance matrix distribution match data case binary data maximum entropy distribution matches first second order statistics data given boltzmann machine probability particular state boltzmann machine given exponential form si exp siaidsj bisi ij interpreting eq neural network parameters aid represent symmetric recurrent weights different units network bi represent local biases unfortunately parameters simply related observed mean covariance nonnegative boltzmann machine oo figure probability density shaded contour plot two dimensional competitive nnbm distribution energy function distribution contains saddle point two local minima generates observed multimodal distribution data normal gaussian instead need adapted using iterative learning rule involves difficult sampling binary distribution boltzmann machine also generalized continuous nonnegative variables case maximum entropy distribution nonnegative data known first second order statistics described distribution previously called rectified gaussian distribution exp ifzi vi energy fimction normalization constant az exp properties nonnegative boltzmann machine nnbm distribution differ quite substantially normal gaussian particular presence nonnegativity constraints allows distribution multiple modes example fig shows two dimensional nnbm distribution two separate maxima located rectifying axes multimodal distribution would poorly modelled single normal gaussian submission discuss multimodal nnbm distribution learned nonnegative data show limitations mean field approximations distribution illustrate recent developments efficient sampling techniques continuous belief networks used tune weights network specific examples learning demonstrated translationally invariant distribution well generative model face images maximum likelihood learning rule nnbm derived maximizing log likelihood observed data eq given set nonnegative vectors tt downs mackay lee indexes different examples log likelihood zl gp ze gz tt taking derivatives ofeq respect parameters gives xixj xixj oaij ol xi xi subscript denotes clamped average data subscript denotes free average nnbm distribution derivatives used define gradient ascent learning rule nnbm similar binary boltzmann machine contrast clamped free covariance matrix used update iteractions difference clamped free means used update local biases mean field approximation major difficulty learning algorithm lies evaluating averages xixj xi analytically intractable calculate free averages exactly approximations necessary learning mean field approximations previously proposed deterministic alternative learning binary boltzmann machine although contrasting views validity investigate utility mean field theory approximating nnbm distribution mean field equations derived approximating nnbm distribution eq factorized form ii ii qri ci different marginal densities xi characterized means ri fixed constant product distributions natural factorizable distribution nonnegative random variables optimal mean field parameters ri determined minimizing kullback leibler divergence nnbm distribution factorized distribution qiip dxq log logz finding minimum eq setting derivatives respect mean field parameters ri zero gives simple mean field equations aii bi aijrj nonnegative boltzmann machine xi figure slice sampling one dimension given current sample point height randomly chosen defines slice st new ci chosen multidimensional slice new point chosen using ballistic dynamics specular reflections interior boundaries slice equations solved self consistently ri free statistics nnbm replaced statistics factorized distribution xi ti xixj ij titj fidelity approximation determined well factorized distribution models nnbm distribution unfortunately distributions one shown fig mean field approximation quite different true multimodal nnbm distribution suggests naive mean field approximation inadequate learning nnbm fact attempts use approximation fail learn examples given following sections however mean field approximation still used initialize parameters reasonable values using sampling techniques described monte carlo sampling direct approach calculating free averages eq numerically approximate accomplished using monte carlo sampling generate representative set points sufficiently approximate statistics continuous distribution particular markov chain monte carlo methods employ iterative stochastic dynamics whose equilibrium distribution converges desired distribution binary boltzmann machine sampling dynamics involves random spin flips change value single binary component unfortunately single component dynamics easily caught local energy minima converge slowly large systems makes sampling binary distribution difficult specialized computational techniques simulated annealing cluster updates etc developed try circumvent problem nnbm use continuous variables makes possible investigate different stochastic dynamics order efficiently sample distribution first experimented gibbs sampling ordered overrelaxation found required inversion error function computationally expensive instead recently developed method slice sampling seems particularly well suited implementation nnbm basic idea slice sampling algorithm shown fig given sample point ci random ci first uniformly chosen slice defined connected set points ap new point chosen downs mackay lee io figure contours two dimensional competitive nnbm distribution overlaid mean field approximation reflected slice samples randomly slice distribution large shown converge desired density nnbm solving boundary points along particular direction given slice quite simple since involves solving roots quadratic equation order efficiently choose new point within particular slice reflective billiard ball dynamics used random initial velocity chosen new point evolved travelling certain distance current point specularly reflecting boundaries slice intuitively reversibility reflections allows dynamics satisfy detailed balance fig mean field approximation reflective slice sampling used model two dimensional competitive nnbm distribution poor fit mean field approximation apparent unimodality factorized density sample points reflective slice sampling algorithm representative underlying nnbm distribution higher dimensional data mean field approximation becomes progressively worse therefore necessary implement numerical slice sampling algorithm order accurately approximate nnbm distribution translationally invariant model ben yishai et al proposed model orientation tuning primary visual cortex interpreted cooperative nnbm distribution absence visual input firing rates cortical neurons described minimizing energy function parameters aij bi cos li jl ij distribution used test nnbm learning algorithm first large set dimensional nonnegative mining vectors generated sampling distribution fi using samples mining data parameters learned unimodal initialization evolving mining vectors using reflective slice sampling evolved vectors used calculate free averages eq estimates updated procedure iterated evolved averages matched training data learned parameters found almost exactly match original form eq representative samples learned nnbm distribution shown fig nonnegative boltzmann machine figure representative samples taken nnbm training learn translationally invariant cooperative distribution figure morphing face image successive sampling learned nnbm distribution samples generated normal gaussian generative model faces also used nnbm learn generative model images human faces nnbm used model correlations coefficients nonnegative matrix factorization nmf face images nmf reduces dimensionality nonnegative data decomposing face images parts correponding eyes noses ears etc since different parts coactivated reconstructing face activations parts contain significant correlations need captured generative model briefly demonstrate nnbm able learn correlations sampling nnbm stochastically generates coefficients graphically displayed face images fig shows representative face images reflective slice sampling dynamics evolves coefficients also displayed figure analogous images generated normal gaussian used model correlations instead clear nonnegativity constraints multimodal nature nnbm results samples cleaner distinct faces downs mackay lee discussion introduced nnbm recurrent neural network model able describe multimodal nonnegative data application made practical efficiency slice sampling monte carlo method learning algorithm incorporates numerical sampling nnbm distribution able learn observations nonnegative data demonstrated application nnbm learning cooperative translationally invariant distribution well real data images human faces extensions present work include incorporating hidden units recurrent network addition hidden units implies modelling certain higher order statistics data requires calculating averages hidden units anticipate marginal distribution units commonly unimodal hence mean field theory valid approximating averages another possible extension involves generalizing nnbm model continuous data confined within certain range situation slice sampling techniques would also used efficiently generate representative samples case hope work stimulates research using types recurrent neural networks model complex multimodal data acknowledgements authors acknowledge useful discussion john hopfield sebastian seung nicholas socci gayle wittenberg indebted haim sompolinsky pointing maximum entropy interpretation boltzmann machine work funded bell laboratories lucent technologies downs grateful moral support open ears minds beth brittle gunther lenz sandra scheitz references hinton ge sejnowski tj optimal perceptual learning ieee conference computer vision pattern recognition washington dc ackley dh hinton ge sejnowski tj learning algorithm boltzmann machines cognitive science socci nd lee dd seung hs rectified gaussian distribution advances neural information processing systems mackay djc introduction monte carlo methods learning graphical models kluwer academic press nato science series galland cc limitations deterministic boltzmann machine learning network kappen hj rodriguez fb mean field approach learning boltzmann machines pattern recognition practice amsterdam neal rm suppressing random walks markov chain monte carlo using ordered overrelaxation technical report dept statistics university toronto neal rm markov chain monte carlo methods based slicing density function technical report dept statistics university toronto ben yishai bar rl sompolinsky theory orientation tuning visual cortex proc nat acad sci usa lee dd seung hs learning parts objects non negative matrix factorization nature
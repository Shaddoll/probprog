abstract analyze conditions synaptic learning rules based action potential timing approximated learning rules based firing rates particular consider form plasticity synapses depress presynaptic spike followed postsynaptic spike potentiate opposite temporal ordering differential anti hebbianplasticity approximated certain conditions learning rule depends time derivative postsynaptic firing rate learning rule acts stabilize persistent neural activity patterns recurrent neural networks introduction recent experiments demonstrated types synaptic plasticity depend temporal ordering presynaptic postsynaptic spiking cortical hippocampal synapses long term potentiation induced repeated pairing presynaptic spike succeeding postsynaptic spike long term depression results order reversed dependence change synaptic strength difference tpost tpre postsynaptic presynaptic spike times measured quantitatively pairing function sketched figure positive negative width tens milliseconds pairing function differential tpost tpr ii illl illill ii ii illlllll llll post iiiilllllll illllllllllll ooo time ms figure pairing function differential hebbian learning change synaptic strength plotted versus time difference postsynaptic presynaptic spikes pairing function differential anti hebbian learning differential antihebbian learning driven changes firing rates synaptic learning rule eq applied two poisson spike trains synaptic strength remains roughly constant time except postsynaptic rate changes lobes correspond potentiation depression refer synaptic plasticity associated hebbian plasticity hebbian conditions xie seung potentiation predicted hebb differential driven difference opposing processes potentiation depression pairing function figure characteristic synapses example opposite temporal dependence observed electrosensory lobe synapses electric fish shown figure lb synapses depress presynaptic spike followed postsynaptic one potentiate order reversed refer differential anti hebbian plasticity according experiments maximum ranges differential hebbian antihebbian pairing functions roughly ms respectively fairly short seems compatible descriptions neural activity based spike timing rather instantaneous firing rates fact show conditions spike based learning rules approximated rate based learning rules people also studied relationship spike based rate based learning rules pairing functions figures lb lead rate based learning rules like traditionally used neural networks except depend temporal derivatives firing rates well firing rates argue differential antihebbian learning rule figure lb could general mechanism tuning strength positive feedback networks maintain short term memory analog variable persistent neural activity number recurrent network models proposed explain memory related neural activity motor prefrontal cortical areas well head direction system oculomotor integrator models require precise tuning synaptic strengths order maintain continuously variable levels persistent activity simple illustration tuning differential antihebbian learning model persistent activity maintained integrate fire neuron excitatory autapse studied spike based learning rule pairing functions like figure measured using repeated pairing single presynaptic spike single postsynaptic spike quantitative measurements synaptic changes due complex patterns spiking activity yet done assume simple model synaptic change due arbitrary spike trains sum contributions possible pairings presynaptic postsynaptic spikes model unlikely exact description real synapses could turn approximately valid write spike train ith neuron series dirac delta functions si nth spike time ith neuron synaptic weight neuron time denoted wij change synaptic weight induced presynaptic spikes occurring time interval modeled wij wij dtj dti ti tj si ti sj tj presynaptic spike paired postsynaptic spikes produced pairing synaptic weight changed amount depending pairing function pairing function assumed nonzero inside interval zero outside refer pairing range according model presynaptic spike results induction plasticity latency accordingly arguments wij left hand side equation shifted relative limits integral right hand side spike based learning stabilization persistent neural activity assume latency greater pairing range wi time influenced presynaptic postsynaptic spikes happened time therefore learning rule causal relation rate based learning rules learning rule eq driven correlations presynaptic postsynaptic activities dependence made explicit making change variables ti tj eq yields wij wo defined cross correlation duf ci ci fo dt si made use fact vanishes outside interval immediate goal relate eq learning rules based cross correlation firing rates jate dtvi vj number ways defining instantaneous firing rates sometimes computed averaging repeated presentations stimulus situations defined temporal filtering spike trains following discussion general apply definitions firing rates rate correlation commonly subtracted total correlation obtain spike correlation cid pik cij ate derive rate based approximation learning rule rewrite spike wij wij du du cij simply neglect second term shortly discuss conditions good approximation first derive another form first term applying approximation ubi obtain fo duf jate dt lpi define au duuf approximation good firing rates vary slowly compared pairing range learning rule depends postsynaptic rate hi first term dominates second learning rule conventional one based correlations firing rates sign determines whether rule hebbian anti hebbian remainder paper discuss novel case holds pairing functions shown figures lb positive negative lobes areas exactly cancel definition dependence xie seung postsynaptic activity purely time derivative firing rate differential hebbian learning corresponds figure differential anti hebbian learning leads figure lb summarize case synaptic changes due rate correlations approximated diff hebbian wij cr biv diff anti hebbian slowly varying rates formulas imply constant postsynaptic firing rate causes net change synaptic strength instead changes rate required induce synaptic plasticity illustrate point figure shows result applying differential anti hebbian learning two spike trains presynaptic spike train generated hz poisson process postsynaptic spike train generated inhomogeneous poisson process rate shifted hz hz sec shift synaptic strength fluctuates remains roughly constant upward shift firing rate causes downward shift synaptic strength accord sign differential anti hebbian rule eq rate based approximation works well example second term eq important let us return issue general conditions term neglected poisson spike trains spike correlations ike zero limit cx finite fluctuate zero integral second term dampens fluctuations amount dampening depends pairing range sets limits integration figure used relatively long pairing range ms made fluctuations small even small hand short fluctuations would small large averaging large relevant amplitude small rate learning slow case takes long time significant synaptic changes accumulate plasticity effectively driven integrating long time periods eq brain nonvanishing spike correlations sometimes observed even oc limit unlike poisson spike trains correlations often roughly symmetric zero case produce little plasticity pairing functions antisymmetric figures lb hand spike correlations asymmetric could lead substantial effects effects recurrent network dynamics learning rules eq depend presynaptic postsynaptic rates like learning rules conventionally used neural networks special feature depend time derivatives computational consequences recurrent neural networks form wrr bi classical neural network equations derived biophysically realistic models using method averaging mean field approximation firing rate neuron conventionally identified xj cost function xi wij quantifies amount drift firing rate point state space network consider bi function wij defined gradient cost function respect wij given oe ow vj assuming rr monotonically increasing function xi follows differential hebbian update increases cost function spike based learning stabilization persistent neural activity hence increases magnitude drift velocity contrast differential antihebbian update decreases drift velocity suggests differential anti hebbian update could useful creating fixed points network dynamics persistent activity spiking autapse model preceding arguments drift velocity based approximate rate based descriptions learning network dynamics important implement spike based learning spiking network dynamics check approximations valid therefore numerically simulated simple recurrent circuit integrate fire neurons shown figure core circuit memory neuron makes excitatory autapse onto also receives synaptic input three input neurons tonic neuron excitatory burst neuron inhibitory burst neuron known circuit store shortterm memory analog variable persistent activity strengths autapse tonic synapse precisely tuned show tuning accomplished spikebased learning rule eq differential anti hebbian pairing function like figure memory neuron described equations iiiiiiiiiiiiilllllllllttlllllllllll tonic excitatory burst memory ii ii lltlllllltllll iiiiiiiiiii inhibitory burst figure circuit diagram autapse model gl ge dv dt dr membrane potential reaches es spike considered occurred reset et spike time causes jump synaptic activation size decays exponentially time constant next spike synaptic conductances memory neuron given wr woro term wr recurrent excitation autapse strength autapse synaptic activations tonic excitatory burst inhibitory burst neurons governed equations like differences neurons synaptic input firing patterns instead determined applied currents pp pp pp tonic neuron constant applied current makes fire repetitively roughly hz figure excitatory inhibitory burst neurons applied current normally zero except brief ms current pulses cause bursts action potentials shown figure synaptic strengths wo arbitrarily set learning burst neurons cause transient changes firing rate memory neuron applying spike based learning rule tune wo memory xie seung llllllllllll iiiiiiii iiiiiiilllll untuned tuned iiiiiiii ii iii sec ii iiiiiiiiiiiii ir lm iiiiiiiiiiiilllll figure untuned tuned autapse activity middle three traces membrane potentials three input neurons figure spikes drawn reset times integrate fire neurons learning activity memory neuron persistent shown top trace spike based learning rule applied synaptic weights burst inputs cause persistent changes activity cm nf mv mv vi mv vtnre mv eset mv ms iapp ha iapp ha ymo ms yn sy ms neuron able maintain persistent activity interburst intervals one burst next made synaptic changes using differential antihebbian pairing function sin rt spike time differences range ms resulting increase persistence time seen figure along values synaptic weights versus time quantify performance system maintaining persistent activity determined relationship dv dt using long sequence interburst intervals defined reciprocal interspike interval fixed optimally tuned values still residual drift shown figure parameters allowed adapt continuously even good tuning achieved residual drift even smaller magnitude learning rule tweaks synapfic weights interburst interval reducing drift particular firing rate autapse learning driven autocorrelation spike train rather crosscorrelation peak autocorrelogram zero lag effect since pairing function zero origin since autocorrelation zero small time lags used fairly large pairing range simulations recurrent network many neurons shorter pairing range would suffice cross correlation vanish near zero discussion shown differential anti hebbian learning tune recurrent circuit maintain persistent neural activity behavior understood reducing spike based learning rule rate based learning rules eqs rate based approximations good two conditions satisfied first pairing range must large rate learning must slow second spike synchrony must weak little effect learning due shape pairing function differential anti hebbian pairing function results learning rule uses hi negative feedback signal reduce amount drift firing rate illustrated simulations integrate fire neuron excitatory autapse generally learning rule could relevant tuning strength positive feedback networks maintain short term memory analog variable persistent neural activity spike based learning stabilization persistent neural activity wor ttme rate hz figure tuning autapse persistence time activity increases weights wo tuned transition driven pseudorandom bursts input systematic relationship drift dr dr firing rate measured long sequence interburst intervals weights continuously fine tuned drift less fixed well tuned weights example learning rule could used improve robustness oculomotor integrator head direction system mistuning parameters deriving differential forms learning rules assumed areas positive negative lobes pairing function equal integral defining vanishes reality cancellation might exact ratio would limit persistence time achieved learning rule oculomotor integrator head direction system also able integrate vestibular inputs produce changes activity patterns problem finding generalizations present learning rules train networks integrate still open
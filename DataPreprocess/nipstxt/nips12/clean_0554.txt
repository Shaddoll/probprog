abstract bayesian mixture model necessary priori limit number components finite paper infinite gaussian mixture model presented neatly sidesteps difficult problem finding right number mixture components inference model done using efficient parameter free markov chain relies entirely gibbs sampling introduction one major advantages bayesian methodology overfitting avoided thus difficult task adjusting model complexity vanishes neural networks demonstrated neal whose work infinite networks led reinvention popularisation gaussian process models williams rasmussen paper markov chain monte carlo mcmc implementation hierarchical infinite gaussian mixture model presented perhaps surprisingly inference models possible using finite amounts computation similar models known statistics dirichlet process mixture models go back ferguson antoniak usually expositions start dirichlet process west et al derive model limiting case wellknown finite mixtures bayesian methods mixtures unknown finite number components explored richardson green whose methods easily extended multivariate observations finite hierarchical mixture finite gaussian mixture model components may written jjv means sj precisions inverse variances rj mixing proportions must positive sum one iv normalised gaussian specified mean variance simplicity exposition initially assume scalar observations comprise training data yn first consider models fixed value later explore properties limit oc infinite gaussian mixture model gibbs sampling well known technique generating samples complicated multivariate distributions often used monte carlo procedures simplest form gibbs sampling used update variable turn conditional distribution given variables system shown gibbs sampling generates samples joint distribution entire distribution explored number gibbs sweeps grows large introduce stochastic indicator variables ci one observation whose role encode class generated observation indicators take values indicators often referred missing data mixture model context following sections priors component parameters hyperparameters specified conditional distributions needed gibbs sampling derived general form priors chosen hopefully reasonable modelling properties eye mathematical convenience use conjugate priors component parameters component means given gaussian priors jlx af whose mean precision hyperparameters common components hyperparameters given vague normal gamma priors ay exp ray mean variance observations shape parameter ay gamma prior set unity corresponding broad vague distribution conditional posterior distributions means obtained multiplying likelihood eq conditioned indicators prior eq ujlc sj jrtjsj xr yi rtj rtj nj ci occupation number number observations belonging class mean observations hyperparameters eq plays role likelihood together priors eq give conditional posteriors standard form trytry try kr try kr rlfil fi try component precisions given gamma priors sjl whose shape mean hype ameters co components priors inverse ga ga form exp stfictly spewing priors ought depend observations ent proced equivflent normrising obse ations using unit priors wide ety reasonable priors lead si results rasmussen conditional posterior precisions obtained multiplying likelihood eq conditioned indicators prior eq ssic tts nj tt ci hyperparameters eq plays role likelihood together priors eq give wlsl exp sjw exp latter density standard form shown log lsl sk log concave may generate independent samples distribution log using adaptive rejection sampling ars technique gilks wild transform get values mixing proportions rj given symmetric dirichlet also known multivariate beta prior concentration parameter ii rl rklc dirichlet mixing proportions must positive sum one given mixing proportions prior occupation numbers multinomial joint distribution indicators becomes cl ck rk jkronecker ci using standard dirichlet integral may integrate mixing proportions write prior directly terms indicators cl ii ld order able use gibbs sampling discrete indicators ci need conditional prior single indicator given others easily obtained eq keeping single indicator fixed ci jlc subscript indicates indexes except number observations excluding yi associated component posteriors indicators derived next section lastly vague prior inverse gamma shape put concentration parameter cro exp infinite gaussian mixture model likelihood may derived eq together prior eq gives akr alk cr ak exp notice conditional posterior depends number observations number components observations distributed among components distribution log ik log concave may efficiently generate independent samples distribution using ars infinite limit far considered fixed finite quantity section explore limit oc make final derivations regarding conditional posteriors indicators model variables except indicators conditional posteriors infinite limit obtained substituting number classes data associated krep equations previously derived finite model indicators letting oc eq conditional prior reaches following limits components ci jlc compo ci ci ilc nents combined shows conditional class prior components associated observations proportional number observations combined prior classes depends notice analytical tractability integral eq essential since allows us work directly finite number indicator variables rather infinite number mixing proportions may combine likelihood eq conditioned indicators prior eq obtain conditional posteriors indicators components rt ci tj sj oc ci jlc yil sj cr exp sj yi components combined ci ci ic oc ci ci fc yi sj uj sj jdsj likelihood components observations yi currently associated gaussian component parameters sj likelihood pertaining currently unrepresented classes parameters associated obtained integration prior distribution note need differentiate infinitely many unrepresented classes since parameter distributions identical unfortunately integral analytically tractable follow neal suggests sample priors gaussian gamma shaped order generate monte carlo estimate probability generating new class notice approach effectively generates parameters sampling prior classes unrepresented since monte carlo estimate unbiased resulting chain sample exactly desired distribution matter many samples used approximate integral found using single sample works fairly well many applications detail three possibilities computing conditional posterior class probabilities depending number observations associated class rasmussen rbi observations associated class posterior class probability given top line eq ci observation yi currently observation associated class peculiar situation since observations associated class class still parameters turns situation handled unrepresented class rather sampling parameters one simply uses class parameters consult neal detailed derivation unrepresented classes values mixture parameters picked random prior parameters gaussian gamma shaped classes parameters associated easily evaluate likelihoods gaussian priors take form components observations yi associated remaining class hitherto unrepresented classes chosen new class introduced model classes removed become empty inference spirals example illustrate model use dimensional spirals dataset ueda et al containing data point plotted figure five data points generated isotropic gaussians whose means follow spiral pattern resented components concentration shape figure cases three dimensional spirals data crosses represent single random sample posterior mixture model krep represented classes account mass lines indicate std dev gaussian mixture components thickness lines represent mass class right histograms samples posterior krep fi shown multivariate generalisation generalisation multivariate observations straightforward means zi precisions become vectors matrices respectively prior posterior infinite gaussian mixture model distributions become multivariate gaussian wishart similarly hyperparameter becomes vector multivariate gaussian prior become matrices wishart priors parameter stays scalar prior gamma mean dimension dataset specifications stay setting recovers scalar case discussed detail inference mixture model started single component large number gibbs sweeps performed updating parameters hyperparameters turn sampling conditional distributions derived previous sections figure auto covariance several quantities plotted reveals maximum correlation length iterations performed modelling purposes taking minutes cpu time pentium pc steps initially burn followed generate roughly independent samples posterior spaced evenly apart figure represented components one sample posterior visualised data right figure see posterior number represented classes concentrated around concentration parameter takes values around corresponding mass predictive distribution belonging unrepresented classes shape parameter takes values around gives effective number points contributed prior covariance matrices mixture components predictive distribution given particular state markov chain predictive distribution two parts represented classes gaussian unrepresented classes updating indicators may chose approximate unrepresented classes finite mixture gaussians whose parameters drawn prior final predictive distribution average eg samples posterior spirals data density roughly components represented classes plus however many used represent remaining mass attempted show distribution however one imagine smoothed version single sample shown figure averaging models slightly varying numbers classes parameters small mass unrepresented classes spreads diffusely entire observation range log og log mean iog det iog det glo iteration lag time monte carlo iteration figure left plot shows auto covariance length various parameters markov chain based iterations number represented classes krep significant correlation effective correlation length approximately computed sum covariance coefficients lag right hand plot shows number represented classes growing initial phase sampling initial iterations discarded rasmussen conclusions infinite hierarchical bayesian mixture model reviewed extended practical method shown good performance without overfitting achieved multidimensional data efficient practical mcmc algorithm free parameters derived demonstrated example model fully automatic without needing specification parameters vague prior corroborates falsity common misconception difference bayesian non bayesian methods prior arbitrary anyway tests variety problems reveals infinite mixture model produces densities whose generalisation highly competitive commonly used methods current work undertaken explore performance high dimensional problems terms computational efficiency generalisation infinite mixture model several advantages finite counterpart many applications may appropriate limit number classes number represented classes automatically determined use mcmc effectively avoids local minima plague mixtures trained optimisation based methods eg em ueda et al much simpler handle infinite limit work finite models unknown sizes richardson green traditional approaches based extensive crossvalidation bayesian infinite mixture model solves simultaneously several long standing problems mixture models density estimation acknowledgments thanks radford neal helpful comments naonori ueda making spirals data available work funded danish research councils computational neural network center connect thor center neuroinformatics
abstract present new learning architecture decision directed acyclic graph ddag used combine many two class classifiers multiclass classifier class problem ddag contains classifiers one pair classes present vc analysis case node classifiers hyperplanes resulting bound test error depends margin achieved nodes dimension space motivates algorithm dagsvm operates kernel induced feature space uses two class maximal margin hyperplanes decision node ddag dagsvm substantially faster train evaluate either standard algorithm max wins maintaining comparable accuracy algorithms introduction problem multiclass classification especially systems like svms present easy solution generally simpler construct classifier theory algorithms two mutually exclusive classes mutually exclusive classes believe constructing class svms still unsolved research pr oblem standard method class svms construct svms ith svm trained examples ith class positive labels examples negative labels refer svms trained way ov svms short oneversus rest final output svms class corresponds svm highest output value unfortunately bound generalization error svm training time standard method scales linearly another method constructing class classifiers svms derived previous research combining two class classifiers knerr suggested constructing possible two class classifiers training set classes classifier trained platt cristianini shawe taylor two classes would thus classifiers applied svms refer svms short one versus one knerr suggested combining two class classifiers gate friedman suggested max wins algorithm classifier casts one vote preferred class final result class votes friedman shows circumstances algorithm bayes optimal kref el applies max wins algorithm support vector machines excellent results significant disadvantage approach however unless individual classifiers carefully regularized svms overall class classifier system tend overfit combination method max wins combination method bounds generalization error finally size classifier may grow superlinearly hence may slow evaluate large problems section introduce new multiclass learning architecture called decision directed acyclic graph ddag ddag contains nodes associated classifier section present vc analysis ddags whose classifiers hyperplanes showing margins achieved decision nodes size graph affect performance dimensionality input space vc analysis indicates building large margin dags high dimensional feature spaces yield good generalization performance using bound guide section introduce novel algorithm multiclass classification based placing svms nodes ddag algorithm called dagsvm efficient train evaluate empirical evidence efficiency shown section decision dags directed acyclic graph dag graph whose edges orientation cycles rooted dag unique node node arcs pointing rooted binary dag nodes either arcs leaving use rooted binary dags order define class functions used classification tasks class functions computed rooted binary dags formally defined follows definition decision dags ddags given space set boolean functions class ddag decision dags classes functions implemented using rooted binary dag leaves labeled classes internal nodes labeled element nodes arranged triangle single root node top two nodes second layer final layer leaves th node layer connected th st node st layer evaluate particular ddag input starting root node binary function node evaluated node exited via left edge binary function zero right edge binary function one next node binary function evaluated value decision function value associated final leaf node see figure path taken ddag known evaluation path input reaches node graph node evaluation path refer decision node distinguishing classes node assuming number leaf class node th node th layer provided similarly nodes nodes involving class internal nodes two diagonals containing leaf labeled ddag equivalent operating list node eliminates one class list list initialized list classes test point evaluated decision node corresponds first last elements list node prefers large margin dags multiclass classification test points side hyperplane cannot class vs svm test points side hyperptane cannot class figure decision dag finding best class four classes equivalent list state node shown next node diagram input space four class problem svm exclude one class consideration one two classes class eliminated list ddag proceeds test first last elements new list ddag terminates one class remains list thus problem classes decision nodes evaluated order derive answer current state list total state system therefore since list state reachable one possible path system decision graph algorithm traverses dag simply tree decision dags naturally generalize class decision trees allowing efficient representation redundancies repetitions occur different branches tree allowing merging different decision paths class functions implemented generalized decision trees particular representation presents computational learning theoretical advantages analysis generalization paper study ddags node classifiers hyperplanes define perceptron ddag ddag perceptron every node let unit weight vector correctly splitting classes node threshold define margin node rninc class associated training example note definition take account examples class labels equal theorem suppose able classify random ra sample labeled examples using perceptron ddag classes containing decision nodes margins node bound generalization error probability greater less log ern log rn log radius ball containing distribution support proof see appendix platt cristianini shawe taylor theorem implies control capacity ddags enlarging margin note situations bound may pessimistic ddag partitions input space polytopic regions mapped leaf node assigned specific class intuitively margins matter ones relative boundaries cell given training point assigned whereas bound theorem depends margins graph observations would expect ddag whose node margins large would accurate identifying class even nodes large margins theorem substantiates showing appropriate bound depends node margins first introduce notation ej class misclassified misclassified class theorem suppose able correctly distinguish class classes random sample ddag classes containing decision nodes margins ej log em log log nodes radius ball containing support distribution proof follows exactly lemma theorem omitted dagsvm algorithm based previous analysis propose new algorithm called directed acyclic graph svm dagsvm algorithm combines results svms show combination method efficient train evaluate analysis section indicates maximizing margin nodes ddag minimize bound generalization error bound also independent input dimensionality therefore create ddag whose nodes maximum margin classifiers kernel induced feature space ddag obtained training node subset training points labeled final class decision derived using ddag architecture described section dagsvm separates individual classes large margin safe discard losing class decision hard margin case examples losing class far away decision surface see figure dagsvm choice class order list ddag arbitrary experiments section simply use list classes natural numerical alphabetical order limited experimentation ordering list yield significant changes accuracy performance dagsvm algorithm superior multiclass svm algorithms training evaluation time empirically svm training observed scale super linearly training set size according power law crn algorithms based decomposition method proportionality constant standard multiclass svm training algorithm entire training set used create classifiers hence training time cnm assuming classes number examples training svm requires training examples thus training svms would require cn large margin dags multiclass classification typical case amount time required train svms independent twice training single svm using svms combination algorithm thus preferred training time empirical comparisons conclusions dagsvm algorithm evaluated three different test sets usps handwritten digit data set uci letter data set uci covertype data set usps digit data consists classes whose inputs pixels scaled input image training examples test examples uci letter data consists classes whose inputs measured statistics printed font glyphs used first examples training last testing inputs uci letter data set scaled lie uci covertype data consists classes trees inputs terrain features training examples test examples continuous inputs covertype scaled zero mean unit variance discrete inputs represented code data set trained svms svms using smo soft margins combined svms max wins algorithm dagsvm choice kernel regularizing parameter determined via performance validation set validation performance measured training training set testing combination algorithm training set except covertype uci validation set used best kernel selected set polynomial kernels degree homogeneous inhomogeneous gaussian kernels various gaussian kernel always found best error kernel training cpu classifier size rate evaluations time sec kparameters usps max wins dagsvm neural net uci letter max wins dagsvm neural net uci covertype max wins dagsym neural net table experimental results table shows results experiments optimal parameters three multiclass svm algorithms similar data sets also error rates similar three algorithms data sets neither max wins statistically significantly better dagsvm using mcnemar test significance level usps uci letter uci covertype max wins slightly better either svm based algorithms results neural network trained data sets shown baseline accuracy comparison three algorithms distinguish training time evaluation time classifier size number kernel evaluations good indication evaluation time platt cristianini shawe taylor max wins number kernel evaluations total number unique support vectors svms dagsvm number kernel evaluations number unique support vectors averaged evaluation paths ddag taken test set seen table max wins faster svms due shared support vectors classifiers dagsvm fastest evaluation dagsvm factor times faster evaluate max wins dagsvm algorithm also substantially faster train standard svm algorithm factor times faster two data sets max wins algorithm shares similar training speed advantage svm basis functions drawn limited set shared across classifiers great savings classifier size number parameters dagsvm max wins comparable number parameters svm even though classifiers rather summary created decision dag architecture amenable vcstyle bound generalization error using bound created dagsvm algorithm places two class svm every node ddag dagsvm algorithm tested versus standard multiclass svm algorithm friedman max wins combination algorithm dagsvm algorithm yields comparable accuracy memory usage two algorithms yields substantial improvements training evaluation time appendix proof main theorem definition let set real valued functions say set points shattered relative rx xex real numbers rx indexed binary vectors indexed function fb satisfying bx lb bx rx fat shattering dimension fatr set function positive real numbers integers maps value size largest shattered set set finite maps infinity otherwise relevant example consider class ilwll quote following result theorem let lin restricted origin faty li points ball dimensions radius min bound generalization technique closely resembles technique used study perceptron decision trees give lemma theorem lemma bounds probability double sample first half zero error second error greater appropriate assume ddag classes nodes denote fatyli fat lemma let ddag classes decision nodes margins decision nodes satisfying ki fat fat continuous right following bound holds xy graph separates classes ij node fraction points mis log classified log em ki proof proof lemma omitted space reasons formally analogous proof lemma easily reconstructed large margin dags multiclass classification lemma applies particular ddag specified margin node practice observe quantities generating ddag hence obtain bound applied practice must bound probabilities uniformly possible margins arise give proof theorem proof main theorem must bound probabilities different margins first use standard result due vapnik page bound probability error terms probability discrepancy performance two halves double sample combine result lemma must consider possible patterns ki decision nodes largest allowed value ki fixed bound number possibilities hence applications lemma fixed since let sum st choosing log em log log applications lemma ensures probability statements failing hold less note replaced constant order ensure continuity right required application lemma upper bounded log em ki log em applying vapnik lemma page case probability statement theorem fails hold less details style proof omitted paper space constraints found
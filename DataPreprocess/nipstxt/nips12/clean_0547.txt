abstract present new learn architectur decis direct acycl graph ddag use combin mani two class classifi multiclass classifi class problem ddag contain classifi one pair class present vc analysi case node classifi hyperplan result bound test error depend margin achiev node dimens space motiv algorithm dagsvm oper kernel induc featur space use two class maxim margin hyperplan decis node ddag dagsvm substanti faster train evalu either standard algorithm max win maintain compar accuraci algorithm introduct problem multiclass classif especi system like svm present easi solut gener simpler construct classifi theori algorithm two mutual exclus class mutual exclus class believ construct class svm still unsolv research pr oblem standard method class svm construct svm ith svm train exampl ith class posit label exampl neg label refer svm train way ov svm short oneversu rest final output svm class correspond svm highest output valu unfortun bound gener error svm train time standard method scale linearli anoth method construct class classifi svm deriv previou research combin two class classifi knerr suggest construct possibl two class classifi train set class classifi train platt cristianini shaw taylor two class would thu classifi appli svm refer svm short one versu one knerr suggest combin two class classifi gate friedman suggest max win algorithm classifi cast one vote prefer class final result class vote friedman show circumst algorithm bay optim kref el appli max win algorithm support vector machin excel result signific disadvantag approach howev unless individu classifi care regular svm overal class classifi system tend overfit combin method max win combin method bound gener error final size classifi may grow superlinearli henc may slow evalu larg problem section introduc new multiclass learn architectur call decis direct acycl graph ddag ddag contain node associ classifi section present vc analysi ddag whose classifi hyperplan show margin achiev decis node size graph affect perform dimension input space vc analysi indic build larg margin dag high dimension featur space yield good gener perform use bound guid section introduc novel algorithm multiclass classif base place svm node ddag algorithm call dagsvm effici train evalu empir evid effici shown section decis dag direct acycl graph dag graph whose edg orient cycl root dag uniqu node node arc point root binari dag node either arc leav use root binari dag order defin class function use classif task class function comput root binari dag formal defin follow definit decis dag ddag given space set boolean function class ddag decis dag class function implement use root binari dag leav label class intern node label element node arrang triangl singl root node top two node second layer final layer leav th node layer connect th st node st layer evalu particular ddag input start root node binari function node evalu node exit via left edg binari function zero right edg binari function one next node binari function evalu valu decis function valu associ final leaf node see figur path taken ddag known evalu path input reach node graph node evalu path refer decis node distinguish class node assum number leaf class node th node th layer provid similarli node node involv class intern node two diagon contain leaf label ddag equival oper list node elimin one class list list initi list class test point evalu decis node correspond first last element list node prefer larg margin dag multiclass classif test point side hyperplan cannot class vs svm test point side hyperptan cannot class figur decis dag find best class four class equival list state node shown next node diagram input space four class problem svm exclud one class consider one two class class elimin list ddag proce test first last element new list ddag termin one class remain list thu problem class decis node evalu order deriv answer current state list total state system therefor sinc list state reachabl one possibl path system decis graph algorithm travers dag simpli tree decis dag natur gener class decis tree allow effici represent redund repetit occur differ branch tree allow merg differ decis path class function implement gener decis tree particular represent present comput learn theoret advantag analysi gener paper studi ddag node classifi hyperplan defin perceptron ddag ddag perceptron everi node let unit weight vector correctli split class node threshold defin margin node rninc class associ train exampl note definit take account exampl class label equal theorem suppos abl classifi random ra sampl label exampl use perceptron ddag class contain decis node margin node bound gener error probabl greater less log ern log rn log radiu ball contain distribut support proof see appendix platt cristianini shaw taylor theorem impli control capac ddag enlarg margin note situat bound may pessimist ddag partit input space polytop region map leaf node assign specif class intuit margin matter one rel boundari cell given train point assign wherea bound theorem depend margin graph observ would expect ddag whose node margin larg would accur identifi class even node larg margin theorem substanti show appropri bound depend node margin first introduc notat ej class misclassifi misclassifi class theorem suppos abl correctli distinguish class class random sampl ddag class contain decis node margin ej log em log log node radiu ball contain support distribut proof follow exactli lemma theorem omit dagsvm algorithm base previou analysi propos new algorithm call direct acycl graph svm dagsvm algorithm combin result svm show combin method effici train evalu analysi section indic maxim margin node ddag minim bound gener error bound also independ input dimension therefor creat ddag whose node maximum margin classifi kernel induc featur space ddag obtain train node subset train point label final class decis deriv use ddag architectur describ section dagsvm separ individu class larg margin safe discard lose class decis hard margin case exampl lose class far away decis surfac see figur dagsvm choic class order list ddag arbitrari experi section simpli use list class natur numer alphabet order limit experiment order list yield signific chang accuraci perform dagsvm algorithm superior multiclass svm algorithm train evalu time empir svm train observ scale super linearli train set size accord power law crn algorithm base decomposit method proportion constant standard multiclass svm train algorithm entir train set use creat classifi henc train time cnm assum class number exampl train svm requir train exampl thu train svm would requir cn larg margin dag multiclass classif typic case amount time requir train svm independ twice train singl svm use svm combin algorithm thu prefer train time empir comparison conclus dagsvm algorithm evalu three differ test set usp handwritten digit data set uci letter data set uci covertyp data set usp digit data consist class whose input pixel scale input imag train exampl test exampl uci letter data consist class whose input measur statist print font glyph use first exampl train last test input uci letter data set scale lie uci covertyp data consist class tree input terrain featur train exampl test exampl continu input covertyp scale zero mean unit varianc discret input repres code data set train svm svm use smo soft margin combin svm max win algorithm dagsvm choic kernel regular paramet determin via perform valid set valid perform measur train train set test combin algorithm train set except covertyp uci valid set use best kernel select set polynomi kernel degre homogen inhomogen gaussian kernel variou gaussian kernel alway found best error kernel train cpu classifi size rate evalu time sec kparamet usp max win dagsvm neural net uci letter max win dagsvm neural net uci covertyp max win dagsym neural net tabl experiment result tabl show result experi optim paramet three multiclass svm algorithm similar data set also error rate similar three algorithm data set neither max win statist significantli better dagsvm use mcnemar test signific level usp uci letter uci covertyp max win slightli better either svm base algorithm result neural network train data set shown baselin accuraci comparison three algorithm distinguish train time evalu time classifi size number kernel evalu good indic evalu time platt cristianini shaw taylor max win number kernel evalu total number uniqu support vector svm dagsvm number kernel evalu number uniqu support vector averag evalu path ddag taken test set seen tabl max win faster svm due share support vector classifi dagsvm fastest evalu dagsvm factor time faster evalu max win dagsvm algorithm also substanti faster train standard svm algorithm factor time faster two data set max win algorithm share similar train speed advantag svm basi function drawn limit set share across classifi great save classifi size number paramet dagsvm max win compar number paramet svm even though classifi rather summari creat decis dag architectur amen vcstyle bound gener error use bound creat dagsvm algorithm place two class svm everi node ddag dagsvm algorithm test versu standard multiclass svm algorithm friedman max win combin algorithm dagsvm algorithm yield compar accuraci memori usag two algorithm yield substanti improv train evalu time appendix proof main theorem definit let set real valu function say set point shatter rel rx xex real number rx index binari vector index function fb satisfi bx lb bx rx fat shatter dimens fatr set function posit real number integ map valu size largest shatter set set finit map infin otherwis relev exampl consid class ilwll quot follow result theorem let lin restrict origin fati li point ball dimens radiu min bound gener techniqu close resembl techniqu use studi perceptron decis tree give lemma theorem lemma bound probabl doubl sampl first half zero error second error greater appropri assum ddag class node denot fatyli fat lemma let ddag class decis node margin decis node satisfi ki fat fat continu right follow bound hold xy graph separ class ij node fraction point mi log classifi log em ki proof proof lemma omit space reason formal analog proof lemma easili reconstruct larg margin dag multiclass classif lemma appli particular ddag specifi margin node practic observ quantiti gener ddag henc obtain bound appli practic must bound probabl uniformli possibl margin aris give proof theorem proof main theorem must bound probabl differ margin first use standard result due vapnik page bound probabl error term probabl discrep perform two halv doubl sampl combin result lemma must consid possibl pattern ki decis node largest allow valu ki fix bound number possibl henc applic lemma fix sinc let sum st choos log em log log applic lemma ensur probabl statement fail hold less note replac constant order ensur continu right requir applic lemma upper bound log em ki log em appli vapnik lemma page case probabl statement theorem fail hold less detail style proof omit paper space constraint found
abstract paper presents novel practical framework bayesian model averaging model selection probabilistic graphical models approach approximates full posterior distributions model parameters structures well latent variables analytical manner posteriors fall free form optimization procedure naturally incorporates conjugate priors unlike large sample approximations posteriors generally nongaussian hessian needs computed predictive quantities obtained analytically resulting algorithm generalizes standard expectation maximization algorithm convergence guaranteed demonstrate approach applied large class models several domains including mixture models source separation introduction standard method learn graphical model data maximum likelihood ml given training dataset ml estimates single optimal value model parameters within fixed graph structure however nil well known tendency overfit data overfitting becomes severe complex models involving high dimensional real world data images speech text another problem nil prefers complex models since parameters fit data better hence nil cannot optimize model structure bayesian framework provides principle solution problems rather focusing single model bayesian considers whole finite infinite class models model posterior probability given dataset computed predictions test data made averaging predictions individual models weighted posteriors thus bayesian framework avoids overfitting integrating parameters addition complex models automatically penalized assigned lower posterior probability therefore optimal structures identified unfortunately computations bayesian framework intractable even use term model refer collectively parameters structure attias simple cases factor analysis see existing approximation methods fall two classes markov chain monte carlo methods large sample methods laplace approximation mcmc methods attempt achieve exact results typically require vast computational resources become impractical complex models high data dimensions large sample methods tractable typically make drastic approximation modeling posteriors parameters normal even parameters positive definite covariance matrices addition require computation hessian may become quite intensive paper present variational bayes vb practical framework bayesian computations graphical models vb draws together variational ideas intractable latent variables models bayesian inference turn draw work framework facilitates analytical calculations posterior distributions hidden variables parameters structures posteriors fall free form optimization procedure naturally incorporates conjugate priors emerge standard forms one normal computed via iterative algorithm closely related expectation maximization em whose convergence guaranteed hessian needs computed addition averaging models compute predictive quantities performed analytically model selection done using posterior structure particular bic mdl criteria emerge limiting case general framework restrict attention paper directed acyclic graphs dags bayesian networks let yl denote visible data nodes runs data instances let xx denote hidden nodes let denote parameters simply additional hidden nodes distributions model fixed structure fully defined joint distribution im dag joint factorizes nodes ip ui pai oi ui pai set parents ui oi parametrize edges directed toward ui addition usually assume independent instances np yn xn shall also consider set structures controls number hidden nodes functional forms dependencies ui pai including range values assumed node number components mixture model associated set structures structure prior marginal likelihood posterior parameters fixed structure interested two quantities first parameter posterior distribution second marginal likelihood im also known evidence assigned structure data following reference usually omitted always implied quantities obtained joint models hidden nodes required computations often performed analytically however presence hidden nodes quantities become computationally intractable shall approximate using variational approach follows consider joint posterior hidden nodes parameters since intractable consider variational posterior restricted factorized form xly given data parameters hidden nodes independent variational baysjan framework graphical models restriction key makes approximate tractable notice require complete factorization parameters hidden nodes may still correlated amongst compute optimizing cost function rm defined log logp penalizing complex models see vb objective function penalizes complexity useful rewrite logp kl ii average first term taken term corresponds averaged likelihood second term kl distance prior posterior parameters number parameters increases kl distance follows consequently reduces penalized likelihood interpretation becomes transparent large sample limit oe parameter posterior sharply peaked probable value shown kl penalty reduces log linear number parameters structure rm corresponds precisely bayesian information criterion bic minimum description length criterion mdl see thus popular model selection criteria follow limiting case vb framework free form optimization em like algorithm rather assuming specific parametric form posteriors let fall free form optimization vb objective function results iterative algorithm directly analogous ordinary em step compute posterior hidden nodes solving sq get ogp xio average taken step rather optimal parameters compute posterior distribution parameters solving get gp xl xp average taken concept conjugate priors becomes useful denoting exponential term choose prior family distributions belongs family said conjugate procedure allows us select prior fairly large family distributions includes non informative ones limiting cases first inequality holds arbitrary follows jensen inequality see becomes equality true posterior note always understood include conditioning since bounded marginal likelihood obtain optimal posteriors maximizing shown equivalent minimizing kl distance true posterior thus optimizing produces best approximation true posterior within space distributions satisfying well tightest lower bound true marginal likelihood attias thus compromise generality facilitating mathematical simplicity elegance particular learning vb framework simply amounts updating hyperparameters transforming prior parameters posterior parameters point use conjugate priors widespread statistics far could applied models nodes visible structure posterior compute exploit jensen inequality define general objective function memq ogp logp computing structure posterior obtained free form optimization cr hence prior assumptions likelihood different structures encoded prior affect selection optimal model structures performed according predictive quantities ultimate goal bayesian inference estimate predictive quantities density regression function generally quantities computed averaging models weighting model posterior vb framework exact model averaging approximated replacing true posterior variational density estimation example density assigned new data point given yly fdo oly situations source separation estimate hidden node values new data may required relevant quantity conditional likely value hidden nodes extracted vb approximates cr fdo oiy variational bayes mixture models mixture models investigated analyzed extensively many years however well known problems regularizing likelihood divergences determining required number mixture components still open whereas theory bayesian approach provides solution satisfactory practical algorithm emerged application involved sampling techniques approximation methods problem present solution provided vb consider models form yn yn sn sn denotes nth observed data vector denotes hidden component generated components labeled structure parameter denoting number components whereas approach applied arbitrary models simplicity consider normal component distributions yn sn jv rs mean rs precision inverse covariance matrix mixing proportions rs hindsight use conjugate priors parameters rs rs mixing proportions jointly dirichlet rs means conditioned precisions normal rs precisions wishart rs find parameter posterior fixed variational baysian framework graphical models factorizes rs posteriors obtained following iterative algorithm termed vb mog step compute responsibilities instance using sn yn tf noting ri expression resembles responsibilities ordinary ml differences stem integrating parameters special quantities log log rs log log ei log dlog dlogf dx digaroma function averages taken parameters described step compute parameter posterior two stages first compute quantities es cs nss stage identical step ordinary em produces new parameters vb however quantities help characterize new parameter posteriors posteriors functionally identical priors different parameter values mixing proportions jointly dirichlet rs means normal tq ifs ps precisions wishart fs bs posterior parameters updated second stage using simple rules ao opo po po final values posterior parameters form output vb mog remark whereas specific assumptions made parameter posteriors emerge suitable non trivial generally non normal functional forms computational overhead vb mog compared em minimal coverlance parameter posterior vbmog reduces em regularized priors vb mog divergence problems stability guaranteed existence objective function finally approximate marginal likelihood rm required optimize number components via also obtained closed form omitted predictive density using posteriors integrate parameters show density assigned model new data vector mixture student distributions st ylps component cos mean covariance co proportion reduces nonlinear regression may divide data vector input output parts yi yo use model estimate regression function yi error spheres may extracted conditional ps whmh also turns mixture student distributions means linear covariances mixing proportions nonlinear given terms posterior parameters attias buffalo post office digits misclassification rate histogram figure vb mog applied handwritten digit recognition vb mog applied boston housing dataset uci machine learning repository inputs used predict single output house price random divisions dataset training test points used resulting average mse whereas discriminative method nevertheless competitive breiman bagging technique using regression trees mse comparison em achieved mse classification separate parameter posterior computed class training dataset yc test data vector classified according conditional yc form identical dependent parameters multiplied relative size yc vb mog applied buffalo post office dataset contains examples digit digit gray level pixel array see examples fig left used random digit batches training separate batch testing average misclassification rate obtained using components em achieved misclassification histograms vb solid em dashed shown fig right vb intractable models blind separation example discussion far assumed free form optimization vb objective function feasible unfortunately many interesting models particular models ordinary ml intractable case models modify vb procedure follows specify parametric functional form posterior hidden nodes optimize parameters spirit let parameter posterior fall free form optimization illustrate approach context blind source separation bss problem see problem described yn hxn un xn unobserved dim source vector instance unknown mixing matrix noise un normally distributed unknown precision hi task construct source estimate observed dim data sources independent non normally distributed assume high kurtosis distribution oc cosh appropriate modeling speech sources one important heretofore unresolved problem bss determining number rn sources data another avoid overfitting mixing matrix problems typical ml algorithms remedied using vb non normal nature sources renders source posterior intractable even bayesian treatment use normal variational posterior iina xn pn rn instance dependent mean precision mixing matrix posterior emerges normal simplicity optimized rather integrated resulting vb bss algorithm runs follows variational baysjan framework graphical models looo log pr source reconstruction error snr db figure application vb blind source separation algorithm see text step optimize variational mean pn iterating convergence fixed point equation afit yn fipn tanhpn pn source covariance conditioned data variational precision matrix turns independent ta step update mean precision posterior rules omitted algorithm applied dim data generated linearly mixing mseclong speech music signals obtained commercial cds gaussian noise added different snr levels uniform structure prior used resulting posterior number sources fig left peaked correct value sources reconstructed test data via log reconstruction error plotted vs snr fig right solid ml error includes model averaging also shown dashed larger reflecting overfitting conclusion vb framework applicable large class graphical models fact may integrated junction tree algorithm produce general inference engines minimal overhead compared ml ones dirichlet normal wishart posteriors special models treated emerge general feature current research efforts include applications multinomial models learning structure complex dynamic probabilistic networks acknowledgements thank matt beal peter dayan david mackay carl rasmussen especially zoubin ghahramani important discussions references attias independent factor analysis neural computation bishop variational principal component analysis proc th icann chickering heckerman efficient approximations marginal likelihood bayesian networks hidden variables machine learning hinton van camp keeping neural networks simple minimizing description length weights proc th colt jaakkola jordan bayesian logistic regression variational approach statistics artificial intelligence smyth madigan eds neal hinton view em algorithm justifies incremental sparse variants learning graphical models jordan ed kluwer academic press norwell richardson green bayesian analysis mixtures unknown number components journal royal statistical society saul jaakkola jordan mean field theory sigmoid belief networks journal artificial intelligence research waterhouse mackay robinson bayesian methods mixture experts nips touretzky et al eds mit press
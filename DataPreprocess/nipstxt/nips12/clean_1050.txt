abstract problem reinforcement learning non markov environment explored using dynamic bayesian network conditional independence assumptions random variables compactly represented network parameters parameters learned line approximations used perform inference compute optimal value function relative effects inference value function approximations quality final policy investigated learning solve moderately difficult driving task two value function approximations linear quadratic found perform similarly quadratic model sensitive initialization performed level human performance task dynamic bayesian network performed comparably model using localist hidden state representation requiring exponentially fewer parameters introduction reinforcement learning rl addresses problem learning act maximize reward signal provided environment online rl algorithms try find policy maximizes expected time discounted reward experience performing sample backups learn value function states state action pairs decision problem markov observable states optimal value function state action pairs yields information required find optimal policy decision problem complete knowledge environment available states different may look uncertainty called perceptual aliasing causes decision problems dynamics non markov perceived state correspondence address learning factored representations pomdps partially observable markov decision processes many interesting decision problems markov inputs partially observable markov decision process pomdp formalism assumed process markov respect unobserved hidden random variable state variable time denoted dependent state previous time step action performed currently observed evidence assumed independent previous states observations given current state state hidden variable known certainty belief state maintained instead time step beliefs updated using bayes theorem combine belief state previous time step passed model system dynamics newly observed evidence case discrete time finite discrete state actions pomdp typically represented conditional probability tables cpts specifying emission probabilities state transition probabilities expected rewards states actions corresponds hidden markov model hmm distinct transition matrix action hidden state represented single random variable take one values exact belief updates computed using bayes rule value function discrete state real valued belief state shown value function piecewise linear convex worst case number linear pieces grows exponentially problem horizon making exact computation optimal value function intractable notice localist representation state encoded single random variable exponentially inefficient encoding bits information state process requires possible hidden states bode well abilities models use representation scale problems high dimensional inputs complex non markov structure factored representations bayesian network compactly represent state system set random variables two time slice dynamic bayesian network dbn represents system two time steps conditional dependencies random variables time time within time step represented edges directed acyclic graph conditional probabilities stored explicitly parameterized weights edges graph network densely connected inference intractable approximate inference methods include markov chain monte carlo variational methods belief state simplification applying dbn large problem three distinct issues disentangle well parameterized dbn capture underlying pomdp much dbn hurt approximate inference good must approximation value function achieve reasonable performance try tease issues apart looking performance dbn problem moderately large state space non markov structure algorithm use fully connected dynamic sigmoid belief network dsbn units time slice see figure random variables si binary conditional proba sallans time figure architecture dynamic sigmoid belief network circles indicate random variables filled circle observed empty circle unobserved squares action nodes diamonds rewards bilities relating variables adjacent time steps encoded action specific weights wik weight th unit time step th unit time step assuming action taken time nonlinearity usual sigmoid function exp note bias incorporated weights clamping one binary units observed variables assumed discrete conditional distribution output given hidden state multinomial parameterized output weights probability observing output value given exp rkk exp ukl denotes output weight hidden unit output value approximate inference inference fully connected bayesian network intractable instead use variational method fully factored approximating distribution stls ps variational parameters optimized standard mean field approximation sigmoid belief network parameters optimized iterating mean field equations converge iterations values variational parameters time held fixed computing values step analogous running forward portion hmm forward backward algorithm parameters dsbn optimized online using stochastic gradient ascent exp log likelihood learning factored representations pomdps transition emission matrices respectively aw au learning rates vector contains fully factored approximate belief state vector zeros one tth place notation denotes th element vector column matrix approximating value function computing optimal value function also intractable factored state space representation appropriate natural extreme assume state action value function decomposed way vst qf simplifying assumption still enough make finding optimal value function tractable even states completely independent qk would still piecewise linear convex number pieces scaling exponentially horizon test two approximate value functions linear approximation quadratic approximation tx qk ba parameters approximations notation denotes column matrix denotes matrix transpose denotes element wise vector multiplication update term factored approximation modified learning rule corresponds delta rule target input maxa qv lu qk qk eb ba bat eb oz learning rate temporal discount factor eb bellman residual eb maxqr xt txt experimental results new york driving task involves navigating slower faster one way traffic multi lane highway speed agent fixed must change lanes avoid slower cars move way faster cars agent remains front faster car driver fast car honk horn resulting reward instead colliding slower car agent squeeze past lane resulting reward time step horns lane squeezes constitutes clear progress rewarded see detailed description task sallans table sensory input new york driving task dimension size values hear horn yes gaze object truck shoulder road gaze speed looming receding gaze distance far near nose gaze refined distance far half near half gaze colour red blue yellow white gray tan modified version new york driving task used test algorithm task essentially described except gaze side gaze direction inputs removed see table list modified sensory inputs performance number algorithms approximations measured task random policy learning sensory inputs model localist representation hidden state consisted single multinomial random variable linear quadratic approximate value functions dsbn mean field inference linear quadratic approximations human driver localist representation used linear learning approximation corresponding quadratic approximation quadratic approximations trained random initialization initialization corresponding learned linear models random quadratic portion non human algorithms trained iterations case constant learning rate temporal decay rate used human driver author trained iterations using simple character based graphical display iteration lasting seconds stochastic policies used rl algorithms actions chosen boltzmann distribution temperature decreasing time atl zs exp qv dsbn hidden units per time slice localist model used multinomial states learner table representation entries training non human algorithm tested trials time steps human tested time steps results renormalized comparison methods results shown figure results negative lower numbers indicate better performance graph error bars show one standard deviation across trials little performance difference localist representation dsbn expected dsbn exponentially efficient hidden state representation linear quadratic approximations performed comparably well human performance however dsbn quadratic approximation sensitive initialization initialized random parameter settings failed find good policy however converge reasonable policy linear portion quadratic model initialized previously learned linear model hidden units dsbn encode useful features input whether car near nose position also encode history current gaze direction advantages simple stochastic policy learned via learning learner knows oncoming car randomly select look left right dsbn systematically looks left right wasting fewer actions learning factored representations pomdps qcl algorithm qdr figure results new york driving task nine algorithms random learning lc linear multinomial qcr quadratic multinomial random init qcl quadratic multinomial linear init ld linear dsbn qdr quadratic dsbn random init qdl quadratic dsbn linear init human discussion dsbn performed better standard learner comparably model localist representation despite using approximate inference exponentially fewer parmeters encouraging since efficient encoding state prerequisite tackling larger decision problems less encouraging value function approximation compared human performance clear methods far optimal although factored approximation dsbn hurt performance relative localist multinomial representation sensitivity initialization quadratic approximation worrisome success initializing simpler model suggests staged learning may appropriate simple models learned used initialize complex models findings echo context learning non factored approximate value function number related works fields reinforcement learning bayesian networks use sigmoid belief network mean field approximation given discussed context time series models fully factored approximation approximate inference dynamic bayesian networks discussed additive factored value function used context factored mdps hidden state linear learning approximation given approximate inference combined sophisticated value function approximation knowledge first attempt explore practicality combining techniques order solve single problem several possible extensions described representation learned dsbn tuned task hand reinforcement information could used guide learning dsbn parameters also done reinforcement signals would provide additional evidence state pomdp could used aid inference sophisticated function approximation could used finally although method appears work practice guarantee reinforcement learning converge view work encouraging first step much study required conclusions shown dynamic bayesian network used construct compact representation useful solving decision problem hidden state parameters dbn learned experience learning occurs despite use simple value sallans function approximations mean field inference approximations value function result good performance clearly far optimal fully factored assumptions made belief state value function appear impact performance compared non factored model algorithm presented runs entirely line performing forward inference much room future work including improving utility factored representation learned quality approximate inference value function approximation acknowledgments thank geoffrey hinton zoubin ghahramani andy brown helpful discussions anonymous referees valuable comments criticism particularly peter dayan helpful discussions comments early draft paper research funded nserc canada gatsby charitable foundation
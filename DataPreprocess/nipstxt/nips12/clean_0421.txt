abstract introduce algorithm estimating values function set test points xt xt ra given set training points xl yl xt yt without estimating intermediate step regression function demonstrate direct transducrive way estimating values regression classification pattern recognition accurate traditional one based two steps first estimating function calculating values function points interest introduction following consider general scheme transductive inference suppose exists function fo observe measurements corrupted noise yl xt yt yi find algorithm using given set training data given set test data xt xg ra selects set functions function fa xlxl yl xt yt xt xt minimizes points interest functional fa xi xl yl xt yt xt xt expectation taken training data given vector value test data given usually problem estimating values function points interest solved two steps first given set functions one estimates regression function minimizes functional df chapelle vapnik weston inductive step using estimated function calculate values points interest deductive step note however estimation function equivalent estimating values continuum points domain function therefore solving regression problem using restricted amount information looking general solution required shown using direct estimation method one obtain better bounds two step procedure article develop idea introduced estimating values function given points material organized follows section consider classical inductive ridge regression procedure leave one technique used measure quality solutions section introduces transductive method inference estimation values function based leave one technique section experiments demonstrate improvement given transductive inference compared inductive inference regression pattern recognition presented finally section summarizes results ridge regression leave one procedure order describe transductive method let us first discuss classical twostep inductive plus deductive procedure ridge regression consider set functions linear parameters minimize expected loss unknown minimize following empirical functional called ridge regression functional jte fixed positive constant called regularization parameter minimum given vector coefficients ozt oz xl yl xt yt ktk ry yl yt matrix elements kij ckj xi problem choose value provides small expected loss training sample st xl yl xt yt purpose would like choose fv minimizing also minimizes fv df df transductive inference estimating values funca ons since unknown one cannot estimate minimum directly solve problem instead use leave one procedure almost unbiased estimator leave one error algorithm training sample st ttoo yd leave one procedure consists removing training data one element say xi yi constructing regression function basis remaining training data testing removed element fashion one tests elements training data using different decision rules minimum consider minimum since expectation coincides ridge regression one derive closed form expression leave one error denoting kt error incurred leave one procedure rl kt xt xt let minimum vector yo ktk oi kt xt xt xt xt ridge regression estimate unknown values yt leave one error transductive inference transductive inference goal find algorithm minimizes functional using training data test data suggest following method predict yt finding values minimize leave one error ridge regression training joint set xl yl yt achieved following way suppose treat unknown values yt yt variables fixed value variables minimize following empirical functional ly functional differs second term functional corresponds performing ridge regression extra pairs chapelle vapnik weston suppose vector taken set pairs considered sample drawn distribution pairs xl yi xl case leave one error minimizing set approximates functional measure leaveone error using technique ridge regression using closed form one obtains tl ffly ki ii denote xl xt yl yt yt yt rrr rij fij let us rewrite expression equivalent form separate terms terms introducing ra matrix elements mo cikck ckk obtain equivalent expression ttoo ly order minimize leave one procedure valid required pairs drawn distribution pairs xi satisfy constraint choose vectors set iiy vector yo solution obtained classical ridge regression minimize constraint use functional tz tm plly constant depending find values given points interest remains find minimum note matrix obtained using vectors therefore find minimum functional rewrite equation ff ytmoy tmiy tm plly mo transductive inference estimating values functions matrix matrix matrix taking derivative obtain condition solution gives predictions yo algorithm call transductive regression two parameters control choice ff found using leave one estimator ridge regression leaves free parameter experiments compare one step transductive approach classical two step approach performed series experiments regression problems also describe experiments applying technique problem pattern recognition regression conducted computer simulations regression problem using two datasets delve repository boston kin fh boston dataset well known problem one required estimate house prices according various statistics based locational economic structural features data collected census service boston massachusetts area kin fh dataset realistic simulation forward dynamics link revolute robot arm task predict distance end effector target given inputs contain information joint positions twist angles forth problems nonlinear contain noisy data objective compare transductive inference method directly inductive method ridge regression chose set basis functions exp ii xill rr found values rr ridge regression minimized leave one bound used values parameters transductive approach using basis functions exp iix rr chose fixed value boston dataset followed experimental setup partitioned training set observations randomly times training set observations testing set observations chose values rr taking minimum average leave one error five random splits data stepping parameter space minimum found log transductive method also chose parameter figure la plot mean squared error mse test set averaged runs log rr ridge regression transductive regression transductive regression outperforms ridge regression especially minimum observe influence number test points generalization ability transductive method ran experiments setting chapelle vapnik weston transductive regression ridge regre log sigma transductive regression ridge regression log sigma uj transductive regression dge regress uj test set size transductive regression ridge regression test set size figure comparison transductive regression ridge regression boston dataset error rates varying rr varying test set size kin fh dataset error rates varying rr varying test set size different values figure lb plot mse testing set log rr results indicate increasing test set size gives improved performance transductive regression ridge regression course size testing set influence generalization ability performed similar experiments kin fh dataset time interested large testing sets giving improved performance transductive regression chose splits took subset observations training testing leave one estimator used find values log ridge regression transductive regression also chose parameter plotted mse testing set log rr figure lc size test set log rr also figure ld two algorithms large test set sizes method outperforms ridge regression pattern recognition technique also applied pattern recognition problems solving based minimizing functional technique known linear discriminant ld technique transductive inference estimating values functions ab ab svm tld postal banana diabetes titanic breast cancer heart thyroid table comparison percentage test error adaboost ab regularized adaboost ab support vector machines svm transductive linear discrimination tld seven datasets table describes results experiments classification following problems class digit recognition versus splitting training set runs observations considering testing set observations six problems uci database followed experimental setup performance classifier measured average error one hundred partitions datasets training testing sets free parameter chosen via validation first five training datasets performance transductive ld technique compared support vector machines adaboost regularized adaboost interesting note spite fact ld technique one simplest pattern recognition techniques transductive inference based upon method performs well compared state art methods pattern recognition summary article performed transductive inference problem estimating values functions points interest demonstrate estimating unknown values via one step transductive procedure accurate traditional two step inductive plus deductive one
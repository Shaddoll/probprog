abstract many hierarchical clustering algorithms available lack firm statistical basis set hierarchical probabilistic mixture model data generated hierarchical tree structured manner markov chain monte carlo mcmc methods demonstrated used sample posterior distribution trees containing variable numbers hidden units introduction past decade two mixture models become popular approach clustering competitive learning problems advantage well defined objective function fit general trend viewing neural network problems statistical framework however one disadvantage produce flat cluster structure rather hierarchical tree structure returned clustering algorithms agglomerative single link method see paper formulate hierarchical mixture model retains advantages statistical framework also features tree structured hierarchy basic idea illustrated figure root tree level single centre marked mean gaussian large variance represented large circle random number centres case sampled level gaussian produce new centres marked variance associated level gaussians smaller number level units produced associated level gaussians centre level unit marked sampled parent gaussian hierarchical process could continued indefinitely example generate data level gaussians shown dots figure three level version model would standard mixture model gaussian prior centres located four level model third level centres clumped together around second level means distinguishes model flat mixture model another view generative process given figure tree structure denotes nodes children particular parents note also directed acyclic graph arrows denoting dependence position child parent mcmc approach hierarchical mixture modelling section describe theory probabilistic hierarchical clustering give discussion related work experimental results described section figure basic idea hierarchical mixture model denotes root tree second level centres denoted third level centres data generated third level centres sampling random points gaussians whose means third level centres corresponding tree structure theory describe turn prior trees ii calculation likelihood given data vector iii markov chain monte carlo mcmc methods inference tree structure given data iv related work prior trees describe first prior number units layer prior connections layers consider layer hierarchical model root node level nodes level nodes level collected together vector use markovian model ln nllnl currently taken poisson distributions offset rti irti po xini xi parameter associated level offset used must always least one unit layer given next consider tree formed tree structure describes node ith layer parent node th layer unit indicator vector stores index parent attached collect indicator vectors together matrix denoted probability node layer connecting node layer taken rt thus ii describe generation random tree given simplicity describe generation points although everything extended arbitrary dimension easily mean level gaussian origin lit easy relax assumption ttt prior gaussian distribution located point origin williams level means tt generated tt cry cr variance associated level node similarly position level node generated level parent displacement position level parent displacement gaussian rv zero mean variance cr process continues visible variables order model useful require cr cr cr variability introduced successive levels declines monotonically cf scaling wavelet coefficients calculation likelihood data observe positions points final layer denoted calculate likelihood model need integrate locations means hidden variables levels done explicitly however shorten calculation realizing given generative distribution observables gaussian covariance matrix calculated follows consider two leaf nodes indexed gaussian rvs generated position two leaves denoted xk wk xt calculate covariance xk xt simply calculate xkxt depends crucially many shared nodes cf path analysis example nodes lie different branches tree level covariance zero variance sum variances rv tree covariance xk xt determined finding level tree common parent occurs assumptions log likelihood given lxtc nl log log ic fact calculation speeded taking account tree structure see note also posterior means variances hidden variables calculated based covariances hidden visible nodes calculation carried efficiently see pearl section details inference given problem trying infer connectivity structure given observations course interested posterior distribution zix one approach use markov chain monte carlo mcmc method sample posterior distribution straightforward way use metropolis algorithm propose changes structure changing parent single node time note similarities algorithm work williams adams dynamic trees dts main differences disconnections allowed maintain single tree rather forest ii variables dt image models discrete rather gaussian also need consider moves change effected split merge move split direction consider node parent several children split node randomly assign children two split nodes split nodes keeps parent probability accepting move metropolis hastings scheme nt nt nt nt min lx mcmc approach hierarchical mixture modelling proposal probability configuration given configuration scheme based work mcmc model composition mg madigan york green work reversible jump mcmc another move changes remove dangling nodes nodes children occurs nodes given layer decide use one nodes layer alternative sampling posterior use approximate inference mean field methods currently investigated dt models related work large number papers hierarchical clustering work focussed expressing hierarchical clustering terms probabilistic models example ambros ingerson et al mozer developed models idea cluster data coarse level subtract mean cluster residuals recursively paper seen probabilistic interpretation idea reconstruction phylogenetic trees biological sequence dna protein information gives rise problem inferring binary tree data durbin et al chapter show probabilistic formulation problem developed link agglomerative hierarchical clustering algorithms approximations full probabilistic method see much biological sequence work uses discrete variables diverges somewhat focus current work however work edwards concerns branching brownian motion process similarities model described important differences edwards model continuous time variances particles derived wiener process variance proportional lifetime particle contrast decreasing sequence variances given number levels assumed model one important difference model discussed paper phylogenetic tree model points higher levels phylogenetic tree taken individuals earlier time evolutionary history interpretation require different notion hierarchy mixture models found work autoclass system describe model involving class hierarchy inheritance trees specify dimensions sharing parameters occurs means covariance matrices gaussians contrast model paper creates hierarchy examples labelled rather dimensions xu pearl discuss inference tree structured belief network based knowledge covariances leaf nodes algorithm cannot applied directly case covariances known although note multiple runs given tree structure available covariances might approximated using sample estimates ideas concerning hierarchical clustering discussed experiments describe two sets experiments explore ideas searching fixed level random trees generated prior using values cr cr cr trees leaf williams nodes average tree kept generafive tree sampling carried starting random initial configuration given node proposes changing parent proposal accepted rejected usual metropolis probability one sweep node levels makes move level nodes one parent point move obtain representative sample ln run chain long possible however also use chain find configurations high posterior probability case running longer increases chances finding better configuration experiments sampler run sweeps uniform fixed posterior simply proportional likelihood term would also possible run simulated annealing move set search explicitly maximum posteriori map configuration results cases tree highest posterior probability hpp configuration higher posterior probability generative tree cases tree found cases hpp solution inferior generative tree fact almost cases sampler found configuration good better generative one relatively small number sweeps encouraging figure generative left column hpp trees fixed middle column plotted two examples panel note dangling node level means level nodes left end inferior configuration contrast panel sampler found better less tangled configuration generative model figure show generafive trees two examples corresponding hpp trees fixed plotted variable number panel log posterior probability configuration nodes levels shown located posterior means apparent non tree structures caused two nodes plotted almost top searching given data usually know appropriate numbers hidden units motivates searching achieved using split merge moves discussed section experiments initial numbers units levels denoted mcmc approach hierarchical mixture modelling set using simple minded formulae dim xal proper inferential calculation carried requires solution non linear optimization problem given initial connection configuration chosen randomly search method used propose split merge move probability level sample level level connections propose split merge move level update level level connections comprised single sweep sweeps used experiments conducted trees used section case results cases hpp configuration higher posterior probability generative tree cases tree found cases hpp solution inferior generative tree overall results less good ones section remembered search space much larger would expected one would need search longer comparing results fixed variable shows cases variable method gave higher posterior probability cases lower cases trees found rightmost column figure shows hpp configurations sampling variable two examples discussed panel solution found dissimilar panel although overall probability lower solution found uses one level centre rather two obtains higher posterior probability configurations discussion results indicate proposed model behaves sensibly reasonable solutions found relatively short amounts search method demonstrated univariate data extending multivariate gaussian data dimension independent given tree structure easy likelihood calculation independent dimension many directions model developed firsfly model presented uniform mixing proportions children equally likely connect potential parent generalized non uniform vector connection probabilities layer also given tree structure independent dirichlet priors probability vectors parameters integrated analytically secondly model made generate iid data regarding penultimate layer mixture centres case term lin would ignored computing probability tree thirdly would possible add variance variables mcmc scheme using metropolis algorithm defining suitable prior sequence variances cry constraint variances level equal could also relaxed allowing depend hyperparameters set every level fourthly may improved mcmc schemes devised example current implementation posterior means candidate units taken account proposing merge moves cf fifthly multivariate gaussian version consider tree structured factor analysis model higher levels tree need dimensionality data vectors one also consider version dimension multinomial rather continuous variable case one might consider model multinomial parameter vector ot tree generated parent ot random vector probabilities alternative model could build tree lliams structured prior parameters dirichlet prior multinomial distribution acknowledgments work partially supported epsrc grant gr probabilistic models sequences thank gatsby computational neuroscience unit ucl organizing mixtures day march supporting attendance peter green phil dawid peter dayan helpful discussions meeting also thank amos storkey helpful discussions magnus rattray accidentally pointing towards chapters phylogenetic trees
abstract data visualization feature selection methods proposed based joint mutual information ica visualization methods find many good projections high dimensional data interpretation cannot easily found existing methods new variable selection method found better eliminating redundancy inputs methods based simple mutual information efficacy methods illustrated radar signal analysis problem find viewing coordinates data visualization select inputs neural network classifier keywords feature selection joint mutual information ica visualiz ation classification introduction visualization input data feature selection intimately related good feature selection algorithm identify meaningful coordinate projections low dimensional data visualization conversely good visualization technique suggest meaningful features include model input variable selection important step model selection process given target variable set input variables selected explanatory variables prior knowledge however many irrelevant input variables cannot ruled prior knowledge many input variables irrelevant target variable severely complicate model selection estimation process also damage performance final model selecting input variables model specification model dependent approach however methods slow model space large reduce computational burden estimation selection processes need modelindependent approaches select input variables model specification one approach test approaches based mutual information mi effective evaluating relevance input variable fails eliminate redundant variables paper focus model independent approach input variable selec yang moody tion based joint mutual information jmi increment mi jmi conditional mi although conditional mi used show monotonic property mi used input selection data visualization important human understand structural relations among variables system also critical step eliminate unrealistic models give two methods data visualization one based jmi another based independent gomponent analysis iga methods perform better existing methods methods based pga canonical correlation analysis gga nongaussian data joint mutual information input feature selection let target variable xi inputs relevance single input measured mi pllq kullback leibler divergence two probability functions defined llq log relevance set inputs defined joint mutual information xg xk zg zk ilp zi zn given two selected inputs zj conditional mi defined xi slxj xj xi ylx llp xilx ylx similarly define xj conditioned two variables conditional mi always non negative since weighted average kullback leibler divergence following property xn xn xn xn vlx xn therefore xx xx adding variable always increase mutual information information gained adding variable measured conditional mi conditionally independent given xx conditional mi provides extra information known particular function equality holds reason joint mi used eliminate redundant inputs conditional mi useful input variables cannot distinguished mutual information example assume xt xa problem select zx zx za za since slx yix choose zi rather zi za xi xa yixi otherwise choose zl za possible comparisons represented binary tree figure estimate need estimate joint probability zi suffers curse dimensionality large data visualization feature selection sometimes may able estimate high dimensional mi due sample shortage work needed estimate high dimensional joint mi based parametric non parametric density estimations sample size large enough real world problems mining large data bases radar pulse classification sample size large since parametric densities underlying distributions unknown better use non parametric methods histograms estimate joint probability joint mi avoid risk specifying wrong complicated model true density function yixi yixi yixi xl yix ioc yix yix yix xl xl yix yix ioc xl figure input selection based conditional mi paper use joint mutual information xi xj instead mutual information xi select inputs neural network classifier another application select two inputs relevant target variable data visualization data visualization methods present supervised data visualization methods based joint mi discuss unsupervised methods based ica natural way visualize high dimensional input patterns display using two existing coordinates coordinate corresponds one input variable inputs relevant target variable corresponds best coordinates data visualization let arg max xi xj coordinate axes xi xj used visualizing input patterns since corresponding inputs achieve maximum joint mi find maximum xi xj need evaluate every joint mi xi xj number evaluations noticing xi xj xi xj yixi first maximize mi xi maximize conditional mi algorithm suboptimal requires evaluations joint mis sometimes equivalent exhaustive search one example given next section existing methods visualize high dimensional patterns based dimensionality reduction methods pca cca find new coordinates display data new coordinates found pca cca orthogonal euclidean space space mahalanobis inner product respectively however two methods suitable visualizing nongaussian data projections pca cca coordinates statistically independent nongaussian vectors since jmi method model independent better analyzing nongaussian data yang moody cca maximum joint mi supervised methods pca method unsupervised alternative methods ica visualizing clusters ica technique transform set variables new set variables statistical dependency among transformed variables minimized version ica use based algorithms discovers non orthogonal basis minimizes mutual information projections basis vectors shall compare methods real world application application signal visualization classification joint mutual information visualization radar pulse patterns goal design classifier radar pulse recognition radar pulse pattern dimensional vector first compute joint mis use select inputs visualization classification radar pulse patterns set radar pulse patterns denoted yi consists patterns three different classes yi bundle numbel figure mi vs conditional mi radar pulse data maximizing mi conditional mi evaluations gives xi xj bits joint mi radar pulse data maximizing joint mi gives xj bits evaluations joint mi case let arg maxii xi arg maxjv xj ylxil figure obtain xi xj xj yixi bits number total inputs number evaluations computing mutual information xi conditional mutual information yixi find maximum xi evaluate every xi mis shown bars figure th bundle displays mis xi xj order compute joint mis mi conditional mi evaluated times respectively maximumjoint mi xj bits generally know gi xj particular data iqsualization feature selection application equality holds suggests sometimes use efficient algorithm linear complexity find optimal coordinate axis view xi xj joint mi also gives good sets coordinate axis views high joint mi values first pdnopal component tm fimt ld figure data visualization two principal components spatial relation patterns clear use optimal coordinate axis view zi xj found via joint mi project radar pulse data patterns well spread give better view spatial relation patterns boundary classes cca method ica method bar figure associated pair inputs pairs high joint mi give good coordinate axis view data visualization figure shows data visualizations maximum jmi ica better pca cca data nongaussian radar pulse classification train two layer feed forward network classify radar pulse patterns figure shows difficult separate patterns using two inputs shall use inputs four selected inputs data set divided yang moody training set dx test set consisting percent patterns network trained data set dx using input variables denoted xx wx wx weight matrices vector thresholds hidden layer data set estimate mutual information xi select ix arg max xi given xi estimate conditional mutual information xd xi ix ghoose three inputs xi xi xi largest conditional mi found quartet ix ia two layer feedforward network trained dx four selected inputs denoted xa choices select input variables set reference performance network four inputs comparison choose quartets set jx quartet jx two layer feed forward network trained using inputs xj xj xj xj networks denoted hi xj xj xj xj avenge lesta er wllh qumel leslblg er letected bfiuls xl altd lra tnger wilh salecled uisxl inuninger minpuls figure error rates network four inputs xx xg selected joint mi well average error rates error bars attached networks different input quartets randomly selected shows input quartet xx xa xa rare informative network inputs xx xa xa converges faster network inputs former uses fewer parameters weights thresholds fewer inputs latter classifier four best inputs less expensive construct use terms data acquisition costs training time computing costs real time application mean variance error rates networks computed networks seven hidden units training testing error rates networks epoch shown figure see network four inputs selected joint mi performs better networks randomly selected input quartets converges faster network inputs network fewer inputs faster computing also less expensive data collection data visualization feature selection conclusions proposed data visualization feature selection methods based joint mutual information ica maximum jmi method find many good projections visualizing high dimensional data cannot easily found existing methods maximum jmi method ica method effective visualizing nongaussian data variable selection method based jmi found better eliminating redundancy inputs methods based simple mutual information input selection methods based mutual information mi useful many applications two disadvantages first cannot distinguish inputs mi second cannot eliminate redundancy inputs one input function inputs contrast new input selection method based joint mi offers significant advantages two aspects successfully applied methods visualize radar patterns select inputs neural network classifier recognize radar pulses found smaller yet robust neural network radar signal analysis using jmi acknowledgement research supported grant onr
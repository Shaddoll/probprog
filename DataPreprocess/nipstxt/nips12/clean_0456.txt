abstract transduction inference principle takes training sample aims estimating values function given points contained called working sample opposed whole input space induction transduction provides confidence measure single predictions rather classifiers feature particularly important risk sensitive applications possibly infinite number functions reduced finite number equivalence classes working sample rigorous bayesian analysis reveals standard classification loss cannot benefit considering one test point time probability label given test point determined posterior measure corresponding subset hypothesis space consider pac setting binary classification linear discriminant functions perceptrons kernel space probability labels determined volume ratio version space suggest sample region ergodic billiard experimental results real world data indicate bayesian transduction compares fa vourably well known support vector machine particular posterior probability labellings used confidence measure exclude test points low confidence introduction according vapnik solving given problem one avoid solving eneral problem intermediate step reasoning behind principle order solve general task resources may wasted compromises may made would necessary solution problem hand direct application common sense principle reduces general problem inferring functional dependency whole input space problem estimating values function given points working sample paradigm referred transductive inference formally given probability measure px space data training sample xt yt generated according px additional data points xl xl drawn working sample goal label objects working sample using fixed set functions bayesian transduction minimise predefined loss contrast inductive inference aims choosing single function ft best suited capture dependency expressed unknown px obviously transductive algorithm assigns working sample set labels given training sample set functions define function fs fs result transduction algorithm two crucial differences induction however restricted select single decision function ii transduction algorithm give performance guarantees particular labellings instead functions practical applications difference may great importance risk sensitive applications medical diagnosis financial critical control applications often matters know confident given prediction case general confidence measure classifier whole input distribution would provide desired warranty note linear classifiers guarantee obtained margin section demonstrate coarse confidence measure idea transduction put forward also first algorithmic ideas found later suggested algorithm transduction based linear programming highlighted need confidence measures transduction paper structured follows bayesian approach transduction formulated section section function class kernel perceptrons introduced bayesian transduction scheme applied estimation volumes parameter space present kernel billiard efficient sampling technique finally demonstrate experimentally section confidence measure labellings helps bayesian transduction achieve low generalisation error low rejection rate test points thus outperform support vector machines svms bayesian transductive classification suppose given training sample xl vl xl yl drawn px working sample xt xt drawn px given prior ph set functions likelihood tlh obtain def posterior probability phi phis bayes rule posterior measure induces probability measure labellings working sample py dej phis vx hi sake simplicity let us assume pac style setting exists function space case one define called version space set functions consistent training sample xi yi es xi yi outside posterior psis vanishes py ls represents prior measure functions consistent training sample labelling working sample normalised prior measure functions consistent alone measure ph used incorporate prior knowledge note number different labellings implementable bounded value growth function ii iwl graepel herbrich obermayer inference process knowledge available considerations symmetry may lead uninformative priors given measure qs labellings order arrive risk minimal decision labelling need define loss function ym ym ir labellings minimise expectation ey ls py ls summation runs possible labellings working sample let us consider two scenarios loss exact labelling two labellings bi case choosing labelling argmin highest joint probability ls minimises risk non labelwise loss appropriate goal exactly identify combination labels combination handwritten digits defining postal zip code note classical svm transduction see maximising margin combined training working sample approximates strategy hence minimise standard classification risk single instances intended loss single labels bi two labellings ly bi due independent treatment loss working sample points risk minimised labelling highest marginal probability labels argmaxvey ls xt thus case labelwise loss working sample point offer advantages larger working samples bayes optimal decision since corresponds standard classification setting restrict working samples size one working point xt bayesian transduction volume kernel perceptron consider transductive inference class kernel perceptrons decision functions given sign sign ctik xi climb xi bayesian transduction figure schematic view data space left parameter space right classification toy example using duality given data points left correspond hyperplanes right hyperplanes left thought points right mapping maps input space feature space completely determined inner product function kernel see given training sample xi yi define version space set perceptrons compatible training data additional constraint ilwll ensuring uniqueness order obtain prediction label bl working point xl note xl may bisects volume version space two sub volumes perceptrons would classify xl ratio probability labelling given uniform prior class kernel perceptrons accordingly bl see figure already vapnik noticed troublesome estimate sub volumes version space solution problem suggest use billiard algorithm kernel billiard volume estimation method playing billiard version space first introduced rujan purpose estimating centre mass consequently refined extended kernel spaces bayesian transduction idea bounce billiard ball version space record much time spends sub volumes interest assumption ergodicity uniform measure limit accumulated flight times sub volume proportional sub volume since trajectory located position direction ball expressed linear combinations xi real vectors components fully determine state billiard algorithm determination label xt proceeds follows initialise starting position using kernel perceptron algorithm achieves zero training error svm set graepel herbrich obermayer find closest boundary starting current direction flight times vj points including xt determined using smallest positive flight time vc minj rs vj kernel space corresponds closest data point boundary xc hypersphere note vc randomly generate direction pointing towards version space suming bounce calculate hall new position according rcv iiw calculate distance sphere arccos hypersphere add volume estimate corresponding current label sign xt test point hit keep old direction vector otherwise update reflection direction xc xc go back step unless stopping criterion met note practice one trajectory calculated advance used test points estimators probability labellings given thus algorithm outputs confidence ctrans according bl der argmaxv der max ctrans note bayes point machine bpm aims optimal approximation transductive classification single function well known svm viewed approximation bpm centre largest ball version space thus treating real valued output xl ef cind svm classifiers confidence measure considered approximation consequences demonstrated experimentally following section disregarding issue mixing time dependence trajectories sume stopping criterion fraction time spent volume trajectory length random variable expectation hoeffding inequality bounds probability deviation expectation exp def thus want deviation true label probability less probability le need approximately bounces computational effort algorithm working set size order bayesian transduction rejection rate rejection rate figure generalisation error vs rejection rate bayesian transduction svms hyroid data set heart data set error bars directions indicate one standard deviation estimated means upper curve depicts result svm algorithm lower curve result obtained bayesian transduction experimental results focused confidence xtrans bayesian transduction provides together prediction label confidence rans reflects reliability label estimate given test point rejecting test points whose predictions carry low confidence lead reduction generalisation error remaining test points experiments varied rejection threshold thus obtaining rejeection rate together estimate generalisation error non rejected points curves linked common axis resulting generalisation error versus rejection rate plot used uci data sets hyro hear medical applications confidence single predictions particularly important also high rejection rate due conservative confidence measure may incur considerable costs trained support vector machine using rbf kernels exp tlx cr chosen insure existence version space used different training samples obtained random splits whole data set margin nd test point calculated confidence measure svm classifications comparison determined labels resulting confidences ctrans using bayesian transduction algorithm see section value kernel parameter si nce rejection bayesian transduction cases higher svms level determined max achieves rejection rate svm confidence measures bayesian transduction achieves hyro hear max results two data sets depicted figure hyro example figure one see rans indeed appropriate indicator confidence rejection rate approximately generalisation error approaches zero minimal variance desired generalisation error bayesian transduction needs reject significantly less examples test set compared svm classifiers less generalisation error results hear data set show even pronounced characteristics rejection uci university california irvine machine learning repository graepel herbrich obermayer rate note confidence measures considered cannot capture effects noise data leads generalisation error even maximal rejection corresponding bayes error given function class conclusions future work paper presented bayesian analysis transduction required volume estimates kernel perceptrons version space performed ergodic billiard kernel space importantly transduction determines label given point also returns confidence measure classification form probability label model using confidence measure reject test examples lead improved generalisation error svms billiard algorithm extended case non zero training error allowing ball penetrate walls property cap tured adding constant diagonal kernel matrix research aim discovery pac bayesian bounds generalisation error transduction acknowledgements greatly indebted kockelkorn many interesting suggestions discussions project partially funded technical university berlin via rip
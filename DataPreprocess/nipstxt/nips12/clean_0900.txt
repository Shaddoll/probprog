abstract discuss information theoretic approach categorizing modeling dynamic processes approach learn compact informative statistic summarizes past states predict future observations furthermore uncertainty prediction characterized nonparametrically joint density learned statistic present observation discuss application technique noise driven dynamical systems random processes sampled density conditioned past first case show results dynamics random walk statistics driving noise captured second case present results summarizing statistic learned noisy random telegraph waves differing dependencies past states cases algorithm yields principled approach discriminating processes differing dynamics dependencies method grounded ideas information theory nonparametric statistics introduction noisy dynamical processes abound world human speech frequency sun spots stock market common examples processes difficult model categorize current observations dependent past complex ways classical models come two sorts assume dynamics linear noise gaussian weiner etc assume dynamics discrete hmm approach wildly popular tractable well understood unfortunately many processes underlying theoretical assumptions models false example may wish analyze system linear dynamics non gaussian noise may wish model system unknown number discrete states present information theoretic approach analyzing stochastic dynamic processes model simple processes like mentioned retaining flexibility model wider range complex processes key insight often learn simplifying informative statistic past samples using nonparametric estimates entropy mutual information within tyamework predict future states equal importance characterize uncertainty accompanying learning informative statistics nonparametric approach predictions non parametric model flexible enough describe uncertainty complex second order statistics contrast techniques use squared prediction error drive learning focused mode distribution taking example financial forecasting likely sequence pricing events interest one would also like know accompanying distribution price values even likely outcome appreciation price asset knowledge lower insignificant probability depreciation also valuable towards end describe approach allows us simultaneously learn dependencies process past well uncertainty future states approach novel fold concepts information theory nonparametric statistics learning two types stochastic processes consider challenge summarize past efficient way absence known dynamical probabilistic model learn informative statistic ideally sufficient statistic past minimizes uncertainty future states classical linear state space approach uncertainty characterized mean squared error mse implicitly assume gaussian statistics however linear systems interesting behavior due non gaussian statistics violate assumption underlying mse also nonlinear systems purely probabilistic processes exhibit complex behavior poorly characterized mean square error assumption gaussian noise approach applicable types processes based nonparametric statistics characterize uncertainty predictions general way density possible future states consequently resulting system captures dynamics systems parameterization statistics driving noise nonparametric modeling model used classify new signals make predictions future learning stationary processes paper consider two related types stochastic processes depicted figure processes differ current observations related past first type process described following set equations discrete time dynamical possibly nonlinear system xk xk wg xk xk zk state process time function previous states present value general sequence xk stationary strict sense however fairly mild conditions namely sequence random variables always assume true sequence ek xk xk stationary often termed innovation sequence purpose stationarity suffice leads prediction framework estimating dynamical parameters system adjoin nonparametric characterization uncertainty second type process consider described conditional probability density xk xkll xk case conditional statistics xk concerned definition constant learning informative statistics nonparametric estimators propose determine system parameters minimizing entropy error residuals systems type parametric entropy optimization approaches fisher iii ihler ola xk qp xa xk max figure two related systems dynamical system driven stationary noise probabilistic system dependent finite past dotted box indicates source stochastic process solid box indicates learning algorithm proposed novelty approach however estimate entropy nonparametrically bg argmin fp logp de jj al ek differential entropy integral approximated using function parzen kernel density estimator experiments use gaussian kernel shown minimizing entropy error residuals equivalent maximizing likelihood light proposed criterion seeking maximum likelihood estimate system parameters using nonparametric description noise density consequently solve system parameters noise density jointly explicit dynamical system second system type assume conditional statistics observed sequence constant worst slowly changing line learning algorithm case desire minimize uncertainty predictions future samples summarizing information past challenge efficiently via function recent samples ideally would like find sufficient statistic past however without explicit description density opt instead informative statistic informative statistic simply mean one reduces conditional entropy future samples statistic sufficient mutual information reached maximum previous case propose find statistic maximizing nonparametric mutual information defined wf wf argmin xk wf xk argmin xk xklf equation equivalent optimizing joint marginal entropies practice equation minimizing conditional entropy previously presented two related methods incorporating kernel based density estimators information theoretic learning framework chose method provides exact gradient approximation entropy importantly converted implicit error function thereby reducing computation cost learning informative statistics nonparametric approach distinguishing random walks example random walk feedback function xk xk noise assumed independent identically distributed although sequence xk non stationary increments xk xk stationary context estimating statistics residuals allows discrimination two random walk process differing noise densities furthermore demonstrate empirically even one processes driven gaussian noise implicit assumption mmse criterion knowledge may sufficient distinguish one process another figure shows two random walk realizations associate noise densities solid lines one driven gaussian noise lk driven bi modal mixture gaussians note densities zero mean unit variance learning process modeled fifth order auto regressive ars one hundred samples drawn realization type ar parameters estimated using standard mmse approach approach described regards parameter estimation methods expected yield essentially parameters first coefficient near unity remaining coefficients near zero interested ability distinguish one process another mentioned current approach jointly estimates parameters system well density noise nonparametric estimates shown figure dotted lines estimates used compute accumulated average log likelihood ek ogp xi residual sequence rl known learned densities figure striking surprising hi modal mixture gaussian model dashed lines top differ significantly gaussian driven increments process solid lines top explanation follows fact lim llp true density ofe bi modal assumed density likelihood test unit variance gaussian kullback leibler divergence case lp relatively small true less entropy unit variance gaussian fixed variance gaussian density maximum entropy consequence likelihood test gaussian assumption reliably distinguish two processes likelihood test bi modal density nonparametric estimate figure bottom distinguish two method described limited linear dynamic models certainly used nonlinear models long dynamic well approximated differentiable functions examples multi layer perceptrons described learning structure noisy random telegraph wave noisy random telegraph wave rtw described figure goal demonstrate analyze random telegraph waves rather robustly learn informative statistic past process define noisy random telegraph wave sequence ltk binomially distributed zl lak gaussian process interesting parameters random functions nonlinear combination set xk depending value observe different switching dynamics figure shows examples signals fisher iii ihler iqola laudelan vl learned denait bi modal adom lk hi model leemeal den figure random walk examples left comparison known learned densities right known ou model known bi mod model lelrroed model bi model stit islios figure ek known models left compared learned models right left right rapid switching dynamics possible signals periods longer duration figure noisy random telegraph wave left right experiments learn sufficient statistic form xk past wf hyperbolic tangent function one layer perceptron note multi layer perceptron could also used experiments train samples noisy rtw rtw learn statistics type process using tests situations depth specified specified well perfectly learning informative statistics nonparametric approach rtw ld ener hesi oo figure comparison wiener filter top nonparametric approach bottom synthesis figure informative statistics noisy random telegraph waves trained equal left right specified denote fn xk statistic trained rtwuv process memory depth since implicitly learn joint density xk fn xk synthesis possible sampling density figure compares synthesis using described method bottom wiener filter top estimated data results using information theoretic approach bottom preserve structure rtw wiener filter results achieved collapsing information past samples single statistic avoiding high dimension density estimation figure shows joint density fn xk see estimated densities separable virtue fact learned statistic conveys information future figure shows results monte carlo trials case depth statistic matched process plot shows accumulated conditional log likelihood ei glb xilf underthelearnedstatisticwitherror bars figure shows similar results varying memory depth statistic figures illustrate robustness choice memory depth say memory depth matter must information exploit empirical results indicate useful information extracted conclusions described nonparametric approach finding informative statistics approach novel learning derived nonparametric estimators entropy mutual information allows means efficiently summarize past predict future characterize uncertainty predictions beyond second order statistics futhermore accomplished without strong assumptions accompanying parametric approaches fisher iii ihler ola tm vs rocelm lell room figure conditional ek solid line indicates rtw dashed line indicates rtw thick lines indicate average monte carlo runs thin lines indicate standard deviation left plot uses statistic trained rtw right plot uses statistic trained rtw tmst ro ee figure repeat figure cases obvious breaks indicate new set trials also presented empirical results illustrated utility approach example random walk served simple illustration learning dynamic system spite specification ar model importantly demonstrated ability learn dynamic statistics underlying noise process information later used distinguish realizations nonparametric densities something possible using mmse error prediction even compelling result experiments noisy random telegraph waves demonstrated algorithms ability learn compact statistic efficiently summarized past process identification method exhibited robustness number parameters learned statistic example despite overspecifying dependence memory three cases useful statistic still found conversely despite memory statistic underspecified three experiments useful information available past extracted opinion method provides alternative traditional connectionist approaches time series analysis use nonparametric estimators adds flexibility class densities modeled places less constraint exact form summarizing statistic
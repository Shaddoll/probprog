abstract function approxim essenti reinforc learn standard approach approxim valu function determin polici far proven theoret intract paper explor altern approach polici explicitli repres function approxim independ valu function updat accord gradient expect reward respect polici paramet william reinforc method actor critic method exampl approach main new result show gradient written form suitabl estim experi aid approxim action valu advantag function use result prove first time version polici iter arbitrari differenti function approxim converg local optim polici larg applic reinforc learn rl requir use gener function approxim neural network decis tree instanc base method domin approach last decad valu function approach function approxim effort goe estim valu function action select polici repres implicitli greedi polici respect estim valu polici select state action highest estim valu valu function approach work well mani applic sever limit first orient toward find determinist polici wherea optim polici often stochast select differ action specif probabl see singh jaakkola jordan second arbitrarili small chang estim valu action caus select discontinu chang identifi key obstacl establish converg assur algorithm follow valu function approach bertseka tsitsikli exampl learn sarsa dynam program method shown unabl converg polici simpl mdp simpl function approxim gordon baird tsitsikli van roy bertseka tsitsikli occur even best approxim found step chang polici whether notion best mean squar error sens slightli differ sens residu gradient tempor differ dynam program method paper explor altern approach function approxim rl sutton mcallest singh mansour rather approxim valu function use comput determinist polici approxim stochast polici directli use independ function approxim paramet exampl polici might repres neural network whose input represent state whose output action select probabl whose weight polici paramet let denot vector polici paramet perform correspond polici averag reward per step polici gradient approach polici paramet updat approxim proport gradient op posit definit step size achiev usual assur converg local optim polici perform measur unlik valu function approach small chang caus small chang polici state visit distribut paper prove unbias estim gradient obtain experi use approxim valu function satisfi certain properti william reinforc algorithm also find unbias estim gradient without assist learn valu function reinforc learn much slowli rl method use valu function receiv rel littl attent learn valu function use reduc varianc gradient estim appear essenti rapid learn jaakkola singh jordan prove result similar special case function approxim correspond tabular pomdp result strengthen gener arbitrari differenti function approxim konda tsitsikli prep independ develop simialr result see also baxter bartlett prep marbach tsitsikli result also suggest way prove converg wide varieti algorithm base actor critic polici iter architectur barto sutton anderson sutton kimura kobayashi paper take first step direct prove first time version polici iter gener differenti function approxim converg local optim polici baird moor obtain weaker superfici similar result vap famili method like polici gradient method vap includ separ parameter polici valu function updat gradient method howev vap method climb gradient perform expect long term reward measur combin perform valuefunct accuraci result vap converg local optim polici except case weight put upon valu function accuraci case vap degener reinforc similarli gordon fit valu iter also converg valu base find local optim polici polici gradient theorem consid standard reinforc learn framework see sutton barto learn agent interact markov decis process mdp state action reward time denot st rt respect environ dynam character state transit probabl pr st st expect reward rt st vs agent decis make procedur time character polici pr alst vs ji paramet vector assum diffenti respect paramet exist also usual write polici gradient method rl function approxim function approxim two way formul agent object use one averag reward formul polici rank accord long term expect reward per step lim rl rn limt pr st stationari distribut state assum exist independ polici averag reward formul valu state action pair given polici defin ao vse aea second formul cover design start state care long term reward obtain give result appli formul well definit lrt lrt st xk discount rate allow episod task formul defin discount weight state encount start follow tpr st first result concern gradient perform metric respect polici paramet theorem polici gradient mdp either averag reward start state formul op oo oo proof see appendix way express gradient first discuss averag reward formul marbach tsitsikli base relat express term state valu function due jaakkola singh jordan cao chen extend result start state formul provid simpler direct proof william theori reinforc algorithm also view impli event key aspect express gradient term form oa effect polici chang distribut state appear conveni approxim gradient sampl exampl sampl distribut would unbias estim obtain follow oe cours also normal known must estim one ap proach use actual return rt oo oo ek ek vk iwt start state formul approxim st lead william episod reinforc algorithm oc correct oversampl action prefer known follow expect valu william polici gradient approxim consid case approxim learn function approxim approxim suffici good might hope use place sutton mc lester singh mansour still point roughli direct gradient exampl jaakkola singh jordan prove special case function approxim aris tabular pomdp one could assur posit inner product gradient suffici ensur improv move direct extend result gener function approxim prove equal gradient let fw approxim qx paramet natur rt learn fw follow rr updat rule awt fw ao st unbias st st st tj estim st perhap pu process converg local optimum fu ofu ow theorem polici gradient function approxim fw satisfi compat polici parameter sens ofu ow op proof combin give oo tell us error fu orthogon gradient polici parameter express zero subtract polici gradient theorem yield op oo applic deriv algorithm advantag given polici parameter theorem use deriv appropri form valu function parameter exampl consid polici gibb distribut linear combin featur eb eotq vs tsitsikli person commun point linear featur given righthand side may way satisfi condit polici gradient method rl function approxim qbsa dimension featur vector character state action pair meet compat condit requir ofw qbsb natur parameter fw word fw must linear featur polici except normal mean zero state algorithm easili deriv varieti nonlinear polici parameter multi layer backpropag network care reader notic form given requir zero mean state fw sens better think fw approxim advantag function much baird rather qx converg requir realli fw get rel valu action correct state absolut valu variat state state result view justif special statu advantag target valu function approxim rl fact gener includ arbitrari function state ad valu function approxim exampl gener sd fw arbitrari function follow immedi choic affect theorem substanti affect varianc gradient estim issu entir analog use reinforc baselin earlier work william dayan sutton practic presum set best avail approxim result establish approxim process proceed without affect expect evolut converg polici iter function approxim given theorem prove first time form polici iter function approxim converg local optim polici theorem polici iter function approxim let rr fw differenti function approxim polici valu function respect satisfi compat condit max oo aoj oo let step size sequenc limk ak ak oo mdp bound reward sequenc rrk defin rk ok wk rk ok otk dw rk oo converg limk proof theorem assur updat direct gradient mdp reward togeth assur us bound oo ooj ao ao sutton mcallest singh mansour also bound togeth step size requir necessari condit appli proposit page bertseka tsitsildi assur converg local optimum acknowledg author wish thank martha steenstrup doina precup comment michael kearn insight notion optim polici function approxim 
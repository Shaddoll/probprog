abstract long term potentiation ltp long held biological substrate associative learning recently evidence emerged long term depression ltd results presynaptic cell fires postsynaptic cell computational utility ltd explored synaptic modification kernels ltp ltd proposed laboratories based studies one postsynaptic unit interaction time dependent ltp ltd studied small networks introduction long term potentiation ltp neurophysiological phenomenon observed laboratory conditions two neurons neural populations stimulated high frequency resulting measurable increase synaptic efficacy lasts several hours days ltp thus provides direct evidence supporting neurophysiological hypothesis articulated hebb increase synaptic strength must countered mechanism weakening synapse biological correlate long term depression ltd also observed laboratory synapses observed weaken low presynaptic activity coincides high postsynaptic activity mathematical formulations hebbian learning produce weights presynaptic unit postsynaptic unit capture covariance eq instantaneous activities pairs units aj ij idea generalized capture covariance activities shifted time resulting framework model systems temporal delays dependencies eq ij ls dt dt ltd facilitates learning noisy environment shown following sections depending choice function formulation encompasses broad range learning rules support comparably broad range biological evidence aw tpre tpost figure synaptic change function time difference spikes presynaptic neuron postsynaptic neuron note tpre tpo ltp results aw tpre tpo result ltd recent biological data indicates increase synaptic strength ltp presynaptic activity precedes postsynaptic activity ltd reverse case postsynaptic precedes presynaptic ideas started appear theoretical models neural computation thus figure shows form dependence synaptic change aw difference spike arrival times general framework given specific assumptions integral eq separated two integrals one representing ltp one representing ltd eq ij kp ai aj dt kd ai aj dt oo tp td activities depend factored integrals giving two hebb like products instantaneous activity one cell weighted time average activity eq ij ai ai kx dt kernel functions kv ko chosen select precise times convoluted function average across functions arbitrary range alpha function useful eq high value tx selects immediate time small value approximates longer time average flxve axr ao fi fio munro hernandez high values tro pre post synaptic activities close temporally interact modify synapse simulation discrete step sizes reasonably approximated considering single time step awij sumnfing zlwo zlwo gives net change weights zl wo wo wo two time steps wij ai ai first term predictive form delta rule acts training signal aj temporal hopfield network temporal contrast enhancement computational role ltp term eq well established second term contribute possibility term analogous lateral inhibition temporal domain suppressing associations wrong temporal direction system may robust noise input resulting system may able detect onset offset signal reliably system using anti hebbian ltd term extent ltd term able enhance temporal contrast likely depend idiosyncratically statistical qualities particular system parameters system might valid signals specific statistical properties parameters might adaptive either possibilities lies beyond scope analysis paper simulations two preliminary simulation studies illustrate use learning rule predictive behavior temporal contrast enhancement every simulation kernel functions specified parameters number time steps nv nz sampled approximation integral task sequential shifter first task simple shifter set units system trained stimuli tested see reconstruct sequence given initial input task given noise temporal noise see figure task designed examine utility ltd approach learning sequence temporal noise ability network reconstruct noise free sequence training noisy sequence tested different ltd kernel functions note patterns presented time slice one units active shifts either skip repeat time experiments run units active task time series reconstruction task set units trained external sinusoidal signals varied according frequency phase purpose task examine role ltd providing temporal context network tested condition ltd facilitates learning noisy environment external signals provided one units activity deprived unit compared training signal sequence recons ction clean noisy ltp alone ltp ltd figure reconstruction clean shifter sequence using input noisy stimulus shifter sequence time slice one units active clean sequence activity shifts cyclically around units noisy sequence random jitter resultssequential shifter results networks trained clean sequence learn task ltp alone networks could learn shifter task based noisy training sequence unless also ltd term without ltd term units would saturate maximum values range ltd parameters network would converge without saturating reconstruction performance found sensitive ltd parameters parameters ac shown table needed chosen specifically get perfect reconstruction done trial error narrow range parameters near optimal values reconstructed sequence close noise free target however parameters ac shown table estimated experimental result zhang et al table results sec uential shifter task rl rl ao time task shift pattern unit time step block units active parameters kernel functions number values sampled kernel number time slices used estimate integral nz number steps used begin reconstruction nr usually nr given table last column table time reports number iterations required perfect reconstruction munro hernandez table results sequential shifter task using parameters nr nv ao fl time results active units always adjacent respect shifting direction cases noncontiguous active units reconstruction never exact networks trained ltp alone would saturate would converge sequence close target fig ltd term added sequence clean noisy reconstruction ltp alone ltp ltd didid fll lflfl ddi illd dil fl figure base pattern noncontiguous active units presented shifted sequence noise target sequence partially reconstructed ltp ltd used together time series reconstruction results network four units trained hundreds iterations units externally driven sinusoidally varying input networks trained ltp alone fail reconstruct time series units deprived external input testing simulations noise patterns ltd shown necessary reconstruction patterns fig figure reconstruction sinusoids target signals training dashed plotted reconstructed signals solid left best reconstruction using ltp alone right typical result ltp ltd together high values tzv tzz reconstruction sinusoids sensitive values flz fie figure shows results iflo fie values close first case top iflol slightly smaller fie first two neurons left right saturate contrary case bottom first two neurons ltd facilitates learning noisy environment show almost null activation however third fourth neurons left right cases top bottom show predictive behavior la figure reconstruction sinusoids examples target signals training dashed plotted reconstructed signals solid top iflt flv bottom discussion half century elapsed since hebb articulated neurophysiological postulate neuroscience community come recognize fundamental role plasticity hebb hypothesis clearly transcends original motivation give neurophysiologically based account associative memory phenomenon ltp provides direct biological support hebb postulate hence clear cognitive implications initially discovery laboratory computational role ltd thought flip side ltp interpretation would synapses strengthen activities correlated weaken anti correlated theory appealing elegance formed basis many network models however dependence synaptic change relative timing pre post synaptic activity recently shown laboratory inconsistent story calls computational interpretation network trained learning rule cannot converge state weights symmetric example since aw awji simulations reported simple preliminary illustrate two tasks benefit inclusion time dependent ltd case sequential shifter examination complex predictive tasks planned near future expected require architectures unclamped hidden units role ltd temporally enhance contrast way analogous role lateral inhibition computing spatial contrast enhancement retina time series example illustrates possible role ltd providing temporal context munro hernandez
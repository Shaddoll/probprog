abstract consider problem reconstructing temporal discrete sequence multidimensional real vectors part data missing assumption sequence generated continuous process particular case problem multivariate regression difficult underlying mapping one many propose algorithm based joint probability model variables interest implemented using nonlinear latent variable model point sequence potentially reconstructed modes conditional distribution missing variables given present variables computed using exhaustive mode search gaussian mixture mode selection determined dynamic programming search minimises geometric measure reconstructed sequence derived continuity constraints illustrate algorithm toy example apply real world inverse problem acoustic toarticulatory mapping results show algorithm outperforms conditional mean imputation multilayer perceptrons definition problem consider mobile point following continuous trajectory subset imagine possible obtain finite number measurements position point suppose measurements corrupted noise sometimes part variables missing problem considered reconstruct sequence part observed particular case present variables missing ones every point problem one multivariate regression pattern missing variables general problem one missing data reconstruction consider problem regression present variables uniquely identify missing ones every point data set problem adequately solved universal function approximator multilayer perceptron probabilistic framework conditional mean missing variables given present ones minimise average squared reconstruction error however underlying mapping one many regions space present variables identify uniquely missing ones case conditional mean mapping fail since give compromise value average correct ones inverse problems inverse probabilistic sequential data reconstruca mapping one many type include acoustic articulatory mapping speech different vocal tract shapes may produce acoustic signal robot arm problem different configurations joint angles may place hand position situations data reconstruction means objective classification inference deal solely data reconstruction temporally continuous sequences according squared error algorithm apply data sets either lack continuity discrete variables lost due undersampling shuffling follow statistical learning approach attempt reconstruct sequence learning mapping training set drawn probability distribution data rather solving physical model system algorithm described briefly follows first joint density model data learned unsupervised way sample data pointwise reconstruction achieved computing modes conditional distribution missing variables given present ones current point principle modes potentially plausible reconstruction reconstructing sequence repeat mode search every point sequence find combination modes minimises geometric sequence measure using dynamic programming sequence measure derived local continuity constraints curve length algorithm detailed illustrate toy problem apply acoustic articulatory like problem discusses results compares approach previous work notation follows represent observed variables vector form td data set possibly temporal sequence represented groups variables represented sets indices tz joint generafive modelling using latent variables starting point joint probability model observed variables compute conditional distributions form tz picking representative points derive multivalued mapping tz thus contrarily approaches adopt multiple pointwise imputation show obtain single reconstructed sequence points although density estimation requires parameters mapping approximation fundamental advantage density model represents relation variables allows choose missing present variable combination mapping approximator treats asymmetrically variables inputs present rest outputs missing easily deal relations existence functional relationships even one many observed variables indicates data must span low dimensional manifold data space suggests use latent variable models modelling joint density however possible use kinds density models latent variable modelling assumption observed high dimensional data generated underlying low dimensional process defined small number latent variables ar latent variables mapped fixed examples use complete training data missing data perfectly possible estimate probability model incomplete training data using em algorithm carreira perpifidn transformation dimensional data space noise added particular model specified three parametric elements prior distribution latent space smooth mapping latent space data space noise model data space marginalising joint probability density function latent space gives padistribution data space given observed sample data space rameter estimate found maximising log likelihood typically using em algorithm consider following latent variable models allow easy computation conditional distributions form tj tz factor analysis mapping linear prior latent space unit gaussian noise model diagonal gaussian density data space gaussian constrained covariance matrix use baseline comparison sophisticated models generafive topographic mapping gtm nonlinear latent variable model mapping generalised linear model prior latent space discrete uniform noise model isotropic gaussian density data space constrained mixture isotropic gaussians latent variable models sample latent space prior distribution like gtm mixture centroids data space associated latent space samples trainable parameters improve density model higher computational cost generalisation loss increasing number mixture components note number components required depend exponentially intrinsic dimensionality data ideally coincident latent space observed one exhaustive mode finding given conditional distribution tj tz consider modes plausible predictions tj requires exhaustive mode search space tj gaussian mixtures using maximisation algorithm starting centroid fixed point iteration gradient ascent combined quadratic optimisation particular case variables missing rather performing mode search return predictions component centroids also possible obtain error bars mode locally approximating density function normal distribution however dimensionality tj high error bars become wide due curse dimensionality advantage multiple pointwise imputation easy incorporation extra constraints missing variables constraints might include keeping modes lie interval dependent present variables discarding low probability spurious modes speeds reconstruction algorithm may make robust faster way generate representative points cr tz simply draw fixed number samples may also give robustness poor density models however practice resulted higher reconstruction error continuity constraints dynamic programming dp search application exhaustive mode search conditional distribution every point sequence produces one candidate reconstructions per point select actually given value tz centroids negligible posterior probability removed mixture practically loss accuracy thus large number mixture components may used without deteriorating excessively computational efficiency probabilistic sequential data reconstruction trajectory factor mean dpmod average squared reconstruction error missing factor mlp gtm pattern analysis mean dpmode cmode athe mlp cannot applied varying patterns missing data table trajectory reconstruction problem table gives average squared reconstruction error missing row tl missing row exactly one variable per point missing random row percentage values missing random rows graph shows reconstructed trajectory missing factor analysis straight dotted line mean thick dashed alpmode superimposed trajectory single reconstructed sequence define local continuity constraint consecutive points time also lie nearby data space suitable distance tn tn small define global geometric measure sequence def tn take distance euclidean becomes simply length sequence considered polygonal line finding sequence modes minimal efficiently achieved dynamic programming results toy problem illustrate algorithm generated data set curve tl sin normal isotropic noise standard deviation added thus mapping tl one one inverse one tl multivalued one dimensional factor analysis parameters gtm models parameters estimated point sample well two hidden unit multilayer perceptrons parameters one mapping gtm tried several strategies select points conditional distribution mean conditional mean alpmode mode selected dynamic programming cmode closest mode actual value missing variable cmode unknown practice used compute lower bound performance mode based strategy strategies picking global mode random mode using local greedy search instead dynamic programming gave worse results dpmode table shows results reconstructing point trajectory nonlinear nature problem causes factor analysis break cases one one mapping case missing methods perform well recover original trajectory mean attaining lowest error predicted theory one many case tl missing see fig mlp mean unable track one branch mapping alpmode still recovers original mapping random missing combined strategy could retain optimality mean one one case advantage modes one many case choosing conditional mean rather mode conditional distribution unimodal modes otherwise carreira perpihdn missing factor gtm pattern analysis mean dpmode cmode plp epg blocks table average squared reconstruction error utterance last row corresponds missing pattern square blocks totalling utterance patterns dpmode able cope well high amounts missing data consistently low error cmode shows modes contain important information possible options predict missing values performance dpmode close cmode even large amounts missing data shows application continuity constraint allows recover information results real speech data report preliminary experiment using acoustic electropalatographic epg data utterance put hat hatrack coat cupboard speaker fg accor database th order perceptual linear prediction coefficients plus log energy computed hz acoustic waveform epg data consisted bit frames sampled hz consider dimensional vectors real numbers preprocessing data carried thus resulting sequence consisted dimensional real vectors constructed training set picking random order vectors whole utterance used reconstruction test trained two density models dimensional factor analysis parameters two dimensional gtm parameters grid resulting mixture isotropic gaussians dimensional data space table confirms linear method factor analysis fares worst despite use latent space dimension dpmode attains almost always lower error conditional mean improvement larger higher amount missing data shuffled version utterance thus lost continuity reconstructed error dpmode consistently higher mean indicating application continuity constraint responsible error decrease discussion using joint probability model allows flexible construction predictive distributions missing data varying patterns missing data multiple pointwise imputations possible opposed standard function approximators shown modes conditional distribution missing variables given present ones potentially note nature missing pattern missing random missing completely random etc matter reconstruction although estimation epg datum binary contact pattern tongue palate selected locations latter note incomplete articulatory representation speech latent space dimensions clearly low data computational complexity gtm prevents use higher one still nonlinear character compensates partly probabilistic sequential data reconstruction plausible reconstructions missing values application local continuity constraints hold help recover actually plausible ones previous work key aspects approach use joint density model learnt unsupervised way exhaustive mode search definition geometric trajectory measure derived continuity constraints implementation dynamic programming several ideas applied earlier literature review briefly use joint density model prediction basis statistical technique multiple imputation several versions complete data set generated appropriate conditional distributions analysed standard complete data methods results combined produce inferences incorporate missing data uncertainty ghahramani jordan also proposed use joint density model generate single estimate missing variables applied classification problem conditional distributions approximated mlps rather density estimation lacks flexibility varying patterns missing data requires extra model input variables distribution unless assumed uniform rohwer van der rest introduce cost function description length interpretation whose minimum approximated densest mode distribution neural network trained cost function learn one branch multivariate mapping unable select branches may correct given time continuity constraints implemented via dynamic programming used acoustic articulatory mapping problem reasonable results better using mlp approximate mapping obtained using large codebook acoustic articulatory vectors rahim et al achieve similar quality much less computational requirements using assembly mlps one trained different area acoustic articulatory space locally approximate mapping however clustering space heuristic guarantee mapping one one region training assembly difficult also lacks flexibility varying missingness patterns number trajectory measures used robot arm problem literature minimised dynamic programming energy torque acceleration jerk etc temporal modelling important remark approach attempt model temporal evolution system joint probability model estimated statically temporal aspect data appears indirectly posteriori application continuity constraints select trajectory respect approach differs dynamical systems models based markovian assumptions hidden markov models trajectory models however fact duration speed trajectory plays role algorithm may make invariant time warping robust fast slow speech styles choice density model fact modes key aspect approach make sensitive density model finite mixtures spurious modes appear ripple superimposed density function regions mixture components sparsely distributed little interaction modes lead dp search wrong trajectory possible solutions improve density model perhaps increasing number components see regularisation smooth conditional distribution look bumps regions high probability mass instead modes however method may derived assuming distribution whole sequence normal markovian dependence adjacent frames carreira perpi dn computational cost dp search complexity nm average number modes per sequence point number points sequence experiments usually small dp search fast even long sequences bottleneck reconstruction part algorithm obtaining modes conditional distribution every point sequence many missing variables work envisage thorough experiments using data wisconsin ray microbeam database comparing recurrent mlps mlp committee may suitable multivalued mappings extensions algorithm include different geometric measures curvature based rather length based different strategies multiple pointwise imputation bump searching multidimensional constraints temporal spatial practical applications include audiovisual mappings speech hippocampal place cell reconstruction wind vector retrieval scatterometer data acknowledgments thank steve renals useful conversations comments paper references bartholomew latent variable models factor analysis charles griffin company ltd london bernstein coordination regulation movements pergamon oxford bishop neural networks pattern recognition oxford university press bishop svens williams gtm generative topographic mapping neural computation jan carreira perpifi mode finding gaussian mixtures technical report cs dept computer science university sheffield uk mar available online http www dcs shef ac uk miguel papers html ghahramani jordan supervised learning incomplete data via em approach nips pages hermansky perceptual linear predictive plp analysis speech acoustic soc amer apr josifovski cooke green vizinho state based imputation missing data robust speech recognition speech enhancement proc eurospeech pages little rubin statistical analysis missing data john wiley sons new york london sydney marchal hardcastle accor instrumentation database cross language study coarticulation language speech rahim goodyear kleijn schroeter sondhi use neural networks articulatory speech synthesis acoustic soc amer feb rohwer van der rest minimum description length regularization multimodal data neural computation apr roweis constrained hidden markov models nips volume saul rahim markov processes curves automatic speech recognition nips pages schroeter sondhi techniques estimating vocal tract shapes speech signal ieee trans speech audio process jan tresp neuneier ahmad efficient methods dealing missing data supervised learning nips pages
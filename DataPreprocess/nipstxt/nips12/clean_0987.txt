abstract reinforcement learning nonstationary environments generally regarded important yet difficult problem paper partially addresses problem formalizing subclass nonstationary environments environment model called hidden mode markov decision process hm mdp assumes environmental changes always confined small number hidden modes mode basically indexes markov decision process mdp evolves time according markov chain hm mdp special case partially observable markov decision processes pomdp modeling hm mdp environment via general pomdp model unnecessarily increases problem complexity variant baum welch algorithm developed model learning requiring less data time introduction reinforcement learning rl learning paradigm based upon framework markov decision process mdp traditional rl research assumes environment dynamics mdp parameters always fixed stationary assumption however realistic many real world applications elevator control instance passenger arrival departure rates vary significanfly one day modeled fixed mdp nonetheless rl nonstationary environments regarded difficult problem fact impossible task regularity ways environment dynamics change hence degree regularity must assumed typically nonstationary environments presummed change slowly enough online rl algorithms employed keep track changes online approach memoryless sense even environment ever revert previously learned dynamics learning must still need started choi yeung zhang proposed model paper proposes formal model nonstationary environments repeats dynamics certain ways model inspired observations real world nonstationary tasks following properties property environmental changes confined small number modes stationary environments distinct dynamics environment exactly one modes given time concept modes seems applicable many real world tasks elevator control problem example system might operate morning rush hour mode evening rush hour mode non rush hour mode one also imagine similar modes control tasks traffic control dynamic channel allocation property unlike states modes cannot directly observed current mode estimated according past state transitions analogous elevator control example passenger arrival rate pattern inferred occurrence pick drop requests property mode transitions stochastic events independent control system responses elevator control problem instance events change current mode environment could emergency meeting administrative office tea break staff th floor obviously elevator response control occurrence events property mode transitions relatively infrequent words mode likely retain time switching another one consider emergency meeting example employees different floors take time arrive administrative office thus would generate similar traffic pattern drop requests floor period time property number states often substantially larger number modes common property many real world applications elevator example state space comprises possible combinations elevator positions pick drop requests certainly would huge hand mode space could small instance elevator control system simply three modes described approximate reality based properties environment model proposed introducing mode variable capture environmental changes mode specifies mdp hence completely determines current state transition function reward function property mode however directly observable property evolves time according markov process property model therefore called hidden mode model note model impose constraint satisfy properties words hidden mode model work environments without two properties nevertheless shown later properties improve learning practice related work hidden mode model related nonstationary model proposed dayan sejnowski although model restrictive terms representational power involves much fewer parameters thus easier learn besides number possible modes assume knowledge environment model nonstationary reinforcement learning way environment dynamics change dayan sejnowski hand assume one knows precisely environment dynamics change hidden mode model also viewed special case hidden state model partially observable markov decision process pomdp shown later hidden mode model always represented hidden state model state augmentation nevertheless modeling hidden mode environment via hidden state model unnecessarily increase problem complexity paper conversion former latter also briefly discussed focus two approaches rl model based rl first acquires environment model optimal policy derived model free rl contrary learns optimal policy directly interaction environment paper concerned first part model based approach hidden mode model learned experience address policy learning problem separate paper hidden mode markov decision processes section presents hidden mode model basically hidden mode model defined finite set mdps share state space action space possibly different transition functions reward functions mdps correspond different modes system operates states completely observable transitions governed mdp contrast modes directly observable transitions controlled markov chain refer process hidden mode markov decision process hm mdp example hm mdp shown figure time es mode ff ff actloll state mode state action hm mdp evolution hm mdp arcs indicate dependencies variables figure hm mdp formally hm mdp tuple represent sets modes states actions respectively mode transition function maps mode fixed probability zm state transition function defines transition probability ym state given mode action stochastic reward function returns rewards mean value rm ii denote prior probabilities modes states respectively evolution modes states time depicted figure choi yeung zhang hm mdp subclass pomdp words former reformulated special case latter specifically one may take ordered pair mode observable state hm mdp hidden state pomdp observable state former observation latter suppose observable states modes rn respectively two hmmdp states together corresponding modes form two hidden states ra pomdp counterpart transition probability ra simply mode transition probability xm multiplied state transition probability ym mode state action hm mdp equivalent pomdp thus observations mn hidden states since state transition probabilities collapsed mode transition probabilities parameter sharing number parameters hm mdp mk much less corresponding pomdp learning hidden mode model two ways learn hidden mode model one may learn either hm mdp equivalent pomdp instead pomdp models learned via variant baum welch algorithm pomdp baum welch algorithm requires time storage learning mode nstate action hm mdp given data items similar idea applied learning hm mdp intuitively one estimate model parameters based expected counts mode transitions computed set auxiliary variables major difference original algorithm consecutive state transitions rather observations considered additional effort thus needed handling boundary cases hm mdp baum welch algorithm described figure empirical studies section empirically examines pomdp baum welch hm mdp baumwelch algorithms experiments based various randomly generated models real world environments conducted results quite consistent illustration simple traffic control problem presented problem one direction two way traffic blocked cars two different directions left right forced share remaining road coordinate traffic two traffic lights equipped sensors set system two possible actions either signal cars left cars right pass simplicity assume discrete time steps uniform speed cars system possible states correspond combinations whether cars waiting left right directions stop signal position previous time step traffic modes first one cars waiting left right directions probabilities respectively second mode probabilities reversed last one probabilities addition mode transition probability cost results xchrisman algorithm also attempts learn minimal possible number states paper concerns learning model parameters environment model nonstationary reinforcement learning given collection data initial model parameter vector repeat compute forward variables ri yi al ieq xij yj st st ieq vieq vieq compute backward variables xo ieq vieq vieq compute auxiliary variables gt vi jeq compute new model parameter maxi ioi oil figure hm mdp baum welch algorithm car waits either side experiments run initial model data sets various sizes algorithms iterated maximum change model parameters less threshold experiment repeated times different random seeds order compute median learned models compared pomdp forms using kullback leibler kl distance total cpu running time sun ultra workstation measured figure report results generally speaking algorithms learn accurate environment model data size increases figure result expected algorithms statistically based hence performance relies largely data size training data size small algorithms perform poorly however data size increases hm mdp baum welch improves substantially faster pomdp baum welch hm mdp general consists fewer free choi yeung zhang error transition function required learning time figure empirical results model learning parameters pomdp counterpart hm mdp baum welch also runs much faster pomdp baum welch figure holds general reason discussed note computational time necessarily monotonically increasing data size total computation depends data size also number iterations executed experiments noticed number iterations tends decrease data size increases larger models also tested hm mdp baum welch able learn models several hundred states modes pomdp baum welch unable complete learning reasonable time additional experimental results found discussions future work usefulness model depends validity assumptions made discuss assumptions hm mdp shed light applicability real world nonstationary tasks possible extensions also discussed modeling nonstationary environment number distinct mdps mdp flexible framework widely adopted various applications modeling nonstationary environments distinct mdps natural extension tasks comparing pomdp model comprehensive mdp naturally describes mode environment moreover formulation facilitates incorporation prior knowledge model initialization step states directly observable modes completely observable states helpful infer current mode also possible extend mode allow partially observable states case extended model would equivalent representational power pomdp could proved easily showing reformulation two models directions environment model nonstationary reinforcement learning mode changes independent agent responses property may always hold real world tasks applications agent actions might affect state well environment mode case mdp used govern mode transition process mode transitions relatively infrequent property generally holds many applications model however limited condition tried apply model learning algorithms problems property hold find model still outperforms pomdp although required data size typically larger models number states substantially larger number modes key property significantly reduces number parameters hm mdp compared pomdp practice introduction modes sufficient boosting system performance modes might help little thus trade performance response time must decided additional issues need addressed first efficient algorithm policy learning required although principle achieved indirectly via pomdp algorithm efficient algorithm based model based approach possible address issue separate paper next number modes currently assumed known investigating remove limitation finally exploration exploitation issue currently ignored future work address important issue apply model real world nonstationary tasks
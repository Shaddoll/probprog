abstract describe iterative algorithm building vector machines used classification tasks algorithm builds ideas support vector machines boosting generalized additive models algorithm used various continuously differential functions bound discrete classification loss simple implement test proposed algorithm two different loss functions synthetic natural data also describe norm penalized version algorithm exponential loss function used adaboost performance algorithm natural data comparable support vector machines typically running time shorter svm introduction support vector machines svm boosting highly popular effective methods constructing linear classifiers theoretical basis svms stems vapnik seminal learning generalization proved great practical usage first boosting algorithms hand developed answer certain fundamental questions pac learnability mathematically beautiful algorithms rather impractical later freund schapire developed ariaboost algorithm proved practically useful meta learning algorithm ariaboost works making repeated calls weak learner call weak learner generates single weak hypothesis weak hypotheses combined ensemble called strong hypothesis recently schapire singer studied simple generalization ariaboost weak hypothesis assign real valued confidence prediction even recently friedman hastie tibshirani presented alternative view boosting statistical point view also described new family algorithms constructing generalized additive models base learners similar fashion ariaboost work friedman hastie tibshirani generated lots attention motivated research classification algorithms employ various loss functions work combine ideas research mentioned devise alternative approach construct vector machines classification svm base predictors use mercer kernels value kernel evaluated input pattern dot product two instances embedded high dimensional space viewed real valued prediction describe simple extension additive models prediction base learner linear transformation given kernel describe iterative algorithm greedily adds kernels derive algorithm using exponential loss function used ariaboost loss function used friedman hastie tibshirani logitboost brevity call resulting classifiers boosted vector machines bvm logistic vector machines lvm would like note passing leveraged vector machines resulting algorithms boosting algorithms pac sense instance weak learnability assumption weak learner always find weak hypothesis violated therefore adopt terminology used call resulting classifiers leveraged vector machines leveraging procedure give adopts chunking technique svm presenting basic leveraging algorithms compare performance svm synthetic data experimental results show leveraged vector machines achieve similar performance svm often resulting vector machines smaller ones obtained svm experiments also demonstrate bvm especially sensitive malicious label noise lvm seems insensitve also describe simple norm penalized extension bvm provides partial solution overfitting presence noise finally give results experiments performed natural data uci repository conclude preliminaries let xl xm ym sequence training examples instance xi belongs domain instance space label yi methods described paper build vector machines svms extended solve multiclass problems using instance error correcting output coding methods beyond scope paper discussed elsewhere convenience use denote yi boosting assume access weak base learning algorithm accepts input weighted sequence training examples given input weak learner computes weak base hypothesis general form interpret sign predicted label assigned instance magnitude ih confidence prediction build vector machines use notion confidence rated predictions take base hypotheses sample based mercer kernels define confidence magnitude prediction base learner value dot product another instance sign prediction set label corresponding instance formally base hypothesis exist xj yj yjk xj defines inner product feature space ak jk tt bk denote function induced instance label pair yj kernel ckj yjk xj goal find classifier called strong hypothesis context boosting algorithms form tt atht signs predictions classifier agree much possible labels training instances leverage algorithm describe maintains distribution indices distribution simply vector non negative weights one weight per example exponential function classifier built incrementally exp yif xi zexp yif xi random function input instances labels denote sample expectation according ed eim xi yi also use notation denote expectation matrices random functions convert confidencerated classifier randomized predictor using soft max function denote xi exp xi xi exp xi exp xi exp xi sin leveraging algorithm basic procedure construct leveraged vector machines builds ideas extending prediction linear function base classifiers algorithm works rounds constructing new classifier ft previous one ft adding new base hypothesis ht current classifier ft denoting dt pt distribution probability given eqn eqn using ft ft algorithm attempts minimize either exponential function arise adaboost exp yift xi exp yi ft xi atht xi dt exp yi atht xi logistic loss function log exp yift xi log exp yi ft xi atht xi log pt xi log pt xi initialize fo zero everywhere run procedure predefined number rounds final classifier therefore ft atht atht et would like note parenthetically possible use loss functions bound classification loss see instance focus loss functions fixing ft ht functions convex guarantees mild conditions details omitted due lack space uniqueness fit round look current base hypothesis ht reduce loss function discussed input instance xj defines function candidate ht general close form solution eqn finding possible input instance time consuming therefore use quadratic approximation loss functions using quadratic approximation find analytically calculate reduction loss function let vz oz oz oz column vectors partial derivatives andvl fixing ft ht similarly let matrices second order derivatives respect quadratic approximation yields vl round maintain distribution dt defined ft given eqn conditional class probability estimates pt xi given eqn solving linear equation possible instance done setting ht get otj ied ed ed leveraged vector machines figure comparison test error function number leveraging rounds using full numerical search one step numerical search based quadratic approximation loss function one step search chunking instances note equations share much common require pre computing zi amount computation time calculating value instance arj simply evaluate corresponding value loss function choose instance attains minimal loss set ht numerically search optimal value iterating eqn eqn summing values would like note typically two three iterations suffice save time using value found using quadratic approximation without full numerical search optimal value see also fig repeat process rounds instance serve base hypothesis note instance chosen although consecutive iterations typically small fraction instances actually used building roughly speaking instances support patterns leveraged machines although necessarily geometric support patterns svms order make search base hypothesis efficient pre compute store pairs storing values require space might prohibited large problems save space employ idea chunking used svm partition blocks size divide iterations sub groups iterations belonging ith sub group use evaluate kernels based instances ith block switching new block need compute values sk division blocks might expensive since typically use block instances however storage kernel values done place thus save factor memory requirements practice found chunking hurt performance fig show test error function number rounds using full numerical search determine round using quadratic approximation one step find using quadratic approximation chunking number instances experiment block chunking size switch different block every iterations description data given next section example iterations virtually difference performance different schemes experiments synthetic data section describe experiments synthetic data comparing different aspects leveraged vector machines svms original instance space two dimensional positive class includes points inside circle radius instance ul labeled iff instances picked random according zero mean unit variance normal distribution set exactly half instances belong positive class experiments described section generated groups training test sets includes train test examples overall training examples test examples singer figure performance comparison svm bvm function training data size left dimension kernels middle number redundant features figure train test errors svm lvm bvm function label noise average variance estimates empirical errors across experiments svm set regularization parameter used iterations build leveraged machines experiments without noise results bvm lvm practically therefore compare bvm svm fig unless said otherwise used polynomials degree two kernels hence data separable absence noise first experiment tested sensitivity number training examples omitting examples training data without modification test sets left part fig plot test error function number training examples test error bvm almost indistinguishable error svm performance methods improves fast function training examples next compared performance function dimension polynomial constituting kernel ran algorithms kernels form results depicted middle plots fig performance bvm svm close note small scale axis test error experiment conclude experiments clean realizable data checked sensitivity irrelevant features input input instance ul augmented random elements us ut form input vector dimension right hand side graphs fig shows test error function see performance algorithms similar next compared performance algorithms presence noise used kernels dimension two instances without redundant features label instance flipped probability ran sets experiments set included runs used training examples test examples fig show average training error left average test error right algorithms apparent graphs bvms built based exponential loss much sensitive noise svms lvms generalization error degrades significantly even low noise rates generalization error lvms hand slightly worse svms although leveraged vector machines figure training error test error cumulative norm zt la function number leveraging iterations lvm bvm pbvm algorithmic difference constructing bvms lvms loss function fact lvms exhibit performance similar svm partially attributed fact asymptotic behavior loss functions norm penalized version one problems boosting corresponding leveraging algorithm exponential loss described might increase confidence instances misclassifying many instances albeit small confidence often happens late rounds distribution dt concentrated examples leveraging algorithm typically assigns large weight weak hypothesis effect instances therefore desired control complexity leveraged classifiers limiting magnitude base hypotheses weights several methods proposed limit confidence adaboost using instance regularization smoothing predictions propose norm penalized method bvm simple implement maintains convexity properties objective function following idea cortes vapnik svms nonseparable case add following penalization term exp tt lat simple algebric manipulation implies objective function tth round bvms penalization term dt exp yi atht xi bt exp latlp also easy show penalty parameter updated round exp lat llv zt since zt unless kernel function better random typically increases function forcing new weights small note eqn implies search base predictor ht weights fit round still done independently previous rounds maintaining distribution dt single regularization value penalty term simply adds diagonal term matrix second order derivatives eqn algorithm follows line details omitted brevity call norm penalized leveraging procedure pbvm fig plot test error right training error middle latl functions number rounds lvm bvm pbvm training set example made small purpose examples contaminated label noise small example lvm bvm overfit pbvm stops increasing weights finds reasonably good classifier plots demonstrate norm penalized version safeguard overfitting preventing weights growing arbitrarily large effect penalized version similar early stopping would like singer svm lvm bvm rbvm svm lvm bvm pbvm example data set size size size size error error error error source feature labor uci echocard uci bridges uci hepatitis uci horse colic uci liver uci ionosphere uci vote uci ticketl att ticket att ticket att bands uci bmast wisc uci pinna uci german uci weather uci network art splice uci boa att table summary results collection binary classification problems note found experimentally norm penalized version compensate incorrect estimates due malicious label noise experimental results given next section show however indeed help preventing overfitting training set small experiments natural data compared practical performance leveraged vector machines svms collection nineteen dataset uci machine learning repository networking marketing data svm set built leveraged vector machines using rounds pbvm used used chunking building leveraged vector machines dividing training set blocks datasets exception boa used fold cross validation calculate test error dataset boa training examples test examples performance svm lvm pbvm seem comparable fact exception datasets differences error rates statistically significant three methods svm pbvm lvm lvm simplest implement time required build lvm typically much shorter svm also worth noting size leveraged machines often smaller size corresponding svm finally apparent pbvms frequently yield better results bvms especially small medium size datasets
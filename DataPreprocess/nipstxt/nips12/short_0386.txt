abstract present new technique time series analysis based dynamic probabilistic networks approach observed data modeled terms unobserved mutually independent factors recently introduced technique independent factor analysis ifa however unlike ifa factors factor temporal statistical characteristics derive family em algorithms learn structure underlying factors relation data algorithms perform source separation noise reduction integrated manner demonstrate superior performance compared ifa introduction technique independent factor analysis ifa introduced provides tool modeling dim data terms unobserved factors factors mutually independent combine linearly added noise produce observed data mathematically model defined yt hxt ut xt vector factor activities time yt data vector mixing matrix ut noise origins ifa lie applied statistics one hand signal processing hand statistics ancestor ordinary factor analysis fa assumes gaussian factors contrast ifa allows factor arbitrary distribution modeled semi parametrically dim mixture gaussians mog mog parameters well mixing matrix noise covariance matrix learned observed data expectation maximization em algorithm derived signal processing ancestor ifa independent component analysis ica method blind source separation ica factors termed sources task blind source separation recover observed data knowledge mixing process sources ica non gaussian distributions unlike ifa distributions usually fixed prior knowledge quite limited adaptability significant restrictions dynamic independent factor analysis number set data dimensionality square mixing mixing matrix assumed invertible data assumed noise free ut contrast ifa allows including sources sensors well non zero noise unknown covariance addition use flexible mog model often proves crucial achieving successful separation therefore ifa generalizes unifies fa ica tl model learned used classification fitting ifa model class completing missing data context blind separation optimal reconstruction sources xt data obtained using map estimator however ifa ancestors suffer following shortcoming oblivious temporal information since attempt model temporal statistics data see square noise free mixing words model learned would affected permuting time indices yt unfortunate since modeling data time series would facilitate filtering forecasting well accurate classification moreover source separation applications learning temporal statistics would provide additional information sources leading cleaner source reconstructions see one may think problem blind separation noisy data terms two components source separation noise reduction possible approach might following two stage procedure first perform noise reduction using wiener filtering second perform source separation cleaned data using ica algorithm notice procedure directly exploits temporal second order statistics data first stage achieve stronger noise reduction alternative approach would exploit temporal structure data indirectly using temporal source model resulting single stage algorithm operations source separation noise reduction coupled approach taken present paper following present new approach independent factor problem based dynamic probabilistic networks order capture temporal statistical properties observed data describe source hidden markov model hmm resulting dynamic model describes multivariate time series terms several independent sources temporal characteristics section presents em learning algorithm zero noise case section presents algorithm case isotropic noise case non isotropic noise turns computationally intractable section provides approximate em algorithm based variational approach notation multivariable gaussian density denoted ry exp zt wo rk point time blocks denoted xt ith coordinate xt xl function denotes averaging ensemble blocks zero noise mog source model employed ifa advantages capable approximating arbitrary densities ii learned efficiently data em gaussians correspond hidden states sources labeled assume time source state sl signal xl generated order sampling gaussian distribution mean uis variance capture temporal statistics data endow sources temporal structure introducing transition matrix states focusing attias time block resulting probabilistic model defined rs st st yl det xl xi joint density sources time points last equation follows xt gyt unmixing matrix usual noise free scenario see section assuming mixing matrix square invertible graphical model observed density defined parametrized gij es rs model describes source first order hmm reduces time independent model whereas temporal structure described means moving average autoregressive model hmm advantageous since models high order temporal statistics facilitates em learning omitting derivation maximization respect gij results incremental update rule xt xttg es es natural gradient used appropriately chosen learning rate source parameters obtain update rules et es et et used standard hmm initial probabilities updated via notation posterior densities computed step source given terms data via gijyt using forward backward procedure algorithm may used several possible generalized em schemes efficient one given following two phase procedure freeze source parameters learn separating matrix using ii freeze learn source parameters using go back repeat notice rule similar natural gradient version bell sejnowski ica rule fact two coincide time independent sources ck xi ogp xi oxi also recognize baum welch method hence phase algorithm separates sources using generalized ica rule whereas phase ii learns hmm source remark often one would like model given variable time series terms smaller number factors framework noise free model yt hxt achieved applying algorithm largest principal components data notice data indeed generated factors remaining principal components would vanish equivalently one may apply algorithm data directly using non square unmixing matrix results figure demonstrates performance method mixture speech signals passed non linear function modify distributions mixture inseparable ica source model used latter fit actual source densities see discussion also applied dynamic network mixture speech signals whose distributions dynamic independent factor analysis hmm ica ica figure left two four source distributions middle outputs em algorithm nearly independent right outputs ica correlated made gaussian appropriate non linear transformation since temporal information crucial separation case see mixture inseparable ica ifa however algorithm accomplished separation successfully isotropic noise turn case non zero noise ut assume noise white zero mean gaussian distribution covariance matrix general case computationally intractable see section reason estep requires computing posterior distribution source states zero noise case also source signals posterior quite complicated structure show assume isotropic noise aij sij well square invertible mixing posterior simplifies considerably making learning inference tractable done adapting idea suggested dynamic probabilistic network start pre processing data using linear transformation makes covariance matrix unity ytyt sphering denotes averaging point time blocks follows hsh xtxt diagonal covariance matrix sources square invertible implies hth diagonal fact since unobserved sources determined within scaling factor set variance source unity obtain thogonality property shown source posterior factorizes product individual sources xx yx hi ixi lyl yl vtp vop means variances time well quantities depend data yt states particular hjiyt vs av ys using expression omitted transition probabilities hence posterior distribution effectively defines new hmm source yt dependent emission transition probabilities derive learning rule first compute conditional mean source signals time given data done recursively using forward backward procedure obtain czc eytit attias fractional form results imposing orthogonality constraint hth using lagrange multipliers computed via diagonalization procedure source parameters computed using learning rule omitted similar noise free rule easy derive learning rule noise level well fact ordinary fa rule would suffice point algorithm derived case perfectly well defined though sub optimah see non isotropic noise general case non isotropic noise non square mixing computationally intractable exact step requires summing possible source configurations st times tl intractability tl problem stems fact sources independent sources conditioned data vector correlated resulting large number hidden configurations problem arise noise free case avoided case isotropic noise square mixing using orthogonality property cases exact posterior sources factorizes em algorithm derived based variational approach approach introduced context sigmoid belief networks constitutes general framework ml learning intractable probabilistic networks used hmm context idea use approximate tractable posterior place lower bound likelihood optimize parameters maximizing bound starting point deriving bound likelihood neal hinton ill formulation em algorithm ogp eqlogp yt ixt eqlogp eqlogq eq denotes averaging respect arbitrary posterior density hidden variables given observed data xl yl exact em shown obtained maximizing bound respect posterior corresponding step model parameters mstep however resulting true intractable posterior contrast variational em choose differs true posterior facilitates tractable step step use sb li htq parametrized sl st xt xt pt thus variational transition probabilities described multiplying parameters subject normalization constraints original ones source signals time jointly gaussian mean pt covariance means covariances transition probabilities time datadependent pt etc parametrization scheme motivated become form posterior notice quantities vs variational parameters related scheme used different context since parameters adapted independently model parameters non isotropic algorithm expected give superior results compared isotropic one dynamic independent factor analysis lo mixing reconstruction snr db snr db right quality source figure left quality model parameter estimates reconstructions see text course true posterior xt correlated temporally among st latter factorize best approximate variational parameters optimized maximize bound equivalently minimize kl distance true posterior requirement leads fixed point equations pt hta bt hta lyt bt hta bt exp logv es ij es fit ensure factors malization hmm quantities computed forward backward procedure using variational transition probabilities variational parameters determined solving eqs iteratively block yl practice found less iterations usually required convergence step update rules given mixing parameters source parameters computed using variational transition probabilities notice learning rules source parameters baum welch form spite correlations conditioned sources variational approach correlations hidden manifested fact fixed point equations couple parameters across time points since depends ks sources source reconstruction xt yl observe map source estimate given pt yl depends results algorithm demonstrated source separation task figure used speech signals transformed non linearities arbitrary one point densities mixed random matrix different signalto noise snr levels used error estimated left solid line quantified size non diagonal elements ho relative ytpt ptpt et ytyt ytpt attias diagonal results obtained ifa use temporal information plotted reference dotted line mean squared error reconstructed sources right solid line corresponding ifa result right dashed line also shown estimate reconstruction errors algorithm consistently smaller ifa reflecting advantage exploiting temporal structure data additional experiments different numbers sources sensors gave similar results notice algorithm unlike previous two allows also considered situations number sensors smaller number sources separation quality good although expected less opposite case conclusion important issue addressed model selection applying algorithms arbitrary dataset number factors hmm states factor determined whereas could done principle using cross validation required computational effort would fairly large however recent paper develop new framework bayesian model selection well model averaging probabilistic networks framework termed variational bayes proposes em like algorithm approximates full posterior distributions hidden variables also parameters model structure well predictive quantities analytical manner currently applied algorithms presented good preliminary results one field approach may find important applications speech technology suggests building economical signal models based combining independent low dimensional hmms rather fitting single complex hmm may also contribute toward improving recognition performance noisy multispeaker reverberant conditions characterize real world auditory scenes references attias independent factor analysis neut comp bell sejnowski information maximization approach blind separation blind aleconvolution neut comp amari cichocki yang new learning algorithm blind signal separation adv neut info roc sys ed touretzky et al mit press cambridge pearlmutter parra maximum likelihood blind source separation context sensitive generalization ica adv neut info proc sys ed mozer et al mit press cambridge hyv inen oja fast fixed point algorithm independent component analysis neut comp attias schreiner blind source separation aleconvolution dynamic component analysis algorithm neut comp rabiner juang fundamentals speech recognition prentice hall englewood cliffs nj lee sompolinsky unpublished lee personal communication saul jaakkola jordan mean field theory sigmoid belief networks art int res ghahramani jordan factorial hidden markov models mach learn neal hinton view em algorithm justifies incremental sparse variants learning graphical models ed jordan kluwer academic press attias variational bayesian framework graphical models adv neut info proc sys ed leen et al mit press cambridge
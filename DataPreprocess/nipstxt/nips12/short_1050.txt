abstract problem reinforcement learning non markov environment explored using dynamic bayesian network conditional independence assumptions random variables compactly represented network parameters parameters learned line approximations used perform inference compute optimal value function relative effects inference value function approximations quality final policy investigated learning solve moderately difficult driving task two value function approximations linear quadratic found perform similarly quadratic model sensitive initialization performed level human performance task dynamic bayesian network performed comparably model using localist hidden state representation requiring exponentially fewer parameters
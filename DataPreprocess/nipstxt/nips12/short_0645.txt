abstract propose novel approach building finite memory predictive models similar spirit variable memory length markov models vlmms models constructed first transforming block structure training sequence spatial structure points unit hypercube longer common suffix shared two blocks closer lie point representations transformation embodies markov assumption blocks long common suffixes likely produce similar continuations finding set prediction contexts formulated resource allocation problem solved vector quantizing spatial block representation compare model classical variable memory length markov models three data sets different memory stochastic components models superior performance yet construction fully automatic shown problematic case vlmms
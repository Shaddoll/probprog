abstract paper present committee new multi class learning algorithm related winnow family algorithms committee algorithm combining predictions set sub experts online mistake bounded model learning sub expert special type attribute predicts distribution finite number classes committee learns linear function sub experts uses function make class predictions provide bounds committee show performs well target represented relevant sub experts also show committee used solve traditional problems composed attributes leads natural extension learns multi class problems contain traditional attributes sub experts introduction paper present new multi class learning algorithm called committee committee learns class target function combining information large set sub experts sub expert special type attribute predicts distribution target classes target space functions linear max functions define functions take linear combination sub expert predictions return class maximum value may useful think sub experts individual classifying functions attempting predict target function even though individual sub experts may perfect committee attempts learn linear max function represents target function truth picture quite accurate reason call sub experts experts even though individual sub expert might poor prediction may useful used linear max function example sub experts might used add constant weights linear max function algorithm analyzed line mistake bounded model learning lit useful model type incremental learning algorithm use feedback current hypothesis improve performance model algorithm goes series learning trials trial composed three steps first algorithm part work supported nec research institute princeton nj mesterharm receives instance case predictions sub experts second algorithm predicts label instance global prediction committee last algorithm receives true label instance committee uses information update estimate target goal algorithm minimize total number prediction mistakes algorithm makes learning target analysis performance committee similar another learning algorithm winnow lit winnow algorithm learning linear threshold function maps attributes binary target algorithm effective concept represented relevant attributes irrespective behavior attributes committee similar deals learning target contains relevant sub experts learning sub experts interesting right turns distinction two tasks significant show section transform attributes sub experts using particular transformations committee identical winnow algorithms balanced wma lit furthermore generalize transformations handle attribute problems multi class targets transformations naturally lead hybrid algorithm allows combination sub experts attributes multi class learning problems opens range new practical problems easily fit previous framework attributes binary classification previous work many people successfully tried winnow algorithms real world tasks course work made modifications algorithms fit certain aspects problem modifications include multi class extensions example dkr use winnow algorithms text classification problems multiclass problem special form document belong one class property makes sense learn different binary classifier class linear functions allowed even desired overlap however paper concerned cases possible example gr correct spelling word must selected set many possibilities setting desirable algorithm select single word work gr presents many interesting ideas modifications winnow algorithms minimum modification useful improving performance winnow particular problems part work also extends winnow algorithm general multi class problems results favorable contribution paper give different algorithm stronger theoretical foundation customizing particular multi class problem blum also works multi class winnow algorithms calendar scheduling problem mcf blu modified winnow given theoretical arguments good performance certain types multi class disjunctions paper results extended new algorithm committee cover wider range multi class linear functions related theoretical work multi class problems includes regression algorithm eg kw kivinen warmuth introduce eg algorithm related winnow used regression problems general regression useful framework many multi class problems straightforward extend regression concepts learned committee particular problem inability current regression techniques handle loss multi class linear learning algorithm related winnow algorithm section paper describes details committee give formal statement algorithm near end section prediction scheme assume sub experts sub expert positive weight used vote different classes let wi weight sub expert sub expert vote several classes spreading weight prediction distribution example sub expert may give weight class weight class weight class let xi represent prediction distribution fraction committee predicts weight sub expert gives class vote class wix class highest vote ties algorithm picks one classes involved tie call function computed prediction scheme linear max function since maximum class value taken linear combination sub expert predictions target function goal committee minimize number mistakes quickly learning sub expert weights correctly classify target function assume exists vector nonnegative weights correctly classifies target notice tz multiplied constant without changing target remove confusion normalize weights sum tzi let target vote class bti part difficulty learning problem hidden target weights intuitively target function difficult learn small difference votes correct incorrect classes measure difficulty looking minimum difference trials vote correct label vote labels assume trial pt correct label min tetrials weights target target always makes correct prediction one problem assumptions allow noise cases however variations analysis allow limited amounts noise lit lit also experimental work lit lm shows family winnow algorithms much robust noise theory would predict based similarity algorithm analysis preliminary experiments committee able tolerate noise updates committee updates mistakes using multiplicative updates algorithm starts initializing weights trials let correct label predicted label committee weight sub expert multiplied ct corresponds increasing weights sub experts predicted mesterharm correct label instead label committee predicted value initialized start algorithm optimal value bounds depends often known advance experiments winnow algorithms suggest algorithms flexible often performing well wider range values lm last weights renormalize sum strictly necessary normalizing several advantages including reducing likelyhood underflow overflow errors committee code initialization vi wi set trials instance sub experts xl xn prediction first class update let co ect label mistake wi wi nomalize weights classes mistake bound space give proof mistake bound committee technique similar proof winnow algorithm balanced given lit complete proof reader refer mes theorem committee makes mistakes target conditions section satisfied set surprisingly bound refer number classes effects larger values show indirectly value obvious bound shows committee performs well target represented small fraction sub experts call sub experts target relevant sub experts since function target depends relevant sub experts hand remaining sub experts small effect bound since represented ln factor means mistake bound committee fairly stable even adding large number additional sub experts truth mean algorithm good bound relevant sub experts cases small number sub experts give arbitrarily small value general problem winnow algorithms mean given problem increasing number irrelevant sub experts logarithmic effect mistake bound attributes sub experts often obvious sub experts use solving learning problem many times information available set attributes attributes show use committee learn natural kind class target function linear machine learn target transform attribute separate sub experts use notion committee help understand transformation multi class linear learning algorithm related winnow attribute target linear machine linear machine dh prediction function divides feature space disjoint convex regions class corresponds one region predictions made comparing value different linear functions function corresponds class formally assume attributes classes let zi attribute assume target function represented using linear functions attributes let ei tz zi linear function class tz weight attribute class notice added one extra attribute attribute set needed constant portion linear functions target function labels instance class largest function ties defined therefore similar voting function class used committee transforming target one difficulty linear functions may negative weights since committee allows targets nonnegative weights need transform equivalent problem nonnegative weights difficult since concerned relative difference functions allowed add function functions long add functions gives us simple procedure remove negative weights example lz add every function remove negative weights straightforward extend remove negative weights also need normalize weights since relative difference functions matter divide functions constant normalize weights sum point without loss generality assume original functions nonnegative normalized last step identify value use definition committee substituting corresponding functions linear machine assume trial pt correct label min pt tetmals transforming attributes transformation works follows convert attribute zi sub experts sub expert always vote one classes value zi target weight sub experts corresponding target weight attribute label pair functions every attribute zi zi notice using distributions sub expert predictions sub expert prediction converted distribution adding constant amount class prediction example sub expert predicts zl changed zl adding class conversion affect predicting updating committee mesterharm theorem committee makes ln mk mistakes linear machine defined section set proof target transformation creates mk normalized target sub experts vote functions linear machine therefore set sub experts value plugging values bound committee gives result transformation provides simple procedure solving linear machine problems details transformation may look cumbersome actual implementation algorithm relatively simple need explicitly keep track sub experts instead algorithm use linear machine type representation class keeps vector weights one weight attribute update correct class weights predicted class weights changed correct class weights multiplied predicted class weights multiplied procedure similar balanced algorithm lit fact identical similar transformation duplicates behavior linear threshold learning version wma given lit zi zi transformation shows advantages research needed determine proper way generalize multi class case transformations bounds given paper equivalent except superficial adjustment notation wma original bounds given lit combining attributes sub experts transformations suggest proper way hybrid algorithm combines sub experts attributes use transformations create new sub experts attributes combine original sub experts running committee may even desirable break original sub experts attributes use algorithm sub experts may perform better certain classes example felt sub expert particularly good class perform following transformation instead using one weight whole sub expert committee also learn based sub expert performance first class even good target representable original sub experts additional sub experts large effect logarithmic bound vein may useful add constant attributes set sub experts add extra sub experts allow algorithm represent larger set target functions conclusion paper introduced committee multi class learning algorithm feel algorithm important practice extending range problems handled winnow family algorithms solid theoretical foundation researchers customize winnow algorithms handle various multi class problems multi class linear learning algorithm related winnow part customization includes feature transformations show committee handle general linear machine problems transforming attributes sub experts suggests way hybrid learning algorithm allows combination sub experts attributes techniques also used add representational power standard sub expert problem future plan empirically test committee feature transformations real world problems part testing include modifying algorithm use extra information related proof technique jmes attempt lower number mistakes speculate adjusting multiplier increase change progress per trial useful certain types multi class problems acknowledgments thank nick littlestone stimulating work suggesting techniques converting balanced algorithm multi class targets also thank haym hirsh nick littlestone warren smith providing valuable comments corrections
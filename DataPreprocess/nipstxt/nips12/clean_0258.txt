abstract recent interpretations adaboost algorithm view performing gradient descent potential function simply changing potential function allows one create new algorithms related adaboost however new algorithms generally known formal boosting property paper exmines question potential functions lead new algorithms boosters two main results general sets conditions potential one set implies resulting algorithm booster implies algorithm conditions applied previously studied potential functions used logitboost doom ii introduction first boosting algorithm appeared rob schapire thesis algorithm able boost performance weak pac learner resulting algorithm satisfies strong pac learning criteria call method builds strong pac learning algorithm weak pac learning algorithm pac boosting algorithm freund schapire later found improved pac boosting algorithm called adaboost also tends improve hypotheses generated practical learning algorithms adaboost algorithm takes labeled training set produces master hypothesis repeatedly calling given learning method given learning method used different distributions training set produce different base hypotheses master hypothesis returned adaboost weighted vote base hypotheses adaboost works iteratively determining examples poorly classified current weighted vote selecting distribution training set emphasize examples recently several researchers noticed adaboost performing constrained gradient descent exponential potential function margins examples margin example yf valued label example net weighted vote master hypothesis adaboost seen way clear algorithms may derived changing potential function potential boosters exponential potential used adaboost property influence data point increases exponentially repeatedly misclassified base hypotheses concentration hard examples allows ariaboost rapidly obtain consistent hypothesis assuming base hypotheses certain properties however also means incorrectly labeled noisy example quickly attract much distribution appears lack noisetolerance one adaboost drawbacks tt several researchers proposed potential functions concentrate much hard examples however generally show derived algorithms pac boosting property paper return original motivation behind boosting algorithms ask potential functions gradient descent lead pac boosting algorithms boosters create strong pac learning alg orithms arbitrary weak pac learners give necessary conditions met proposed potential functions notably logitboost potential introduced friedman et al furthermore show simple gradient descent proposed potential functions sigmoidal potential used mason et al cannot convert arbitrary weak pac learning algorithms strong pac learners aim work identify properties potential functions required pac boosting order guide search effective potentials potential functions additional tunable parameter change time results yet apply dynamic potentials pac boosting define notions pac learning boosting define notation used throughout paper concept subset learning domain random example pair drawn distribution otherwise concept class set concepts definition strong pac learner concept class property every distribution concepts probability least algorithm outputs hypothesis pd learning algorithm given ability draw random examples distribution must run time bounded poly definition weak pac learner similar strong pa learner except need satisfy conditions particular co pair rather pairs definition pac boosting algorithm generic algorithm leverage weak pa learner meet strong pa learning criteria remainder paper emphasize boosting accuracy much easier boost confidence see haussler et al freund details furthermore emphasize boosting sampling strong pac learner draws large sample iteration weak learning algorithm called distribution sample xto simplify presentation omit instance space dimension taxget representation length parameters duj helmbold throughout paper use following notation cardinality fixed sample xl yl xm ht valued weak hypothesis created iteration weight vote ht master hypothesis may may normalized tt oft ft ott ht master hypothesis iteration ui yi tt ott ht margin xi iteration subscript often omitted note margin positive master zt hypothesis correct normalized margin ui potential instance margin total potential zim ui es probability respect unknown distribution domain probability aud expectations respect uniform distribution sample respectively results apply total potential functions form positive strictly decreasing leveraging learners gradient descent adaboost recently interpreted gradient descent independently several groups interpretation adaboost seen minimizing total potential ui im exp ui via feasible direction gradient descent iteration adaboost chooses direction steepest descent distribution sample calls weak learner obtain new base hypothesis ht weight new weak hypothesis calculated minimize resulting potential im ui im exp ui ott lyiht xi gradient descent idea generalized potential functions duffy et al prove bounds similar gradient descent technique using non componentwise non monotonic potential function note weak learner returns good hypothesis ht training error dt xi yiht xi set assume base hypothesis produced satisfies zim dt xi yiht xi paper consider general gradient descent approach applied various potentials ui note potential function two corresponding gradient descent algorithms see un normalized algorithms like adaboost continually add new weak hypotheses preserving old normalized algorithms scale always sum general call algorithms leveraging algorithms reserving term boosting actually pac boosting property potentials boost section describe sufficient conditions potential functions corresponding leveraging algorithm pac boosting property prediction master hypothesis instance sign ft sour current proofs require actual greater constant say therefore minimizing may need reduced potential boosters apply conditions show two potentials literature lead boosting algorithms theorem let potential function derivative increasing decreasing neither normalized un normalized leveraging algorithms sponding potential pa boosting property co rre theorem proven adversary argument whenever concept class sufficiently rich adversary keep constant fraction sample correctly labeled master hypothesis thus error tolerance goes zero master hypotheses sufficiently accurate apply theorem two potential functions literature friedman et al describe potential call squared error potentialatxiis yi er ef potential written asp ui eu corollary potential squared error lead boosting algorithm proof potential satisfies conditions theorem strictly decreasing second condition holds mason et al examine normalized algorithm using potential pa tanh au algorithm optimizes choices via cross validation uses weak learners slightly different properties however plug potential directly gradient descent framework examine resulting algorithms corollary doomii potential pd lead boosting algorithm fixed proof potential strictly decreasing second condition theorem holds techniques show potentials sigmoidal nature lead algorithms pac boosting property since sigmoidal potentials generally better estimates loss potential used adaboost results imply boosting algorithms must use potential subtle properties simply upper bounding loss potential functions boost section give sufficient conditions potential function corresponding un normalized algorithm pac boosting property result implies adaboost logitboost pac boosting property although previously known adaboost believe new result logitboost vc dimension concept class consisting pairs intervals real line sufficient adversary duj helmbold one set conditions potential imply decreases roughly exponentially un normalized margins large margins exponential region ideas similar used adaboost analysis show minimum normalized margin quickly becomes bounded away zero allows us bound generalization error using theorem bartlett et al second set conditions governs behavior potential function un normalized margins large enough conditions imply total potential decreases constant factor iteration therefore much time spent margins enter exponential region margin value bounding exponential region ui margins ui remain exponential region following theorem gives conditions ensuring ui quickly becomes less theorem following conditions hold strictly decreasing qp vu im ui er ti bq ln mp iterations proof theorem approximates new total potential old potential minus ct times linear term plus error bounding error function ct minimizing demonstrate values ct give sufficient decrease total potential theorem following conditions hold iteration tx vq whenever im ui strictly decreasing ff cp ff vu yft nentially txp fro tx ln qv ln decreases expothe proof theorem generalization adaboost proof combining two theorems generalization bound theorem bartlett et al gives following result vc dimension weak hypothesis class theorem edges exists tx poly ur satisfying conditions theorem poly poly time poly examples norpotential boosters malized margin least ln pv yfr ln ln log choosing appropriately makes error rate sufficiently small algorithm corresponding pa boosting property apply theorem show adaboost logitboost potentials lead boosting algorithms boosting potentials section show direct consequence theorem potential functions adaboost logitboost lead boosting algorithms note logitboost algorithm analyze exactly described friedman et al weak learner optimizes square loss appears better fit potential first derive boosting property adaboost corollary adaboost potential boosts proof prove simply need show potential exp satisfies conditions theorem done setting ln fi andt corollary log likelihood potential used logitboost boosts proof case ln set ln andq exp ur lv thex orem shows poly iterations conditions theorem satisfied conclusions paper examined leveraging weak learners using gradient descent approach approach direct generalization adaboost algorithm adaboost exponential potential function replaced alternative potentials demonstrated properties potentials sufficient show resulting algorithms pac boosters properties imply resulting algorithms pac boosters applied results several potential functions literature new insight gained examining criteria carefully conditions show boosting leave tremendous freedom choice potential function values less perhaps freedom used choose potential functions overly concentrate noisy examples still significant gap two sets properties still long way classifying arbitrary potential functions boosting properties classes leveraging algorithms one class looks distances successive distributions another class changes potential duf helmbom time criteria boosting may change significantly different approaches example freund recently presented boosting algorithm uses time varying sigmoidal potential would interesting adapt techniques dynamic potentials
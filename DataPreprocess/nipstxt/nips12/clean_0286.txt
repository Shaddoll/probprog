abstract important issue neural computing concerns description learning dynamics macroscopic dynamical variables recent progress line learning addresses often unrealistic case infinite training set introduce new framework model batch learning restricted sets examples widely applicable learning cost function fully taking account temporal correlations introduced recycling examples illustration analyze effects weight decay early stopping learning teacher generated examples introduction dynamics learning neural computing complex multi variate process interest macroscopic level thus describe process macroscopic dynamical variables recently much progress made modeling dynamics line learning independent example generated learning step since statistical correlations among examples ignored dynamics simply described instantaneous dynamical variables however studies line learning focus ideal case network access almost infinite training set whereas many applications collection training examples may costly restricted set examples introduces extra temporal correlations learning dynamics much complicated early studies briefly considered dynamics adaline learning recently extended linear percepttons learning nonlinear rules recent attempts using dynamical replica theory made study learning restricted sets examples far exact results published simple learning rules hebbian learning beyond appropriate approximations needed paper introduce new framework model batch learning restricted sets examples widely applicable learning rule minimizes arbitrary cost function gradient descent fully takes account temporal correlations learning therefore exact large networks statistical dynamics batch learning formulation consider single layer perceptron input nodes connecting single output node weights jj convenience assume inputs gaussian variables mean variance output state function activation output node network assigned learn examples map inputs outputs outputs generated teacher perceptron bj namely batch learning gradient descent achieved adjusting weights jj iteratively certain cost function terms student teacher activations minimized hence consider general cost function eg precise functional form depends adopted learning algorithm case binary outputs sgnx early studies learning dynamics considered adaline learning sgny recent studies hebbian learning xs ensure perceptton regularized learning customary introduce weight decay term furthermore avoid system trapped local minima noise often added dynamics hence gradient descent dynamics given djj dt eg ajj respectively represent first second partial derivatives respect weight decay strength noise term temperature ks cavity method theory dynamical version cavity method uses self consistency argument consider happens new example added training set central quantity method cavity activation activation new example perceptron trained without example since original network information new example cavity activation stochastic specifically denoting new example label cavity activation time ho large independently generated examples ho gaussian variable covariance given correlation function weights times ho li wong assumed independent distribution specified teacher student correlation given ho yo suppose perceptron incorporates new example batch mode learning step time activation new example subsequent time longer random variable furthermore activations original examples time also adjusted newcomer turn affect evolution activation example giving rise called onsager reaction effects makes dynamics complex fortunately large assume adjustment small perturbative analysis applied suppose weights original new perceptron time jj respectively perturbation yields jj xo yo first term right hand side describes primary effects adding example training set driving term difference two perceptrons second term describes secondary effects due changes original examples caused added example referred onsager reaction term one note difference cavity generic activations added example former denoted ho corresponds activation perceptron jj whereas latter denoted xo corresponding activation perceptron one used calculating gradient driving term since notations sufficiently distinct omitted superscript xo appears background examples equation solved green function technique yielding sj dsgjk go xo yo gin weight green function satisfying gjn dt exp bare green function step function weight green function describes effects example propagates weight jn learning time weight jj subsequent time including primary secondary effects hence temporal correlations taken account large equation solved diagrammatic approach similar weight green function self averaging distribution examples diagonal limn oo gj dt dt tl tl tl statistical dynamics batch learning example green function given vu dt du allows us express generic activations examples terms cavity counterparts multiplying sides summing get xo dsg equation interpreted follows time generic activation xo deviates cavity counterpart gradient term present batch learning step previous times gradient term propagates influence time via green function statistically equation enables us express activation distribution terms cavity activation distribution thereby getting macroscopic description dynamics solve green functions activation distributions need fluctuation response relation derived linear response theory dt dt finally teacher student correlation given dt solvable case cavity method applied dynamics learning arbitrary cost function applied hebb rule yields results identical exact results present results adaline rule illustrate features learning dynamics derivable study common learning rule bears resemblance common back propagation rule theoretically dynamics particularly convenient analysis since rendering weight green function time translation invariant case dynamics solved laplace transform monitor progress learning interested three performance measures training error et probability error training examples given et xsgny average taken joint distribution training set test error crest probability error inputs training examples corrupted additive gaussian noise variance relevant performance measure perceptron applied process data corrupted versions training data given crest xsgny acxf test error reduces training error generalization error probability error arbitrary input teacher student outputs compared given eg arccos cxf figure shows evolution generalization error weight decay strength varies steady state generalization error minimized optimum opt li wong independent interesting note cases linear perceptton optimal weight decay strength also independent determined output noise unlearnability examples similarly student provided coarse grained version teacher activation form binary bits opt generalization error non monotonic function learning time hence dynamics plagued overtraining desirable introduce early stopping improve perceptron performance similar behavior observed linear percepttons verify theoretical predictions simulations done using samples averaging shown fig agreement excellent figure compares generalization errors steady state early stopping point shows early stopping improves performance opt becomes near optimal compared best result opt hence early stopping speed learning process without significant sacrifice generalization ability however cannot outperform optimal result steadystate agrees recent empirical observation careful control weight decay may better early stopping optimizing generalization res te time weight decay figure evolution generalization error different weight decay strengths theory solid line simulation symbols comparing generalization error steady state oc early stopping point res search optimal learning algorithms important consideration environment performance tested besides generalization performance applications test examples inputs correlated training examples hence interested evolution test error given additive gaussian noise inputs figure shows optimal weight decay parameter opt minimizes test error furthermore weight decay weak early stopping desirable figure shows value optimal weight decay function input noise variance lowest order approximation opt sufficiently large dependence opt input noise rather general since also holds case random examples limit small opt vanishes whereas opt approaches nonzero constant hence statistical dynamics batch learning weight decay necessary training error optimized perceptton applied process increasingly noisy data weight decay becomes important performance enhancement figure also shows phase line kot overtraining occurs lowest order approximation aot cr sufficiently large however unlike case generalization error line onset overtraining coincide exactly line optimal weight decay particular intermediate range input noise optimal line lies region overtraining optimal performance attained tuning weight decay strength learning time however least present case computational results show improvement marginal time figure evolution test error different weight decay strengths opt respectively lines optimal weight decay onset overtraining inset data aot opt magnified versus conclusion based cavity method introduced new framework modeling dynamics learning applicable learning cost function making versatile theory takes full account temporal correlations generated use restricted set examples realistic many situations theories line learning infinite training set adaline rule solvable cavity method still relatively simple model approachable direct methods hence justification method general framework learning dynamics hinges applicability less trivial cases general constant du expanded series namical equations considered starting point perturbation theory results various limits derived limits small large large asymptotic limit another area useful application cavity method case batch learning large learning steps since shown recently learning converges steps dynamical equations remain simple enough meaningful study preliminary results along direction promising reported elsewhere li wong alternative general theory learning dynamics dynamical replica theory recently developed yields exact results hebbian learning approximate results non trivial cases based certain self averaging assumptions theory able approximate dynamics evolution single time functions expense solve set saddle point equations replica formalism every learning instant hand theory retains functions double arguments develops naturally stochastic nature cavity activations contrary suggestion cavity method also applied line learning restricted sets examples hoped adhering exact formalism cavity method provide fundamental insights studies extended sophisticated multilayer networks practical importance method enables us study effects weight decay early stopping shows optimal strength weight decay determined imprecision examples level input noise anticipated applications weaker weight decay generalization performance made near optimal early stopping furthermore depending performance measure optimality may attained combination weight decay early stopping though performance improvement marginal present case question remains open general context consider present work beginning depth study learning dynamics many interesting challenging issues remain explored acknowledgments thank coolen saad fruitful discussions nips work supported grant hkust research grant council hong kong
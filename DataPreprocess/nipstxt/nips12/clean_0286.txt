abstract import issu neural comput concern descript learn dynam macroscop dynam variabl recent progress line learn address often unrealist case infinit train set introduc new framework model batch learn restrict set exampl wide applic learn cost function fulli take account tempor correl introduc recycl exampl illustr analyz effect weight decay earli stop learn teacher gener exampl introduct dynam learn neural comput complex multi variat process interest macroscop level thu describ process macroscop dynam variabl recent much progress made model dynam line learn independ exampl gener learn step sinc statist correl among exampl ignor dynam simpli describ instantan dynam variabl howev studi line learn focu ideal case network access almost infinit train set wherea mani applic collect train exampl may costli restrict set exampl introduc extra tempor correl learn dynam much complic earli studi briefli consid dynam adalin learn recent extend linear perceptton learn nonlinear rule recent attempt use dynam replica theori made studi learn restrict set exampl far exact result publish simpl learn rule hebbian learn beyond appropri approxim need paper introduc new framework model batch learn restrict set exampl wide applic learn rule minim arbitrari cost function gradient descent fulli take account tempor correl learn therefor exact larg network statist dynam batch learn formul consid singl layer perceptron input node connect singl output node weight jj conveni assum input gaussian variabl mean varianc output state function activ output node network assign learn exampl map input output output gener teacher perceptron bj name batch learn gradient descent achiev adjust weight jj iter certain cost function term student teacher activ minim henc consid gener cost function eg precis function form depend adopt learn algorithm case binari output sgnx earli studi learn dynam consid adalin learn sgni recent studi hebbian learn xs ensur perceptton regular learn customari introduc weight decay term furthermor avoid system trap local minima nois often ad dynam henc gradient descent dynam given djj dt eg ajj respect repres first second partial deriv respect weight decay strength nois term temperatur ks caviti method theori dynam version caviti method use self consist argument consid happen new exampl ad train set central quantiti method caviti activ activ new exampl perceptron train without exampl sinc origin network inform new exampl caviti activ stochast specif denot new exampl label caviti activ time ho larg independ gener exampl ho gaussian variabl covari given correl function weight time ho li wong assum independ distribut specifi teacher student correl given ho yo suppos perceptron incorpor new exampl batch mode learn step time activ new exampl subsequ time longer random variabl furthermor activ origin exampl time also adjust newcom turn affect evolut activ exampl give rise call onsag reaction effect make dynam complex fortun larg assum adjust small perturb analysi appli suppos weight origin new perceptron time jj respect perturb yield jj xo yo first term right hand side describ primari effect ad exampl train set drive term differ two perceptron second term describ secondari effect due chang origin exampl caus ad exampl refer onsag reaction term one note differ caviti gener activ ad exampl former denot ho correspond activ perceptron jj wherea latter denot xo correspond activ perceptron one use calcul gradient drive term sinc notat suffici distinct omit superscript xo appear background exampl equat solv green function techniqu yield sj dsgjk go xo yo gin weight green function satisfi gjn dt exp bare green function step function weight green function describ effect exampl propag weight jn learn time weight jj subsequ time includ primari secondari effect henc tempor correl taken account larg equat solv diagrammat approach similar weight green function self averag distribut exampl diagon limn oo gj dt dt tl tl tl statist dynam batch learn exampl green function given vu dt du allow us express gener activ exampl term caviti counterpart multipli side sum get xo dsg equat interpret follow time gener activ xo deviat caviti counterpart gradient term present batch learn step previou time gradient term propag influenc time via green function statist equat enabl us express activ distribut term caviti activ distribut therebi get macroscop descript dynam solv green function activ distribut need fluctuat respons relat deriv linear respons theori dt dt final teacher student correl given dt solvabl case caviti method appli dynam learn arbitrari cost function appli hebb rule yield result ident exact result present result adalin rule illustr featur learn dynam deriv studi common learn rule bear resembl common back propag rule theoret dynam particularli conveni analysi sinc render weight green function time translat invari case dynam solv laplac transform monitor progress learn interest three perform measur train error et probabl error train exampl given et xsgni averag taken joint distribut train set test error crest probabl error input train exampl corrupt addit gaussian nois varianc relev perform measur perceptron appli process data corrupt version train data given crest xsgni acxf test error reduc train error gener error probabl error arbitrari input teacher student output compar given eg arcco cxf figur show evolut gener error weight decay strength vari steadi state gener error minim optimum opt li wong independ interest note case linear perceptton optim weight decay strength also independ determin output nois unlearn exampl similarli student provid coars grain version teacher activ form binari bit opt gener error non monoton function learn time henc dynam plagu overtrain desir introduc earli stop improv perceptron perform similar behavior observ linear perceptton verifi theoret predict simul done use sampl averag shown fig agreement excel figur compar gener error steadi state earli stop point show earli stop improv perform opt becom near optim compar best result opt henc earli stop speed learn process without signific sacrific gener abil howev cannot outperform optim result steadyst agre recent empir observ care control weight decay may better earli stop optim gener re te time weight decay figur evolut gener error differ weight decay strength theori solid line simul symbol compar gener error steadi state oc earli stop point re search optim learn algorithm import consider environ perform test besid gener perform applic test exampl input correl train exampl henc interest evolut test error given addit gaussian nois input figur show optim weight decay paramet opt minim test error furthermor weight decay weak earli stop desir figur show valu optim weight decay function input nois varianc lowest order approxim opt suffici larg depend opt input nois rather gener sinc also hold case random exampl limit small opt vanish wherea opt approach nonzero constant henc statist dynam batch learn weight decay necessari train error optim perceptton appli process increasingli noisi data weight decay becom import perform enhanc figur also show phase line kot overtrain occur lowest order approxim aot cr suffici larg howev unlik case gener error line onset overtrain coincid exactli line optim weight decay particular intermedi rang input nois optim line lie region overtrain optim perform attain tune weight decay strength learn time howev least present case comput result show improv margin time figur evolut test error differ weight decay strength opt respect line optim weight decay onset overtrain inset data aot opt magnifi versu conclus base caviti method introduc new framework model dynam learn applic learn cost function make versatil theori take full account tempor correl gener use restrict set exampl realist mani situat theori line learn infinit train set adalin rule solvabl caviti method still rel simpl model approach direct method henc justif method gener framework learn dynam hing applic less trivial case gener constant du expand seri namic equat consid start point perturb theori result variou limit deriv limit small larg larg asymptot limit anoth area use applic caviti method case batch learn larg learn step sinc shown recent learn converg step dynam equat remain simpl enough meaning studi preliminari result along direct promis report elsewher li wong altern gener theori learn dynam dynam replica theori recent develop yield exact result hebbian learn approxim result non trivial case base certain self averag assumpt theori abl approxim dynam evolut singl time function expens solv set saddl point equat replica formal everi learn instant hand theori retain function doubl argument develop natur stochast natur caviti activ contrari suggest caviti method also appli line learn restrict set exampl hope adher exact formal caviti method provid fundament insight studi extend sophist multilay network practic import method enabl us studi effect weight decay earli stop show optim strength weight decay determin imprecis exampl level input nois anticip applic weaker weight decay gener perform made near optim earli stop furthermor depend perform measur optim may attain combin weight decay earli stop though perform improv margin present case question remain open gener context consid present work begin depth studi learn dynam mani interest challeng issu remain explor acknowledg thank coolen saad fruit discuss nip work support grant hkust research grant council hong kong
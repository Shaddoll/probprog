abstract thinking state hidden markov model corresponding spatial region fictitious topology space possible naturally define neighbouring states connected space transition matrix constrained allow transitions neighbours means valid state sequences correspond connected paths topology space show constrained hmms learn discover underlying structure complex sequences high dimensional data apply problem recovering mouth movements acoustics continuous speech latent variable models structured sequence data structured time series generated systems whose underlying state variables change continuous way whose state output mappings highly nonlinear many one smooth probabilistic unsupervised learning sequences requires models two essential features latent hidden variables topology variables hidden markov models hmms thought dynamic generalizations discrete state static data models gaussian mixtures discrete state versions linear dynamical systems ldss dynamic generalizations continuous latent variable models factor analysis hmms ld provide probabilistic latent variable models time series important limitations traditional hmms powerful model relationship underlying state associated observations state stores private distribution output variables means change hidden state cause arbitrarily complex changes output distribution however extremely difficult capture reasonable dynamics discrete latent variable principle state reachable state time step next state depends current state ldss hand extremely impoverished representation outputs function latent variables since transformation restricted global linear somewhat easier capture state dynamics since state multidimensional vector continuous variables matrix flow acting enforces continuity latent variables across time constrained hidden markov models address modeling state dynamics building topology hidden state representation essential idea constrain transition parameters conventional hmm discretevalued hidden state evolves structured way particular consider parameter restrictions constrain state evolve discretized version continuous multivariate variable inscribes connected paths space lends physical interpretation discrete state trajectories hmm standard trick traditional speech applications hmms use eft right transition matrices special case type constraints investigated paper however eftto right bakis hmms force state trajectories inherently one dimensional uni directional whereas also consider higher dimensional topology free omni directional motion constrained hidden markov models illustrative game consider playing following game divide sheet paper several contiguous nonoverlapping regions cover entirely region inscribe symbol allowing symbols repeated different regions place pencil sheet move around reading order symbols regions passes add noise observation process fraction time incorrect symbols reported list instead correct ones game reconstruct configuration regions sheet ordered list noisy symbols course absolute scale rotation reflection sheet never recovered learning essential topology may possible figure illustrates setup true generafive map iteration loglikelihood figure left true map generates symbol sequences random movement connected cells centre example noisy output sequence noisy symbols circled right learned map training sequences noise probability symbols long cell actually contains entire distribution observed symbols though case upper right cell significant probability mass one symbol see figure display details without noise repeated symbols game easy non probabilistic methods solve presence one way mitigating noise problem statistical averaging example one could attempt use average separation time pair symbols define dissimilarity would possible use methods like multi dimensional scaling sort kohonen mapping though time explicitly construct configuration points obeying distance relations however methods still cannot deal many one state output mappings repeated numbers sheet nature assign unique spatial location symbol playing game analogous unsupervised learning structured sequences game also played continuous outputs although often high dimensional data effectively clustered around manageable number prototypes thus vector time series converted sequence symbols constrained hmms incorporate latent variables topology yet retain powerful nonlinear output mappings deal difficulties noise many one mappings mentioned win game see figs key insight game generates sequences exactly according hidden markov process whose transition matrix allows transitions neighbouring cells whose output distributions probability single symbol small amount symbols account noise observed symbol sequence must informative enough reveal map structure quantified using idea persistent excitation control theory consider network units compete explain input data points unit position output space well position lower dimensional topology space winning unit position output space updated towards data point also recent time winners positions topology space updated towards topology space location current winner rule works well yields topological maps nearby units code data typically occur close together time however cannot learn many one maps one unit different topology locations similar outputs rowels model definition state topologies cell packings defining constrained hmm involves identifying state underlying hidden markov chain spatial cell fictitious topology space requires selecting dimensionality topology space choosing packing hexagonal cubic fills space number cells packing equal number states original markov model cells taken equal size since scale topology space completely arbitrary unit volume thus packing covers volume topology space side length roughly dimensionality packing together define vector valued function rn rn gives location cell rn packing example cubic packing dimensional space defines rn mod state rn markov model assigned cell rn packing thus giving location topology space finally must choose neighbourhood rule topology space defines neighbours cell example connected cells face neighbours within certain radius cubic packings connected neighbours face neighbours dimensional topology space neighbourhood rule also defines boundary conditions space periodic boundary conditions would make cells opposite extreme faces space neighbours transition matrix hmm preprogrammed allow transitions neighbours transition probabilities set zero making transition matrix sparse set permitted transitions equally likely valid state sequences underlying markov model represent connected city block paths topology space figure illustrates three dimensional model figure left physical depiction topology space constrained hmm showing example state trajectory right corresponding transition matrix structure state hmm computed using face centred cubic packing gaps inner bands due edge effects state inference learning constrained hmm exactly inference procedures regular hmm forward backward algorithm computing state occupation probabilities terbi decoder finding single best state sequence discrete state inferences performed transformed using state position function rn yield probability distributions topology space case forward backward paths topology space case viterbi decoding transformation makes outputs state decodings constrained hmms comparable outputs inference procedures continuous state dynamical systems kalman smoothing learning procedure constrained hmms also almost identical hmms particular em algorithm baum welch used update model parameters crucial difference transition probabilities precomputed topology packing never updated learning fact makes learning much easier cases transition probabilities learned structure constrains hidden state sequences way make learning output parameters much efficient underlying data really come spatially structured generative model figure shows example parameter learning game discussed notice case part state space single output except noise final learned output distributions became essentially minimum entropy constrained hmms principle model stochastic multimodal output processes since state stores entire private distribution outputs constrained hidden markov models random imt nliz taon ioglikellhood figure snapshots model parameters constrained hmm learning game described section every iteration cell map complete distribution observed symbols top three symbols cell histogram show font size proportional square root probability make ink roughly proportional map trained noisy sequences symbols long generated map left figure using noise probability final map convergence iterations shown right figure recovery mouth movements speech audio applied constrained hmm approach described problem recovering mouth movements acoustic waveform human speech data containing simultaneous audio articulator movement information obtained university wisconsin ray microbeam database eight separate points four tongue one lip two jaw located midsaggital plane speaker head tracked subjects read various words sentences paragraphs lists numbers coordinates within lmm point sampled hz xray system located gold beads attached feature points mouth producing dimensional vector every ms audio sampled khz roughly bits amplitude resolution presence machine noise data well suited constrained hmm architecture come system whose state variables known physical constraints move connected paths low degree freedom space words normally hidden articulators movable structures mouth whose positions represent underlying state speech production systemfi move slowly smoothly observed speech signal system output characterized sequence short time spectral feature vectors often known spectrogram experiments reported characterized audio signal using line spectral frequencies lsfs measured every ms coincide articulatory sampling rate ms window lsf vectors characterize spectral shape speech waveform short time energy average energy also ms window every ms measured separate one dimensional signal unlike movements articulators audio spectrum energy exhibit quite abrupt changes indicating mapping articulator positions spectral shape smooth furthermore mapping many one different articulator configurations produce similar spectra see unsupervised learning task explain complicated sequences observed spectral features lsfs energies outputs system low dimensional state vector changes slowly smoothly words learn parameters constrained hmm connected paths topology space state space generate acoustic training data high likelihood unsupervised learning task performed show relate learned trajectories topology space true measured articulator movements articulator positions provide complete state information example excitation signal voiced unvoiced captured bead locations however provide much important information state information easily accessible directly acoustics smodel structure dimensionality number states currently set using cross validation rowe many models speech production process predict many one nonsmooth properties articulatory acoustic mapping useful confirm features looking real data figure shows experimentally observed distribution articulator configurations used produce similar sounds computed follows acoustic articulatory data single speaker collected together starting sample called key sample find samples nearest key two measures articulatory distance defined using mahalanobis norm two position vectors global covariance positions appropriate speaker spectral shape distance defined using mahalanobis norm two line spectral frequency vectors using global lsf covariance speaker audio data words find samples look like key sample mouth shape sound like key sample spectral shape plot tongue bead positions key sample thick cross nearest samples mouth shape thick ellipse spectral shape dots points primary interest dots show distribution tongue positions used generate similar sounds thick ellipses shown control ensure many nearby points key sample exist dataset spread multimodality dots indicates many different articulatory configurations used generate sound tongue lip mm ay mml tongue bodyl ram tongue bodyl ram oi io tongue mm mm tongue body mini ransue aor mini figure inverse mapping acoustics articulation ill posed real speech production data group four articulator space plots shows samples nearest one key sample thick cross dots nearest samples using acoustic measure based line spectral frequencies spread multimodality dots indicates many different articulatory configurations used generate similar sounds positions four tongue beads plotted two examples different key samples shown one left group four panels another right group thick ellipses shown control two standard deviation contour nearest samples using articulatory position distance metric direct supervised learning short time spectral features lsfs articulator positions ill posed nature inverse problem shown figure makes impossible illustrate difficulty attempted recover articulator positions acoustic feature vectors using kalman smoothing lds case since access hidden states articulator positions system outputs lsfs compute optimal parameters model directly particular state transition matrix obtained regression articulator positions velocities time onto positions time output matrix regression articulator positions velocities onto lsf vectors noise covariances residuals regressions figure shows results smoothing recovery quite poor constrained hmms applied recovery problem previously reported earlier results used small subset database continuous speech provide hard experimental verification fig many one problem constrained hidden markov models figure recovered articulator movements using state inference constrained hmm four dimensional model states trained data beads single speaker including test utterance shown dots show actual measured articulator movements single bead coordinate versus time thin lines estimated movements corresponding acoustics unsuccessful recovery articulator movements using kalman smoothing global lds model speaker dependent parameters underlying linear dynamical system known set optimal values using true movement information training data furthermore example test utterance shown included training data used estimate model parameters bead coordinates vertical axes scale bead names shown left horizontal movements plotted left hand column vertical movements right hand column separation two horizontal lines near centre right panel indicates machine measurement error recovery tongue tip vertical motion acoustics time sec kalman smoothing optimal linear dynamical system time sec horizontal movements vertical movements basic idea train unsupervised sequences acoustic spectral features map topology space state trajectories onto measured articulatory movements figure shows movement recovery using state inference four dimensional model states trained data beads single speaker naive unsupervised learning runs severe local minima problems avoid simulations shown models trained slowly annealing two learning parameters term used place zeros sparse transition matrix used place mt observations inference state occupation probabilities inverse temperature raised infer continuous state trajectory utterance learning first viterbi decoding acoustics generate discrete state sequence mt interpolate smoothly positions mr state easier way used previously find good minima initialize models using articulatory data provide impressive structure discovery annealing still yields system capable inverting acoustics articulatory movements previously unseen test data first constrained hmm trained articulatory movements works easily natural geometric physical constraints next take distribution acoustic features lsfs times training data viterbi decoding places model particular state use lsf distributions initialize equivalent acoustic constrained hmm new model retrained convergence using baum welch roweis unsupervised learning single linear fit performed continuous state trajectories actual articulator movements training data model cannot discover units system axes used represent articulatory data recover articulator movements previously unseen test utterance infer continuous state trajectory apply single linear mapping learned training data conclusions extensions work enforcing simple constraint transition parameters standard hmm link forged discrete state dynamics motion real valued state vector continuous space complex time series generated systems whose underlying latent variables fact change slowly smoothly constrained hmms provide powerful unsupervised learning paradigm model state output mappings highly nonlinear many one smooth furthermore rely well understood learning inference procedures come convergence guarantees results synthetic real data show models successfully capture lowdimensional structure present complex vector time series particular shown speaker dependent constrained hmm accurately recover articulator movements continuous speech within measurement error data acoustic articulatory inversion problem long history speech processing see references therein many previous approaches attempted exploit smoothness articulatory movements inversion modeling hogden et al provided early inspiration ideas address many one problem simon blackburn investigated forward mapping articulation acoustics explicitly attempt inversion early work waterloo suggested similar constraints improving speech recognition systems look real articulatory data recent work rutgers developed similar system much good success perpififin considers related problem sequence learning using epg speech data example note described diffusion type dynamics transitions neighbours equally likely also possible consider directed flows give certain neighbours state lower zero probability left right hmms mentioned earlier example one dimensional topologies higher dimensions flows derived discretization matrix linear dynamics physical structural constraints also possible many connected local flow regimes either diffusive directed rather one global regime discussed gives rise mixtures constrained hmms block structured rather banded transition matrices smyth considered models case one dimensional topologies directed flows applied learning character sequences english text another application investigated map learning multiple sensor readings explorer robot navigates unknown environment records time many local measurements altitude pressure temperature humidity etc wish reconstruct sequences readings topographic maps sensor variable area well trajectory explorer final application tracking inferring movements articulated bodies using video measurements feature positions
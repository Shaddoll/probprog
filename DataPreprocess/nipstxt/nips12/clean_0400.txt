abstract samy bengio idiap cp rue du simplon martigny switzerland bengiooidiap ch curse dimensionality severe modeling high dimensional discrete data number possible combinations variables explodes exponentially paper propose new architecture modeling high dimensional data requires resources parameters computations grow square number variables using multi layer neural network represent joint distribution variables product conditional distributions neural network interpreted graphical model without hidden random variables conditional distributions tied hidden units connectivity neural network pruned using dependency tests variables experiments modeling distribution several discrete data sets show statistically significant improvements methods naive bayes comparable bayesian networks show significant improvements obtained pruning network introduction curse dimensionality hits particularly hard models high dimensional discrete data many possible combinations values variables possibly observed data set even large data sets common datamining applications paper dealing particular multivariate discrete data one tries build model distribution data used example detect anomalous cases data mining applications used model class conditional distribution observed variables order build classifier simple multinomial maximum likelihood model would give zero probability combinations encountered training set would likely give zero probability sample test cases smoothing model assigning non zero probability unobserved cases would satisfactory either would provide much generalization training set could obtained using multivariate multinomial model whose parameters estimated maximum posteriori map principle greatest probability given training data using diffuse prior dirichlet parameters graphical model bayesian network represents joint distribution random variables zn zn zilearentsi part work done cirano montreal qc canada modeling high dimensional discrete data neural networks parents set random variables called parents variable graphical model directly condition zi arrow drawn graphical model zi parents fully connected left right graphical model illustrated figure left corresponds model zi zn zilzl zi figure left fully connected qeft right graphical model right architecture neural network simulates fully connected left right graphical model observed values zi zi encoded corresponding input unit group hi group hidden units li group output units depend zi representing parameters distribution zi conditional probabilities ziiz zi multiplied obtain joint distribution note representation depends ordering variables previous variables order taken parents call combination values parentsi context exact model full table possible contexts orders equivalent approximations used different predictions could made different models assuming different orders graphical models curse dimensionality shows representation conditional distributions lparentsi zi many parents zj parents take rtj values ij rtj different contexts occur one would like estimate distribution zi serious problem addressed past two types approaches sometimes combined modeling dependencies variables approach mainly taken graphical models bayes networks set independencies assumed using priori human expert knowledge learned data see also set parentsi restricted one element chosen maximize correlation approximating mathematical form joint distribution form takes account dependencies lower order takes account possible dependencies rademacher walsh expansion multi binomial low order polynomial approximation full joint binomial distribution used experiments reported paper approach putting forward paper mostly second category although using simple non parametric statistics dependency pairs variables reduce number required parameters multi binomial model joint distribution set binary variables approximated polynomial whereas exact representation zn zn function zn polynomial degree rt approximated lower bengio bengio degree polynomial approximation easily computed using rademacherwalsh expansion similar expansions bahadur lazarsfeld expansion therefore instead parameters approximated model zn requires parameters typically order used model proposed also requires parameters allows model dependencies tuples variables variables time previous related work frey fully connected graphical model used see figure left conditional distributions represented logistic take account first order dependency variables zi zi exp wjzj paper basically extend frey idea using neural network hidden layer particular architecture allowing multinomial continuous variables propose prune network weights frey named model logistic autoregressive bayesian network larc argues prior variances logistic weights correspond inverse weight decays chosen inversely proportional number conditioning variables number inputs particular output neuron model tested task learning classify digits binary pixel images models different orderings variables compared yield significant differences performance averaging predictive probabilities different models obtained considering different random orderings frey obtained small improvements likelihood classification model performed better equivalently models tested cart naive bayes nearest neighbors various bayesian models hidden variables helmholtz machines results impressive taking account simplicity larc model proposed architecture proposed architecture neural network implementation graphical model variables observed training set hidden units playing significant role share parameters across different conditional distributions figure right illustrates model simpler case fully connected left right graphical model figure left neural network represents parametrized function fo zl zn log po zl zn zn approximating joint distribution variables parameters weights neural network architecture three layers layer organized groups associated variables log probability computed sum conditional log probabilities fo zl zn log zi zi gi zi gi zi vector valued output th group output units gives value parameters distribution zi zi zi example ordinary discrete case gi may vector probabilities associated possible values multinomial random variable zi case example softmax output th group may used force parameters positive sum egi gi modeling high dimensional discrete data neural networks linear combinations hidden units outputs ranging number elements parameter vector associated distribution zi fixed value zi guarantee functions gi zl zi depend zl zi zi zn connectivity struture hidden units must constrained follows arc biases arc weights output layer hj output th unit raj units th group hidden layer nodes may bc computed follows nk hj tanh cj vj zk biases weights hidden layer zk th element vectorial input representation value example binary case zi used one input node zi binomial zi zi multinomial case use one hot encoding zi zi zi otherwise input layer groups value zn zn used input hidden layer also groups corresponding variables since zx represented unconditionally first output group corresponding group need hidden units inputs biases discussion number free parameters model maxi mj maximum number hidden units per hidden group associated one variables basically quadratic number variables like multi binomial approximation uses polynomial expansion joint distribution however increased representation theorems neural networks suggest able approximate arbitrary precision true joint distribution course true limiting factor amount data tuned according amount data experiments used cross validation choose value mj hidden groups sense neural network representation zn polynomial expansions multi binomial ordinary multilayer neural networks function approximation polynomial function approximators allows capture high order dependencies number hidden units controls many dependencies captured data chooses actual dependencies useful maximizing likelihood unlike bayesian networks hidden random variables learning proposed architecture simple even conditional independencies optimize parameters simply used gradient based optimization methods either using conjugate stochastic line gradient maximize total log likelihood sum values eq training examples prior parameters incorporated cost function map estimator obtained easily maximizing total log likelihood plus log prior parameters experiments used weight decay penalty inspired analysis frey penalty proportional number weights incoming neuron bengio bengio however clear distribution could generally marginalized except summing possibly many combinations values variables integrated another related question whether one could deal missing values total number values missing variables take reasonably small one sum values order obtain marginal probability maximize probability variables systematically missing values put end variable ordering case easy compute marginal distribution taking product output probabilities missing variables similarly one easily compute predictive distribution last variable given first variables framework easily extended hybrid models involving continuous discrete variables case continuous variables one choose parametric form distribution continuous variable parents conditioning context fixed example one could use normal log normal mixture normals instead softmax outputs th output group would compute parameters continuous distribution mean log variance another type extension allows build conditional distribution model znlx xm one adds extra input units represent values conditioning variables xm finally architectural extension implemented allow direct input tooutput connections still following rules ordering allow depend zi zi therefore case number hidden units obtain larc model proposed frey choice topology another type extension model found useful experiments allow user choose topology fully connected left right experiments used non parametric tests heuristically eliminate connections network one could also use expert prior knowledge regular graphical models order cut number free parameters experiments used pairwise test statistical dependency kolmogorov smirnov statistic works continuous discrete variables statistic variables sup xi yi xi yi number examples empirical distribution obtained counting training data ranked pairs according value statistic chosen pairs value statistic threshold value chosen cross validation pairs zi zj chosen part model assuming without loss generality pairs connections kept network addition th hidden group th output group hidden group output group input group hidden group every zj pair experiments experiments compared following models naive bayes likelihood obtained product multinomials one per variable multinomial smoothed dirichlet prior multi binomial using rademacher walsh expansion order since handles case binary data applied dna data set simple graphical model pairs variables variable ordering selected neural network conditional distribution modeled modeling high dimensional discrete data neural networks separate multinomial conditioning context works number conditioning variables small mushroom audiology soybean experiments reduce number conditioning variables following order given tests multinomials also smoothed dirichlet prior neural network architecture described without hidden units larc without pruning fold cross validation used select number hidden units per hidden group weight decay neural network larc cross validation also used choose amount pruning neural network larc amount smoothing dirichlet priors multinomials naive bayes model simple graphical model results four data sets obtained web uc machine learning statlog databases meant classification tasks instead ignored classification used data learn probabilistic model input features dna statlog binary features cases used training cross validation testing mushroom uci discrete features taking values cases used training cross validation testing audiology uci discrete features taking values cases used training testing original train test partition concatenated split data obtain significant test figures soybean uci discrete features taking values cases used training testing table clearly shows proposed model yields promising results since pruned neural network superior models cases pairwise differences models statistically significant cases except audiology difference network without hidden units larc significant conclusion paper proposed new application multi layer neural networks modelization high dimensional distributions particular discrete data model could also applied continuous mixed discrete continuous data like polynomial expansions previously proposed handling high dimensional distributions model approximates joint distribution reasonable number free parameters unlike allows capture high order dependencies even number parameters small model also seen extension previously proposed auto regressive logistic bayesian network using hidden units capture high order dependencies experimental results four data sets many discrete variables encouraging comparisons made naive bayes model multi binomial expansion larc model simple graphical model showing neural network significantly better terms sample log likelihood cases approach pruning neural network used experiments based pairwise statistical dependency tests highly heuristic better results might obtained using approaches take account higher order dependencies selecting conditioning variables methods based pruning fully connected network weight elimination penalty also tried also tried optimize bengio bengio dna mushroom mean stdev value mean stdev value naive bayes le multi binomial order le ordinary graph model le le larc le pruned larc le le full conn neural net le le pruned neural network audiology soybean mean stdev value mean stdev value naive bayes le le multi binomial order ordinary graph model le larc le pruned larc le full conn neural net le le pruned neural network table average sample negative log likelihood obtained various models four data sets standard deviations average parenthesis value test null hypotheses model true generalization error pruned neural network pruned neural network better models cases pair wise difference always statistically significant except respect pruned larc audiology order variables combine different networks obtained different orders like
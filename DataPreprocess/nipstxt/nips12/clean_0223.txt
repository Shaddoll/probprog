abstract give necessary sufficient conditions uniqueness support vector solution problems pattern recognition regression estimation general class cost functions show solution unique support vectors necessarily bound give simple examples non unique solutions note uniqueness primal dual solution necessarily imply uniqueness dual primal solution show compute threshold solution unique support vectors bound case usual method determining work introduction support vector machines svms attracted wide interest means implement structural risk minimization problems classification regression estimation fact training svm amounts solving convex quadratic programming problem means solution found global unique set global solutions convex furthermore objective function strictly convex solution guaranteed unique quadratic programming problems convexity objective function equivalent positive semi definiteness hessian strict convexity positive definiteness reference summarize basic uniqueness result following theorem proof found theorem solution convex programming problem objective function strictly convex unique positive definiteness hessian implies strict convexity objective function note general strict convexity objective function neccesarily imply positive definiteness hessian furthermore solution still unique even objective function loosely convex use term loosely convex mean convex strictly convex thus question uniqueness xthis contrast case neural nets local minima objective function occur burges crisp convex programming problem objective function loosely convex one must examined case case basis paper give necessary sufficient conditions support vector solution unique even objective function loosely convex clasification regression cases general class cost function one central features support vector method implicit mapping data rn feature space accomplished replacing dot products data points zi zj wherever occur train test algorithms symmetric function zi zj inner product zi zj zi zj xi xj denote mapped points order hold kernel function must satisfy mercer positivity condition algorithms amount constructing optimal separating hyperplane pattern recognition case fitting data linear regression tube suitable choice loss function regression estimation case without loss generality work space whose dimension denote dr conditions find non uniqueness solution depend explicitly approaches solving support vector training problem employ wolfe dual describe uniqueness primal dual solution mean uniqueness set primal dual variables solution notice strict convexity primal objective function imply strict convexity dual objective function example optimal hyperplane problem problem finding maximal separating hyperplane input space case separable data primal objective function strictly convex dual objective function loosely convex whenever number training points exceeds dimension data input space case dual hessian necessarily positive semidefinite since submatrix cases cost function also contributes block diagonal hessian gram matrix training data rows matrix necessarily linearly dependent cases support vector pattern recognition regression estimation studied one four cases occur primal dual solutions unique primal solution unique dual solution dual unique primal solutions unique case occurs unique primal solution one expansion terms dual variables give example case easy construct trivial examples case holds based discussion clear construct examples however since geometrical motivation interpretation svms rests primal variables theorems given address uniqueness primal solution case pattern recognition consider slightly generalized form problem given namely minimize objective function ci arecall gram matrix matrix whose ij th element form xi xj inner product xi element vector space rank gram matrix maximum number linearly independent vectors xi appear adue space constraints proofs details omitted complete details given elsewhere uniqueness svm solution constants cw ci subject constraints yi xi vector weights scalar threshold positive slack variables introduced handle case nonseparable data yi polarities training samples yi xi images training samples space mapping ci determine much errors penalized allowed pattern penalty index labels training patterns goal find values primal variables solve problem workers choose since results particularly simple dual formulation problem convex go details support vector classification algorithms refer interested reader note solution determined karush kuhn tucker kkt conditions see include definition solution convenience note theorem gives immediate proof solution optimal hyperplane problem unique since objective function strictly convex constraints eq variables removed linear inequality constraints therefore define convex set discussion need dual formulation problem case takes following form minimize eij otiotjyiyj xi xj oti subject constraints alp ci solution takes form otiyixi kkt conditions satisfied solution rli oq yi xi lagrange multipliers enforce positivity lagrange multipliers enforce constraint implicitly encapsulated condition ci retain emphasize equations imply whenever must ci note given solution support vector defined point xi suppose solution problem let af denote set yi xi af set yi xi jv set yi xi af set yi xi jv set yi xi af set yi xi following theorem theorem solution soft margin problem unique solution unique least one following two conditions holds furthermore whenever solution unique solutions share support vector xi lagrange multiplier satisfying cq ci course new result see example burges crisp holds afa contains support vectors holds iv contains support vectors proof case objective function strictly convex since sum strictly convex functions strictly convex function since function strictly convex furthermore constraints define convex set since set simultaneous linear inequality constraints defines convex set hence theorem solution unique case define dr component vector zi wi df zi terms variables problem still convex programming problem hence property solution global solution suppose two solutions form family solutions zt zt zx tz since solutions global zt expanding zt terms differentiating twice respect find wx given completely determined kkt conditions thus solution unique unique define min miniear miniear xi suppose condition holds different solution given vi aft vi iv uaf since construction remains constraints satisfied primed variables similarly suppose condition holds define min minie minie xi different solution given byw since construction unchanged constraints still met thus given conditions sufficient solution nonunique show necessity assume solution unique argument solutions must differ values given particular solution suppose also solution since set solutions convex also correspond solution given use kkt conditions compute choose sufficiently small iv previously zero becomes nonzero find order remain condition must hold solution similar reasoning shows condition must hold show final statement theorem use equality constraint together fact kkt conditions support vectors xi indices af ujv satisfy ai ci substituting gives ai ci ai implies result since ai non negative similarly substituting gives fa el cq implies result cl corollary solution unique letting denote set indices corresponding set support vectors must ciyi furthermore number data points finite least one family solutions support vectors corresponding note follows corollary ci chosen exists subset train data ciyi solution guaranteed unique even ifp furthermore done choosing ci close central value although resulting solution depend sensitively values chosen see example immediately finally note ci equal theorem shows necessary condition solution non unique negative positive polarity support vectors equal number uniqueness svm solution simple example non unique solution case given train set one dimension two examples xl yx straightforward show analytically solution unique ix margin equal family solutions corresponds margin thecasec case section dual unique primal since dual variables uniquely specified note also family solutions also satisfies condition solution smoothly deformable another solution cx solution becomes unique quite different unique solution found equal one interpret happens terms mechanical analogy central separating hyperplane sliding away point exerts higher force point lies edge margin region note solution unique possible values fall interval real line case suitable choice would one minimizes estimate bayes error svm output densities modeled using validation set alternatively requiring continuity cases one would choose value would result considering family solutions generated different choices taking limit would result unique solution case regression estimation one set pairs xl xi yi goal estimate unknown functional dependence function assumed related measurements xi yi yi xi hi ni represents noise details refer reader generalize original formulation follows choice positive error penalties el positive el minimize ilwll constant subject constr nts yi xi ei xi yi ei adopted notation formulation results insensitive loss function penalty sociated point xi yi xi bl el let fl fl lagrange multipliers introduced enforce constraints dual gives margin defined distance two hyperplanes corresponding equality eq namely wll margin region defined set points two hyperplanes method used estimate similar circumstances notation section coincides used section convenient burges crisp need formulation following theorem given solution define xi yi yi xi define set indices xi yi el iv set xi yi el jv set xi yi el jv set xi yi el solution unique unique least one following two conditions holds ie furthermore whenever solution unique solutions share support vectors bound either ci holds jv contains support vectors holds iv contains support vectors theorem shows non unique case one able move tube get another solution one change normal trivial example non unique solution data fits inside tube room spare case solutions normal tubes always lies along direction another example ci equal data falls outside tube number points tube computing svs bound threshold eqs usually determined subset constraint equations become equalities solution corresponding lagrange multipliers bound however may solution subset empty section consider situation solution unique solved optimization problem therefore know values lagrange multipliers hence know also wish find unique value solution since known fixed find finding value minimizes cost term primal lagrangian satisfies constraint equations let us consider pattern recognition case first let denote set indices positive negative polarity support vectors also let denote set indices positive negative vectors support vectors straightforward show ci ie ci max maxims xi maxi xi ci ci min mini xi mini xi furthermore ci ci solution unique two values coincide regression case let us denote set indices support vectors complement set indices ci set indices ct note ies ct ies desired value max maxies yi xi el maxie yi xi el ies ci min mini yi xi ei mini yi xi ei srecall ei uniqueness svm solution solution unique also ies ies ci two values coincide discussion shown non uniqueness svm solution exception rather rule occur one rigidly parallel transport margin region without changing total cost non unique solutions encountered techniques finding threshold minimizing bayes error arising model svm posteriors needed method proof theorems straightforward extendable similar algorithms example mangasarian generalized svm fact one extend result problem whose objective function consists sum strictly convex loosely convex functions example follows immediately case svm pattern recognition regression estimation algorithms arbitrary convex costs value normal always unique acknowledgments burges wishes thank keasler lawrence nohl lucent technologies support
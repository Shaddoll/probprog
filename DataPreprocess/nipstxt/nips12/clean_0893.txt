abstract paper examines application reinforcement learning wireless communication problem problem requires channel utility maximized simultaneously minimizing battery usage present solution multi criteria problem able significantly reduce power consumption solution uses variable discount factor capture effects battery usage introduction reinforcement learning rl applied resource allocation problems telecommunications channel allocation wireless systems network routing admission control telecommunication networks demonstrated reinforcement learning find good policies significantly increase application reward within dynamics telecommunication problems however key issue treat commonly occurring multiple reward constraint criteria consistent way paper focus power management wireless packet communication channels channels unlike wireline channels channel quality poor varies time often one side wireless link battery operated device laptop computer environment power management decides transmit receive simultaneously maximize channel utility battery life number power management strategies developed different aspects battery operated computer systems hard disk cpu managing channel different control actions shutting wireless transmitter make state channel side communication unobservable paper consider problem finding power management policy simultaneously maximizes radio communication earned revenue minimizing battery usage problem recast stochastic shortest path problem turn mapped discounted infinite horizon variable discount factor results show significant reductions power usage brown mobile application radio channel radio application figure five components radio communication system problem description problem comprised five components shown figure mobile application mobile radio wireless channel base station radio base station application applications end generate packets sent via radio across channel radio application side application also defines utility given end end performance radios implement simple acknowledgment retransmit protocol reliable transmission base station fixed reliable power supply therefore power constrained mobile power limited battery choose turn radio periods time reduce power usage note even radio mobile system continues draw power uses channel adds errors packets rate errors depends many factors location mobile base station intervening distance levels interference problem requires models components concrete specific models used paper described following sections emphasized order focus machine learning issues simple models chosen sophisticated models readily included channel channel carries fixed size packets synchronous time slots packet rates normalized channel rate channel carries one packet per unit time direction forward reverse channels orthogonal interfere wireless data channels typically low error rates occasionally due interference signal fading channel introduces many errors variation possible even mobile base station stationary channel modeled two state gilbert elliot model model channel either good bad state packet error probabilities pg pb pg pb channel symmetric loss rate directions channel stays state geometrically distributed holding time mean holding times ha hb time slots mobile base station application traffic generated source bursty model alternates generating packets generating packets rate ton holding times geometrically distributed mean holding times hon tof traffic direction independent identically distributed radios radios transmit data application send channel simultaneously receive data radio pass application radios implement simple packet protocol ensure reliability packets sources queued radio sent one one packets consist header data header carries acknowledgements ack recent packet received without error header contains checksum errors payload detected errored packets low power qreless communication via reinforcement learning parameter name symbol value channel error rate good pg channel error rate bad pb channel holding time good hg channel holding time bad hb source rate ton source holding time ton source holding time hof power radio poff power radio pon power radio transmitting ptx real time max delay ax web browsing time scale table application parameters cause receiving radio send packet negative acknowledgment nack radio instructing retransmit packet sequence starting errored packet nack sent immediately even data waiting radio must send empty packet unerrored packets sent application header assumed always received without error since mobile constrained power mobile considered master base station slave base station always ready transmit receive mobile turn radio conserve power every transition generates packet message header indicating change state base station message packets carry data mobile expends power three levels poff pon ptx corresponding radio receiver packet transmitted receiver packet transmitted reward criteria reward earned packets passed direction amount depends application paper consider three types applications mail application real time application web browsing application mail application unit reward given every packet received application real time application unit reward given every packet received application delay less reward zero otherwise web browsing application time important critical value packet delay desired time scale arrivals specific parameters used experiment given table gathered typical values emphasized model simplest model captures essential characteristics problem realistic channels protocols applications rewards readily incorporated paper left clarity packet error rate implies bit error rate less error correcting codes header easily reduce error rate low value main intent simplify protocol paper time outs mechanisms need considered brown component states channel good bad application mobile mobile list waiting unacknowledged packets current delay base station list waiting unacknowledged packets current delay table components system state markov decision processes given time slot system particular configuration defined state components table system state include time order facilitate accounting battery mobile choose toggle radio state rewards generated successfully received packets task learner determine radio policy maximizes total reward packets received batteries run battery life fixed time first depends usage second given drain capacity depends long battery charged long sat since charged age battery etc short battery runs random time system modeled stochastic shortest path problem whereby exists terminal state corresponds battery empty reward possible system remains permanently cost multi criteria objective formally goal learn policy possible system state maximize expectation possible trajectories starting state using policy reward packets received time last time step batteries run typically large inhibits fast learning order promote faster learning convert problem discounted problem removes variance caused random stopping times time given action state terminal state reached probability ps setting value terminal state convert new criterion maximize ps product probability reaching time words future rewards discounted ps discounting larger actions drain batteries faster thus power efficient strategy discount factor closer one correctly extends effective horizon reward captured learning rl methods solve mdp problems learning good approximations optimal value function given solution bellman optimality equation takes low power qreless communication via reinforcement learning following form max st set actions available current state effective immediate payoff expectation possible next states learn appr ximation using watkin learning algorithm bellman equation rewritten factor max every time step following decision made value turning next state compared value turning next state turning higher value mobile turns else mobile turns whatever decision update value function follows transition state action st max ff learning rate order learning perform well potentially important state action pairs must explored state probability apply random action instead action recommended value however still use update values using action recommended values structural limits state space theoretical reasons desirable use table lookup representation practice since mobile radio decides using information available impossible following reasons state channel never known directly receiver observes errored packets possible infer state packets actually received channel state changes introduce inference errors traditional packet applications rarely communicate state information transport layer state information could also inferred given quickly changing application dynamics application state often ignored particular parameters table ton application generates packet state completely specified packet arrivals need inferred serious deficiency complete state space representation mobile radio turns knowledge state changes base station even protocol provisions transferring directly state information implies state information must inferred one approach structural limits use pomdp approach leave future work paper simply learn deterministic policies features estimate state simplifying assumptions beyond structural problems previous section must treat usual problem state space huge instance assuming even moderate maximum queue sizes maximum wait times yields states one considers mail like applications brown component feature mobile radio radio mobile radio number packets waiting mobile mobile radio wait time first packet waiting mobile channel number errors received last time slots base radio number time slots since mobile last table decision features measured mobile radio wait times minutes time slot wait times many packets waiting possible state space exceeds states thus seek representation reduce size complexity state space reduction taken two parts first feature representation possible given structural limits previous section second function approximation based feature vectors feature vectors listed table chosen since measurable mobile radio function approximation use state aggregation since provably converges simulation results section describes simulation based experiments mobile radio control problem initial study simplified problem setting pg pb channel errors state aggregation used aggregate states battery termination probability ps simply power appropriate state action chosen table chosen expected battery life much longer time scale traffic channel processes three policies learned one application reward criteria resulting policies tested simulating time slots test run upper lower bound energy usage computed upper bound case mobile radio always lower bound policy ignores reward criteria still delivers packets policy radio packets accumulated latter portion test run sent one large group policies compared using normalized power savings measure close policy lower bound upper lower bound results given table table also lists average reward per packet received application mail application constraints packets average reward identically one conclusion paper showed reinforcement learning able learn policy significantly reduced power consumption mobile radio maintaining high application utility used novel variable discount factor captured impact different actions battery life able gain possible power savings exist policies exceed power toggle onand offoften generate many notification packets always policy baseline trying improve upon low power wireless communication via reinforcement learning normalized average application power savings reward mail real time web browsing table simulation results application paper used simple model radio channel battery etc also used simple state aggregation ignored partially observable aspects problem future work address accurate models function approximation pomdp approaches acknowledgment work supported career award ncr nsf grant ncr
abstract many researchers explored methods hierarchical reinforcement learning rl temporal abstractions abstract actions defined perform many primitive actions terminating however little known learning state abstractions aspects state space ignored previous work developed maxq method hierarchical rl paper define five conditions state abstraction combined maxq value function decomposition prove maxq learning algorithm converges conditions show experimentally state abstraction important successful application maxq learning introduction work hierarchical reinforcement learning focused temporal abstraction example options framework programmer defines set macro actions options provides policy learning algorithms semi markov learning treat temporally abstract actions primitives learn policy selecting among closely related ham framework programmer constructs hierarchy finitestate controllers controller include non deterministic states programmer sure action perform hamq learning algorithm applied learn policy making choices non deterministic states approaches studies hierarchical rl option finite state controller must access entire state space one exception feudal method dayan hinton introduced state abstractions unsafe way resulting learning problem partially observable hence could provide formal results convergence performance method even brief consideration human level intelligence shows methods cannot scale deciding walk bedroom kitchen need think location car without state abstractions rl method learns value functions must learn separate value state state abstraction maxq hierarchical reinforcement learning world argue solved clever value function approximation methods merit view paper however explore different approach identify aspects mdp permit state abstractions safely incorporated hierarchical reinforcement learning method without introducing function approximations permits us obtain first proof convergence hierarchical rl optimal policy presence state abstraction introduce state abstractions within maxq flamework basic ideas general previous work maxq briefly discussed state abstractions employed experiments however could prove algorithm maxq converged state abstractions usable characterization situations state abstraction could safely employed paper solves problems addition compares effectiveness maxq learning without state abstractions results show state abstraction important cases essential effective application maxq learning maxq framework let markov decision problem states actions reward function probability transition function ls results apply finite horizon undiscounted case infinite horizon discounted case let mn set subtasks subtask mi defined termination predicate ti set actions ai may subtasks primitive actions goal subtask mi move environment state ti satisfied refined using local reward function express preferences among different states satisfying ti omit refinement paper subtasks must form dag single root node subtask may invoke directly indirectly hierarchical policy set policies rn one subtask hierarchical policy executed using standard procedure call return semantics starting root task unfolding recursively primitive actions executed policy mi invoked state let nis probability terminates state executing primitive actions hierarchical policy recursively optimal policy ri optimal given policies descendants dag let value function subtask state value following policy starting reach state satisfying ti similarly let value subtask executing child action state executing current policy termination maxq value function decomposition based observation subtask mi viewed semi markov decision problem reward performing action state equal value function subtask state see consider sequence rewards rt received execute child action continue subsequent actions according hierarchical policy rt rt rt st macro action execute number steps return hence partition sum two terms rt st dietterich first term discounted sum rewards subtask terminates second term cost finishing subtask executed discounted time initiated call second term completion function denote write bellman equation maxq terminate recursion define primitive action expected reward performing action state maxq learning algorithm simple variation learning subtask mi state choose child action invoke current policy returns observe resulting state number elapsed time steps update according ax prove convergence require exploration policy executed learning ordered glie policy ordered policy policy breaks value ties among actions preferring action comes first fixed ordering glie policy policy executes action infinitely often every state visited infinitely often converges probability greedy policy ordering condition required ensure recursively optimal policy unique without condition potentially many different recursively optimal policies different values depending ties broken within subtasks subsubtasks theorem let either episodic mdp deterministic policies proper discounted infinite horizon mdp discount factor let dag defined subtasks mo mk let sequence constants subtask mi lim lim oe let rx ordered glie policy subtask mi state assume vt ict bounded probability algorithm maxq converges unique recursively optimal policy consistent rx proof sketch proof based proposition bertsekas tsitsiklis follows standard stochastic approximation argument due generalized case non stationary noise two key points proof define pt nls probability transition function describes behavior executing current policy subtask time inductive argument show probability transition function converges probability transition function recursively optimal policy second show convert usual weighted max norm contraction weighted max norm contraction straightforward completes proof notable maxq learn value functions subtasks simultaneously need wait value function subtask converge beginning learn value function parent task gives completely online learning algorithm wide applicability state abstraction maxq hierarchical reinforcement learning figure left taxi domain taxi row column right task graph conditions safe state abstraction motivate state abstraction consider simple taxi task shown figure four special locations world marked ed lue reen ellow episode taxi starts randomly chosen square passenger one four locations chosen randomly passenger wishes transported one four locations also chosen randomly taxi must go passenger location source pick passenger go destination location destination put passenger episode ends passenger deposited destination location six primitive actions domain four navigation actions move taxi one square north south east west pickup action putdown action action deterministic reward action additional reward successfully delivering passenger reward taxi attempts execute putdown pickup actions illegally navigation action would cause taxi hit wall action op usual reward task hierarchical structure see fig two main sub tasks get passenger get deliver passenger put subtasks turn involves subtask navigating one four locations navigate bound desired target location performing pickup putdown action task illustrates need support temporal abstraction state abstraction temporal abstraction obvious example get temporally extended action take different numbers steps complete depending distance target top level policy get passenger deliver passenger expressed simply abstractions need state abstraction perhaps less obvious consider get subtask subtask solved destination passenger completely irrelevant cannot affect nagivation pickup decisions perhaps importantly navigating target location either source destination location passenger taxi location identity target location important fact cases taxi carrying passenger cases irrelevant introduce five conditions state abstraction assume state mdp represented vector state variables state abstraction defined combination subtask mi child action identifying subset state variables relevant defining value function policy using relevant variables value functions policies dietterich said abstract first two conditions involve eliminating irrelevant variables within subtask maxq decomposition condition subtask irrelevance let mi subtask mdp set state variables irrelevant subtask state variables partitioned two sets stationary abstract hierarchical policy executed descendants mi following two properties hold state transition probability distribution nis child action mi factored product two distributions nlx nix lx give values variables give values variables pair states sl yl child action taxi problem source destination passenger irrelevant navigate subtask target current taxi position relevant advantages form abstraction similar obtained boutilier dearden goldszmidt belief network models actions exploited simplify value iteration stochastic planning condition leaf irrelevance set state variables irrelevant primitive action pair states differ values variables condition satisfied primitive actions north south east west taxi task state variables irrelevant constant next two conditions involve funnel actions macro actions move environment large number possible states small number resulting states completion function subtasks represented using number values proportional number resulting states condition result distribution irrelevance undiscounted case set state variables yj irrelevant result distribution action abstract policies executed mj descendants maxq hierarchy following holds pairs states differ values state variables yj lsl ls consider example get subroutine optimal policy taxi task regardless taxi position state taxi passenger starting location get finishes executing taxi completed picking passenger hence taxi initial position irrelevant resulting position note true undiscounted settingwith discounting result distributions number steps required get finish depends much starting location taxi hence form state abstraction rarely useful cumulative discounted reward condition termination let mj child task mi property whenever mj terminates causes mi terminate completion state abstraction maxq hierarchical reinforcement learning cost need represented particular kind funnel action funnels states terminal states mi example taxi task states taxi holding passenger put subroutine succeed result terminal state root termination predicate put passenger destination location implies termination condition root means root put uniformly zero states put terminated condition shielding consider subtask mi let state paths root dag mi exists subtask terminated values need represented subtask mi state never executed taxi task simple example arises put task terminated states passenger taxi means need represent root put states result combined termination condition need explicitly represent completion function put applying abstraction conditions taxi task value function represented using values much less values required fiat learning without state abstractions maxq requires values theorem convergence state abstraction let maxq task graph incorporates five kinds state abstractions defined let rx ordered glie exploration policy abstract conditions theorem maxq converges probability unique recursively optimal policy defined rx proof sketch consider subtask mi relevant variables two arbitrary states first show five abstraction conditions value function represented using ignoring values learn values learning algorithm needs samples drawn according nix second part proof involves showing regardless whether execute state resulting distribution hence give correct expectations analogous arguments apply leaf irrelevance termination shielding cases easy experimental results implemented maxq noisy version taxi domain kaelbling hdg navigation task using boltzmann exploration figure shows performance flat maxq without state abstractions tasks learning rates boltzmann cooling rates separately tuned optimize performance method results show without state abstractions maxq learning slower converge flat learning state abstraction much faster conclusion paper shown understanding reasons state variables irrelevant obtain simple proof convergence maxq learning dietterich maxq abs mction flat primitive actions maxq ab tracfion goooo le primitive actions figure comparison maxq without state abstraction fiat learning noisy taxi domain left kaelbling hdg task right horizontal axis gives number primitive actions executed method vertical axis plots average separate runs state abstraction much fruitful previous efforts based weak notions state aggregation suggests future research focus identifying conditions permit safe state abstraction
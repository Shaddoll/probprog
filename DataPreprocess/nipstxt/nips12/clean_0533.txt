abstract present class approximate inference algorithms graphical models qmr dt type give convergence rates algorithms jaakkola jordan algorithm verify theoretical predictions empirically also present empirical results difficult qmr dt network problem obtaining performance new algorithms roughly comparable jaakkola jordan algorithm introduction graphical models formalism provides appealing framework design analysis network based learning inference systems formalism endows graphs joint probability distribution interprets queries interest marginal conditional probabilities joint fixed model one generally interested conditional probability output given input prediction input conditional output diagnosis control learning focus usually likelihood marginal probability conditional probability unobserved nodes given observed nodes em gradient based algorithm conditional probability parameters given observed data bayesian setting cases key computational operation marginalization several methods available computing marginal probabilities graphical models involve form message passing graph exact methods viable many interesting cases involving sparse graphs infeasible dense graphs consider current paper number approximation methods evolved treat cases include search based methods loopy propagation stochastic sampling variational methods variational methods focus current paper applied successfully number large scale inference problems particular jaakkola jordan developed variational inference method qmr dt network benchmark network involving nodes see variational method provided accurate approximation posterior probabilities within second computer time difficult ng dordan inference problem exact methods entirely infeasible see loopy propagation converge correct posteriors murphy weiss jordan stochastic sampling methods slow unreliable jaakkola jordan significant step forward understanding variational inference made kearns saul used large deviation techniques analyze convergence rate simplified variational inference algorithm imposing conditions magnitude weights network established log rate convergence error algorithm fan current paper utilize techniques similar kearns saul derive new set variational inference algorithms rates faster log techniques also allow us analyze convergence rate jaakkola jordan algorithm test algorithms idealized problem verify analysis correctly predicts rates convergence apply algorithms difficult qmr dt network problem background qmr dt network qmr dt quick medical reference decision theoretic network bipartite graph approximately top level nodes di representing diseases approximately lower level nodes representing findings observed symptoms nodes binaryvalued disease given prior probability di obtained archival data finding parameterized noisy model ri set parent diseases finding fi parameters ij obtained assessments medical experts see shwe et al letting zi oio je oi following expression likelihoodl sum sum across approximately configurations diseases note second product product negative findings factorizes across diseases ds factors absorbed priors significant effect complexity inference positive findings couple diseases prevent sum distributed across product generic exact algorithms junction tree algorithm scale exponentially size maximal clique moralized triangulated graph jaakkola jordan found cliques nodes qmr dt rules junction tree algorithm heckerman discovered factorization specific qmr dt reduces complexity substantially however resulting algorithm still scales exponentially number positive findings feasible small subset benchmark cases expression factors dj probabilities associated parent less disease nodes factors probabilities child finding nodes observed positive state factors probabilities negative findings resulting product joint probability marginalized obtain likelihood approximate inference algorithms two layer bayesian networks jaakkola jordan algorithm jaakkola jordan proposed variational algorithm approximate inference qmr dt setting briefly approach make use following variational inequality zl izi ci ci deterministic function inequality holds arbitrary values free variational parameter ai substituting variational upper bounds probabilities positive findings eq one obtains factorizable upper bound likelihood factorizability sum across diseases distributed across joint probability yielding product sums rather sum products one minimizes resulting expression respect variational parameters obtain tightest possible variational bound kearns saul ks algorithm simplified variational algorithm proposed kearns saul whose main goal theoretical analysis rates convergence variational algorithms approach local conditional probability finding fi approximated value point small distance depending whether upper lower bounds desired mean input zi yields variational algorithm values variational parameters optimized assumption weights oij bounded magnitude constant number parent disease nodes kearns saul showed error likelihood algorithm converges rate log algorithms based local expansions inspired kearns saul describe design approximation algorithms qmr dt obtained expansions around mean input finding nodes rather using point approximations kearns saul ks algorithm make use taylor expansions see also plefka barber van de laar perturbational techniques consider generalized qmr dt architecture noisy model replaced general function uniformly bounded derivatives define zk zi fl zi fl likelihood written zl zk also define zi oio ej oop dj simple mean field like approximation obtained evaluating mean values refer approximation mf expanding function second order defining ci zi fil cixq fili il qt zili il ia ng dordan subscripts represent derivatives dropping remainder term bringing expectation inside mf approximation fili il generally obtain mf approximation carrying taylor expansion th order analysis section give two theorems establishing convergence rates mf family algorithms jaakkola jordan algorithm kearns saul results obtained assumption weights magnitude recall number disease nodes large assumption weak interactions implies zi close mean value high probability law large numbers thereby gives justification use local expansions probabilities findings due space constraints detailed proofs theorems given section deferred long version paper instead sketch intuitions proofs theorem let number offindings fixed suppose ioijl fixed constant absolute error mf approximation odd even proof intuition first consider case odd since ioijl quantity ei zi oij dj dj like average random variables hence standard deviation order fl since mf matches th order derivatives find take taylor expansion mf error leading non zero term ei standard st order term contains quantities deviation order unsurprising order gives error mf odd even leading non zero term taylor expansion error st order think ei converging via central limit term quantities theorem effect symmetric distribution since symmetric distributions small odd central moments would small means even may look order term error leads mf big error mf note also consistent mf mf always give estimates hence absolute error theorem may also proved convergence rate jaakkola jordan jj algorithm simplicity state noisy networks closely related result also holds sigmoid networks suitably modified assumptions see full paper theorem let fixed suppose noisy function suppose oij fixed constant gmin fixed gmin absolute error jj approximation note case jj applied log concave noisy networks incidentally weights non negative approximate inference algorithms two layer bayesian networks condition gmi lowerbounding zi ensures findings unlikely hold sufficient bias leak nodes network weights bounded away zero proof intuition neglecting negative findings discussed need handled variationally result proved simplified version jj algorithm always chooses variational parameters exponential upperbound zi tangent zi zi normal version jj error worse simplified one taking taylor expansion approximation error find since upperbound matched zeroth first derivatives error discussed mf proof outline second order term quantities quantity expectation order hence jj error summarize results useful cases find mf convergence rate mf mf rates jj convergence rate simulation results artificial networks carried set simulations intended verify theoretical results presented previous section used bipartite noisy networks full connectivity layers weights oij chosen uniformly number top level disease nodes ranged priors disease nodes chosen uniformly results shown figure one five positive findings similar results obtained additional positive findings diseases lo lff diseases figure absolute error likelihood averaged many randomly generated networks function number disease nodes various algorithms short dashed lines ks upper lower bounds curves overlap left panel long dashed line jj algorithm solid lines mf mf mf latter two curves overlap right panel results entirely consistent theoretical analysis showing nearly exactly expected slopes loglog plot moreover asymptotic results anomalous behavior ks lower bound second panel due fact algorithm generally finds vacuous lower bound case yields error essentially constant function number diseases ng jordan also predictive overall performance mf mf algorithms perform best cases mf jj roughly equivalent ks least accurate qmr dt network present results qmr dt network particular four benchmark cpc cases studied jaakkola jordan cases fewer positive findings thus possible run heckerman quickscore algorithm obtain true likelihood case case exactly treated findings exactly treated findings figure results cpc cases different numbers exactly treated findings horizontal line true likelihood dashed line jj estimate lower solid line mf estimate case case exactly treated findings exactly treated findings figure results cpc cases legend jaakkola jordan hybrid methodology proposed portion findings treated approximately exact methods used treat remaining findings using hybrid methodology figures show results running jj mf four cases experiments run using version jj algorithm optimizes variational parameters without findings treated exactly uses fixed values parameters thereafter order findings chosen treated exactly based jj estimates described jaakkola jordan missing points graphs cases approximate inference algorithms two layer bayesian networks results show mf algorithm yielding results comparable jj algorithm conclusions extension multilayer networks paper presented class approximate inference algorithms graphical models qmr dt type supplied theoretical analysis convergence rates verified rates empirically presented promising empirical results difficult qmr dt problem although focus paper two layer networks mf family algorithms also extended multilayer networks example consider layer network nodes bi parents nodes di parents nodes fi approximate pr using say mf first write pr expectation function zi approximate function via second order taylor expansion calculate expectation taylor approximation need calculate terms expansion di didj di parents quantities easily derived terms disease prior probabilities instead depend joint distribution di dj use two layer version mf applied first two bi di layers network approximate important future work carefully study performance algorithm multilayer setting acknowledgments wish acknowledge helpful advice tommi jaakkola michael kearns kevin murphy larry saul
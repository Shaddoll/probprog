abstract bayesian predictions stochastic like predictions inference scheme generalize finite sample simple variational argument shows bayes averaging generalization optimal given prior matches teacher parameter distribution situation less clear teacher distribution unknown define class averaging procedures temperated likelihoods including bayes averaging uniform prior maximum likelihood estimation special cases show bayes generalization optimal family teacher distribution two learning problems analytically tractable learning mean gaussian asymptotics smooth learners introduction learning stochastic process generalizing random finite sample data often learning problem natural quantitative measure generalization loss function defined natural measure generalization error expected loss random sample independent training set generalizability key topic learning theory much progress reported analytic results broad class machines found litterature describing asymptotic generalization ability supervised algorithms continuously parameterized asymptotic bounds generalization general machines advocated vapnik generalization results valid finite training sets obtained specific learning machines see rich framework analysis generalization bayesian averaging schemes defined averaging become popular tool improving generalizability learning machines context time series forecasting averaging investigated intensely decades neural network ensembles shown improve generalization simple voting later work generalized results types averaging boosting bagging stacking arcing recent examples averaging procedures based data resampling shown useful see recent review references however bayesian averaging particular attaining kind cult status bayesian averaging indeed provably optimal hansen number various ways admissibility likelihood principle etc follows construction bayes generalization optimal given correct prior information teacher parameter distribution situation less clear teacher distribution unknown hence pragmatic bayesians downplay role prior instead averaging aspect emphasized vague priors invoked important note whatever prior used bayesian predictions stochastic like predictions inference scheme generalize finite sample contribution analyse two scenarios averaging improve generalizability show vague bayes average fact optimal among averaging schemes investigated averaging shown reduce variance cost introducing bias bayes happens implement optimal bias variance trade bayes generalization consider model smoothly parametrized whose predictions described terms density function predictions model based given training set finite sample xa stochastic vector whose density teacher denoted xloo words true density assumed defined fixed unknown teacher parameter vector model denoted involves parameter vector predictive density given xld xl oid oid parameter distribution produced training process maximum likelihood scenario distribution delta function centered likely parameters model given data set ensemble averaging approaches like boosting bagging stacking distribution obtained training resampled traning sets bayesian scenario parameter distribution posterior distribution oid dio oih lh oih prior distribution probability density parameters empty sequel consider one model hence suppress model conditioning label generalization error average negative log density also known simply log loss applied statistics works known deviance dlo logp xld xloo dx expected value generalization error training sets produced given teacher given logp oo dxp oo dd limit us conventional density estimation pattern recognition many functional approximations problems formulated density estimation problems well bayesian averaging well temperated playing game guessing probability distribution face random training set also face teacher drawn teacher distribution teacher averaged generalization must defined oo eo deo typical generalization error random training set randomly chosen teacher produced model generalization error minimized bayes averaging teacher distribution used prior see form lagrangian functional xld logq xld oo dxp dloo ddp oo doo xld dx defined positive functions xid second term used ensure xld normalized density compute variational derivative obtain xld xlo dlo equating derivative zero recover predictive distribution bayesian averaging dlo xid xlo dlo used dlo appropriate normalization constant easily verified indeed global minimum averaged generalization error also note bayes average performed another prior teacher distribution expect higher generalization error important question bayesian point view cases averaging generic priors vague uniform priors shown optimal temperated likelihoods come closer quantative statement vague bayes better procedure analyse two problems analytical progress possible consider one parameter family learning procedures including bayes maximum likelihood procedure oi dlo positive parameter plying role inverse temperature family procedures averaging procedures controls width average vague bayes used synonymously bayes uniform prior recoved maximum posterior procedure obtained cooling zero width context generalization design question frased follows optimal temperature family temperated likelihoods example normal variates let teacher distribution given xloo exp hansen model density form unknown er assumed known examples posterior uniform prior old exp zoo temperated likelihood obtained raising fi th power normalizing fi exp predictive distribution found integrating xld la note distribution wider averaging procedures maximum likelihood less variant por small predictive distribution almost independent data set hence highly bi ed straightforward compute generalization error predictive distribution general pirst compute generalization error specific training set fi oo logp fi oo dx log average generalization error found averaging sampling distribution using oo fi fi ddp dloo log first note generalization error independent teacher parameter happened location parameter fi dependency averaged generalization error depicted figure solving fi ofi find optimal fi solves fi note result holds independent teacher parameter bayes averaging unit temperature optimal given value hence teacher distribution may say vague bayes scheme robust teacher distribution clearly much stronger optimality general result proven bias variance tradeoff interesting decompose generalization error eq bias variance components follow heskes define bias error generalization error geometric average distribution fi log xloo dx bayesian averaging well temperated generalization bay temperature figure bias variance trade function width temperated likelihood ensemble temperature fi bias computed generalization error predictive distribution obtained geometric average distribution training set fluctuations proposed heskes predictive distribution produced bayesian averaging corresponds unit temperature vertical line achieves minimal generalization error maximum likelihood estimation reference recovered zero width temperature limit exp log dloo dd inserting eq find integrating teacher distribution find fi log oo variance error given fi fi fi fi quantify statements averaging bias introduced predictive distribution becomes wider decrease variance contribution initially generalization error sum two decreases still higher temperatures bias becomes strong generalization error start increase bayes average unit temperature optimal trade within given family procedures hansen asymptotics smoothly parameterized models go show similar result also holds general learning problems limit large data sets consider system parameterized finite dimensional parameter vector given large training set smooth likelihood function temperated likelihood approximately gaussian centered maximum posterior parameters hence normalized temperated posterior reads olfid fina exp om om om denoting maximum likelihood solution given training sample second derivative hessian matrix given oooo logp predictive distribution given lo ol write exp expand around oml second order find xloml exp xlo xlo position perform integration posterior find normalized predictive distribution xl xlom na exp xlom xlom xlom na proceeding compute generalization error ogp loo dloo dd su ciently smooth likelihoods fluctuations maximum likelihood parameters asymptotic normal see furthermore fluctuations neglected means approximate xloo xloo dx averaged fisher information matrix approximations valid generalization error found log dim denoting dimension parameter vector like example eq find generalization error ymptotically independent teacher parameters minimized conclude bayes well temperated asymptotics holds teacher distribution bayes literature refered prior overwhelmed data decomposing errors bi variance contributions find similar results example bayes introduces optimal bias averaging unit temperature bayesian averaging well temperated discussion seen two examples bayes averaging optimal particular improving maximum likelihood estimation found averaging introduces bias reduces variance generalization error sum bias variance initially decrease bayesian averaging unit temperature optimal width averaging distribution larger temperatures widths bias strong generalization error increases examples special sense lead generalization errors independent random teacher parameter generic course rather generic case mis specified prior lead arbitrary large learning catastrophes acknowledgments thank organizers max planck institute workshop statistical physics neural networks michael biehl wolfgang kinzel ido kanter work initiated thank carl edward rasmussen jan larsen manfred opper stimulating discussions bayesian averaging work funded danish research councils computational neural network center connect thor center neuroinformatics
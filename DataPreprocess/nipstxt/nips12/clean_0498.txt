abstract describe new incremental algorithm training linear threshold functions relaxed online maximum margin algorithm romma romma viewed approximation algorithm repeatedly chooses hyperplane classifies previously seen examples correctly maximum margin known maximum margin hypothesis computed minimizing length weight vector subject number linear constraints romma works maintaining relatively simple relaxation constraints efficiently updated prove mistake bound romma proved perceptron algorithm analysis implies computationally intensive maximum margin algorithm also satisfies mistake bound first worst case performance guarantee algorithm describe experiments using romma variant updates hypothesis aggressively batch algorithms recognize handwritten digits computational complexity simplicity algorithms similar perceptron algorithm generalization much better describe sense performance romma converges svm limit bias considered introduction perceptron algorithm well known simplicity effectiveness case linearly separable data vapnik support vector machines svm use quadratic programming find weight vector classifies training data correctly maximizes margin minimal distance separating hyperplane instances algorithm slower perceptron algorithm generalizes better hand incremental algorithm perceptron algorithm better suited online learning algorithm repeatedly must classify patterns one time finds correct classification updates hypothesis making next prediction paper design analyze new simple online algorithm called romma relaxed online maximum margin algorithm classification using linear threshold relaxed online maximum margin algorithm function romma similar time complexity perceptron algorithm generalization performance experiments much better average moreover romma applied kernel functions conducted experiments similar performed cortes vapnik freund schapire problem handwritten digit recognition tested standard perceptron algorithm voted perceptron algorithm details see new algorithm using polynomial kernel function choice best found new algorithm performed better standard perceptron algorithm slightly better performance voted perceptron research aims similar refer reader paper organized follows section describe romma enough detail determine predictions prove mistake bound section describe romma detail section compare experimental results romma aggressive variant romma perceptron voted perceptron algorithms mistake bound analysis online algorithms concreteness analysis concern case instances also called patterns weight vectors fix standard online learning model learning proceeds trials tth trial algorithm first presented instance next algorithm outputs prediction classification finally algorithm finds correct classification yt yt say algorithm makes mistake worth emphasizing model making prediction tth trial algorithm access instance classification pairs previous trials online algorithms consider work maintaining weight vector updated trials predicting sign sign positive negative otherwise perceptton algorithm perceptron algorithm due rosenblatt starts prediction differs label yt updates weight vector yt prediction correct weight vector changed next three algorithms consider assume data seen online algorithm collectively linearly separable weight vector trial yt sign kernel functions used often case practice ideal online maximum margin algorithm trial algorithm chooses weight vector previous trials sign ys maximizes minimum distance separating hyperplane known implemented choosing minimize subject constraints constraints define convex polyhedron weight space refer pt relaxed online maximum margin algorithm new algorithm first difference trials mistakes made ignored second difference ithe prediction ensures mistake make proofs simpler usual mistake bound proof perceptron algorithm goes change li long algorithm responds mistakes relaxed algorithm starts like ideal algorithm second trial sets shortest weight vector li mistake second trial chooses would ideal algorithm smallest element yl however third trial mistake behaves differently instead choosing smallest element yl ya lets smallest element origin figure romma convex polyhedron weight space replaced halfspace smallest element iiall thought third trial replacing polyhedron defined halfspace tg see figure note halfspace contains polyhedron fact contains convex set whose smallest element thus thought least restrictive convex constraint smallest satisfying weight vector let us call halfspace ha algorithm continues manner tth trial mistake chosen smallest element ht yt ht set tth trial mistake ht hr call ht old constraint yt new constraint note mistake algorithm needs solve quadratic programming problem two linear constraints fact simple closed form expression function lit enables computed incrementally using time similar perceptron algorithm described section relaxed online maximum margin algorithm aggressive updating algorithm previous algorithm except update made trial lit mistakes upper bound number mistakes made prove bound number mistakes made romma previous mistake bound proofs show mistakes result increase measure progress appeal bound total possible progress proof use squared length measure progress first need following lemmas lemma run romma linearly separable data trial mistake new constraint binding new weight vector proof purpose contradiction suppose new constraint binding new weight vector since fails satisfy constraint line connecting intersects border hyperplane new constraint denote intersecting point represented ct ct relaxed online maximum margin algorithm since square euclidean length ii convex function following holds ql llt tll since unique smallest member ht wt implies ii since ht hence contradicts definition lemma run romma linearly separable data trial mistake first one old constraint binding new weight vector ilwtll proof let plane weight vectors make new constraint tight element lit lemma tft let gt tll perpendicular tf satisfies ii ii tll ii tll therefore length vector minimized gt monotone distance gr thus old constraint binding gt since otherwise solution could improved moving little bit toward gr old constraint requires ii tll litxt tll rearranging get lit ilr tll tll means lit tll ii tll zt follows fact data linearly separable iiwtll follows fact least one previous mistake since trial mistake lit contradiction ready prove mistake bound theorem choose sequence li li patternclassification pairs let maxt iir tll weight vector ff lit ff gt number mistakes made romma li wii proof first claim ff hr easily seen since ff satisfies constraints ever imposed weight vector therefore relaxations constraints since smallest element ht ii implies ii xll therefore ii sll claim trial mistake ii sll ii imply induction mistakes squ ed length algorithm weight vector least since algorithm weight vectors longer complete proof bt figure bt pt choose index trial mistake made let lit zt bt lemmas ffl distance call pt satisfies ii tll ii since fact mistake trial implies lit also since wtq wtll pt li long normal vector bt bt ii ii thus applying ii lll discussed completes proof using fact easily proved using induction pt ht easily prove following complements analyses maximum margin algorithm using independence assumptions details omitted due space constraints theorem choose sequence yl ym patternclassification pairs let maxt ii tll weight vector ff yt ff gt rn number mistakes made ideal online maximum margin algorit mon yl gin ym ii proof theorem update made yt gt instead yt zt progress made seen least de applied prove following theorem choose rn sequence gl yi gin ym patternclassification pairs let maxt weight vector ff yt yl grn yrn presented line number trials aggressive romma yt theorem implies sense repeatedly cycling dataset using aggressive romma eventually converge svm note however bias considered efficient implementation prediction romma differs expected label algorithm chooses minimize ii subject titstile simple calculation shows aa xb ii tllell tll yt ft ii lell ii ii dt trials mistake made ct ii trials ct landdt always ctt tq dt lit ii ii note based lemmas denominators never equal zero since computations required romma involve inner products together operations scalars apply kernel method algorithm efficiently solving original problem high dimensional space computationally need modify algorithm replacing inner product computation kernel function computation zi make prediction tth trial algorithm must compute inner product prediction vector order apply kernel function store prediction vector implicit manner weighted sum examples relaxed online maximum margin algorithm mistakes occur training particular represented formula may seem daunting however making use recurrence ct dt obvious complexity new algorithm similar perceptton algorithm born experiments implementation aggressive romma similar experiments experiments using romma aggressive romma batch algorithms mnist ocr database obtained batch algorithm online algorithm usual way making number passes dataset using final weight vector classify test data every example database two parts first matrix represents image corresponding digit entry matrix takes value second part label taking value dataset consists training examples test examples adopt following polynomial kernel zi zj corresponds using expanded collection features including products components original feature vector see let us refer mapping original feature vector expanded feature vector note one component always therefore component weight vector corresponding component viewed bias experiments set rather speed learning coefficient corresponding bias chose since experiments problem conducted best results occur value cope multiclass data trained romma aggressive romma labels classification unknown pattern done according maximum output ten classifiers every entry image matrix takes value order magnitude least might cause round offerror computation ci di scale data dividing entry training romma table experimental results mnist data err misno err misno err misno err misno percep voted percep romma agg romma agg romma nc since performance online learning affected order sample sequence results shown table average random permutations columns marked national institute standards technology special database http www research att com yann ocr information obtaining dataset see li long misno table show total number mistakes made training labels although online learning would involve one epoch present results batch setting four epochs table represents number epochs deal data linearly inseparable feature space also improve generalization friess et al suggested use quadratic penalty cost function implemented using slightly different kernel function kronecker delta function last row table result aggressive romma using method control noise classifiers conducted three groups experiments one perceptron algorithm denoted percep second voted perceptron denoted voted percep whose description third romma aggressive romma denoted agg romm aggressive romma noise control denoted agg romma nc data third group scaled three groups set results table demonstrate romma better performance standard perceptron aggressive romma slightly better performance voted perceptron aggressive romma noise control compared perceptrons without noise control presentation used show performance new online algorithm could achieve course best since classifiers use remarkable phenomenon new algorithm behaves well first two epochs
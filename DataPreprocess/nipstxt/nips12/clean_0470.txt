abstract present general framework discriminative estimation based maximum entropy principle extensions calculations involve distributions structures parameters rather specific settings reduce relative entropy projections holds even data separable within chosen parametric class context anomaly detection rather classification labels training set uncertain incomplete support vector machines naturally subsumed class provide several extensions also able estimate exactly efficiently discriminative distributions tree structures class conditional models within flamework preliminary experimental results indicative potential techniques introduction effective discrimination essential many application areas employing generative probability models mixture models context attractive criterion maximum likelihood used parameter structure estimation suboptimal support vector machines svms example robust techniques specifically designed discrimination approach towards general discriminative training based well known maximum entropy principle enables appropriate training ordinary structural parameters model cf approach limited probability models extends svms maximum entropy classification consider two class classification problem labels assigned extension multi class straightforward formulation also admits easy extension regression problems analogously svms maximum entropy discrimination examples given two generative probability distributions xlov parameters one class corresponding decision rule follows sign discriminant function xlo xlo log bias term usually expressed log ratio log class conditional distributions may come different families distributions parametric discriminant function could specified directly without reference models parameters may also include model structure see later sections parameters chosen maximize classification accuracy consider general problem finding distribution parameters using convex combination discriminant functions xlo decision rule search optimal formalized maximum entropy estimation problem given set training examples xt corresponding labels yt find distribution maximizes entropy subject classification constraints yt xtlo specifies desired classification margin solution unique exists since concave linear constraints specify convex region note preference towards high entropy distributions fewer assumptions applies within admissible set distributions consistent constraints see related work extend basic idea number ways formulation assumes example training examples separated specified margin may also reason prefer parameter values others would therefore like incorporate prior distribution extensions generalizations discussed later paper complete formulation based following minimum relative entropy principle definition let xt yt training examples labels parametric discriminant function fit set margin variables assuming prior distribution find discriminative minimum relative entropy mre distribution minimizing piipo subject yt xtlo dod sign xlo specifies decision rule new example margin constraints preference towards large margin solutions encoded prior allowing negative margin values non zero probabilities also guarantees admissible set consisting distributions consistent constraints never empty even examples cannot separated discriminant function parametric class linear get valid solution miss classification penalties follow well jaakkola meila debara po figure minimum relative entropy mre projection prior distribution admissible set margin prior po potential terms mre formulation solid line svms dashed line case suppose po po po ce shown figure lb penalty margins smaller prior mean given relative entropy distance similar identical use slack variables support vector machines choices prior discussed mre solution viewed relative entropy projection prior distribution admissible set figure la illustrates view point view regularization theory prior probability specifies entropic regularization used approach theorem solution mre problem following general form lo alization constant pa ition function defines set non negative lagrange multipliers one classification constraint set finding unique maximum following jointly concave objective function logz solution sparse lagrange multipliers non zero arises many classification constraints become irrelevant constraints enforced small subset examples sparsity leads immediate weak generalization guarantees expressed terms number non zero lagrange multipliers practical leave one cross validation estimates also derived practical realization mre solution turn finding mre solution begin note disjoint factorization prior corresponding parameters appear distinct additive components ytl xt leads disjoint factorization mre solution example provides factorization result factorization bias term could eliminated imposing additional constraints lagrange multipliers analogous handling bias term support vector machines consider specific realizations support vector machines class graphical models maximum entropy discrimination support vector machines well known log likelihood ratio two gaussian distributions equal covariance matrices yields linear decision rule additional assumptions mre formulation gives support vector machines theorem assuming otx po po po po po approaches non informative prior po given eq lagrange multipliers obtained maximizing subject ktyt xttxt st log atat ytyt difference dual optimization problem svms additional potential term log highlights effect different miss classification penalties case come mre projection figure lb shows however additional potential term always carry huge effect moreover separable case letting two methods coincide decision rules formally identical consider case discriminant function corresponds log likelihood ratio two gaussians different adjustable covariance matrices parameters case means covariances prior must conjugate normal wishart obtain closed form integrals partition function density means covariances prior distribution form af mx zw kvo parameters specified manually one may let get non informative prior integrating parameters margin get isxl zt wt zt xt sl zt wtxtx nix wt scalar weight given wt yt yt weights set wt yt yt kt step function given updating done maximizing resulting marginal mre distribution parameters normalized normal wishart distribution af ml final values predicting label new example involves taking expectations discriminant function normal wishart eld el log constant ts thus obtain discriminative quadratic decision boundaries extend linear boundaries without explicitly resorting kernels generally covariance estimation framework adaptively modifies kernel done generally conjugate priors exponential family jaakkola meila jebara graphical models consider graphical models hidden variables mre distribution case distribution structures parameters finding distribution parameters done closed form conjugate priors observations complete distribution structures general intractable notable exception tree model discuss forthcoming tree graphical model graphical model structure tree model property log likelihood expressed sum local terms log wv uv discriminant function consisting log likelihood ratio pair tree models depending edge sets parameters also expressed form consider distribution tree structures fixed parameters treatment general case including parameters direct extension result distribution edge sets factorizes components uv eu hu uvez functions lagrange multipliers completely define distribution need find optimize theorem classification also need compute averages respect suffices obtain expression partition function discrete distribution possible tree structures variables trees however remarkable graph theory result called matrix tree theorem enables us perform necessary summations closed form polynomial time basis result find theorem normalization constant distribution form wuv iq uv wuv wv shows summing distribution trees distribution factors according trees edges done closed form computing value determinant time since obtain closed form expression optimization lagrange multipliers evaluating resulting classification rule also tractable figure provides comparison discriminative tree approach maximum likelihood tree estimation method dna splice junction problem aeach tree relies different set pairwise node marginals experiments class conditional pairwise marginals obtained directly data maximum entropy discrimination figure roc curves based independent test sets tree estimation discriminative solid ml dashed trees anomaly detection mre solid bayes dashed partially labeled case labeled solid labeled unlabeled dashed labeled unlabeled training examples dotted extensions anomaly detection anomaly detection given set training examples representing one class typical examples attempt capture regularities among examples able recognize unlikely members class estimating probability distribution basis training set xr via ml analogous criterion appropriate reason increase probability examples already well captured model relevant measure involves level sets logp used deciding class membership case estimate parameters optimize appropriate level set definition given probability model xlo set training examples xt set margin variables prior distribution po find mre distribution minimizes plipo subject constraints log xtlo dod note mre projection whose solution obtained choice po po straightforward since margin needs close achievable log probabilities nevertheless find reasonable choice relating prior mean percentile training set log probabilities generated ml estimation criterion denote resulting value define prior po po case prior mean figure shows context simple product distribution choice prior together mre flamework leads real improvement standard bayesian approach believe however effect striking sophisticated models hmms may otherwise easily capture spurious regularities data extension formalism latent variable models provided uncertain incompletely labeled examples examples uncertain labels hard deal probabilistic discriminative classification method uncertain labels however handled within maximum entropy formalism let yt set binary variables corresponding labels training examples define prior uncertainty labels specifying simplicity take product distribution jaakkola meila jebara po pt yt different level uncertainty assigned example consequently find minimum relative entropy projection prior distribution po po po admissible set distributions longer function labels consistent constraints evf yte xt dod mre principle differs transduction provides soft rather hard assignment unlabeled examples fundamentally driven large margin classification mre solution however often feasible obtain practice nevertheless formulate efficient mean field approach context figure demonstrates even approximate method able reap benefit unlabeled examples compare results dna splice junction classification problem details see discussion presented general approach discriminative training model parameters structures parametric discriminant functions formalism based minimum relative entropy principle reducing calculations relative entropy projections idea naturally extends beyond standard classification covers anomaly detection classification partially labeled examples feature selection
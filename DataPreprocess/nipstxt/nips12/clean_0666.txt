abstract dual estimation refers problem simultaneously estimating state dynamic system model gives rise dynamics algorithms include expectation maximization em dual kalman filtering joint kalman methods methods recently explored context nonlinear modeling neural network used functional form unknown model typically extended kalman filter ekf smoother used part algorithm estimates clean state given current estimated model ekf may also used estimate weights network paper points flaws using ekf proposes improvement based new approach called unscented transformation ut substantial performance gain achieved order computational complexity standard ekf approach illustrated several dual estimation methods introduction consider problem learning hidden states parameters discrete time nonlinear dynamic system xkq xk vk observed signal process noise drives dynamic system observation noise given note assuming additivity noise sources number approaches proposed problem dual ekf algorithm uses two separate ekfs one signal estimation one model estimation states estimated given current weights weights estimated given current states joint ekf state model parameters concatenated within combined state vector single ekf used estimate quantities simultaneously em algorithm uses extended kalman smoother step forward dual estimation unscented transformaa backward passes made data estimate signal model updated separate step thorough treatment theoretical basis algorithms relate see nelson rather provide comprehensive comparison different algorithms goal paper point assumptions flaws ekf section offer improvement based unscented transformation filter section unscented filter recently proposed substitute ekf nonlinear control problems known dynamic model paper presents new research use uf within dual estimation framework state weight estimation case weight estimation uf represents new efficient second order method training neural networks general flaws ekf assume know model weight parameters dynamic system equations given noisy observation recursive estimation expressed form optimal prediction yk optimal prediction recursion provides optimal mmse estimate assuming prior estimate current observation gaussian need assume linearity model optimal terms recursion given px js optimal prediction expectation nonlinear function random variables similar interpretation optimal prediction optimal gain term expressed function posterior covariance matrices note terms also require taking expectations nonlinear function prior state estimates kalman filter calculates quantities exactly linear case nonlinear models however extended kf approximates px fi predictions approximated simply function prior mean value estimates expectation taken covariance determined linearizing dynamic equations axk bye cxk dn determining posterior covariance matrices analytically linear system ekf viewed providing first order approximations optimal terms sense expressions approximated using first order taylor series expansion nonlinear terms around mean values second order versions ekf exist increased implementation computational complexity tend prohibit use unscented transformation filter unscented transformation ut method calculating statistics random variable undergoes nonlinear transformation consider propagating random variable dimension nonlinear function assume mean covariance calculate statistics form matrix sigma vectors first vector corresponds rest computed mean plus minus column matrix square root sigma wan merwe nelson vectors propagated nonlinear function mean covariance approximated using weighted sample mean covariance rt xi xi pz scaling hcton note method differs substantially general stapling methods monte methods ticle filters requffe orders magnitude staple poin attempt propagate accurate possibly nongaussian distribution state ut approximations accuram third order gaussian inputs nonline ities non gaussian inputs approximations accurate least second order accuracy detemined choice simple exmple shown figure dimensional system left plots shows mean cov iance propagation using monte stapling center plots show perform ce note sigma points requffed right plo show resulb using line ization approach would done superior performance ut clear actual sampling true mean ut linearized ekf sigma points xd ut tme covariance ut coiariance transformed sigma points arp atpaa figure example ut mean covariance propagation ut first order linear ekf actual unscented filter uf straightforward extension ut recursive estimation equation set denote corresponding sigma matrix uf equations given next page interesting note explicit calculation jacobians hessians necessary implement algorithm total number computations order compared ekf application dual estimation section shows use uf within several dual estimation approaches application domain comparison consider modeling noisy time series nonlinear note matrix square root using cholesky factorization order la however covariance matrices expressed recursively thus square root computed order performing recursive update cholesky factorization dual estimation unscented transformation uf equations wo vv klk yk px pg px rpt py autoregression vk underlying clean signal xk nonlinear function past values driven observed data point yk includes white gaussian process noise variance corresponding additive noise assumed gaussian variance state space representation signal given xk xk xk xk qb uk context dual estimation problem consists simultaneously estimating clean signal xk model parameters noisy data dual ekf dual uf one dual estimation approach dual extended kalman filter developed dual ekf requires separate state space representation signal weights state space representation weights generated considering stationary process identity state transition matrix driven process noise uk nk noisy measurement rewritten observation allows use ekf weight estimation representing second order optimization procedure two ekfs run simultaneously signal weight estimation every time step current estimate weights used signal filter current estimate signal state used weight filter wan merwe nelson dual uf ekf algorithm formed simply replacing ekf state estimation uf still using ekf weight estimation dual uf algorithm state weight estimation done uf note state transition linear weight filter nonlinearity restricted measurement equation uf gives exact measurement update phase estimation use uf weight estimation general discussed detail section joint ekf joint uf alternative approach dual estimation provided joint extended kalmanfilter framework signal state weight vector concatenated single joint state vector zk wk estimation done recursively writing state space equations joint state rb yk uk running ekf joint state space produce simultaneous estimates states discussed joint ekf provides approximate map estimates maximizing joint density signal weights given noisy data approach paper use uf instead ekf provide accurate estimation state resulting joint uf algorithm em unscented smoothing somewhat different iterative approach dual estimation given expectationmaximization em algorithm applied nonlinear dynamic systems iteration conditional expectation signal computed given data current estimate model step model found maximizes function conditional mean step linear models step solved closed form step computed kalman smoother combines forward time estimated mean covariance gr pf signal given past data backward time predicted mean covariance cht ph given future data producing following smoothed statistics given data pk xa mlp neural network model used step longer computed closed form gradient based approach used instead resulting algorithm usually referred generalized em gem step typically approximated extended kalman smoother wherein linearization model used backward propagation state estimates propose improving step em algorithm nonlinear models using uf instead ekf compute forward backward passes kalman smoothen rather linearize model backward pass neural network trained backward dynamics well forward dynamics allows exact backward estimation phase using uf enables development unscented smoother us exact step possible using rbf networks dual estimation unscented transformation experiments present results two simple time series provide clear illustration use uf ekf first series mackey glass chaotic series additive wgn snr db second time series also chaotic comes autoregressive neural network random weights driven gaussian process noise also corrupted additive wgn snr db standard mlp tanh hidden activation functions linear output layer used filters process measurement noise variances assumed known results training testing data well training curves different dual estimation methods shown quoted numbers normalized clean signal variance mean square estimation prediction errors superior performance ut based algorithms especially dual uf clear note also stable learning curves using uf approaches improvements found consistent statistically significant number additional experiments mackey glass train test chaotic ar nn train test algorithm est pred est pred dual ekf dual uf ekf dual uf joint ekf joint uf mackey glass oo dual ek dual uf ekf dual uf joint uf joint ekf nos iteration algorithm est pred est pred dual ekf dual uf ekf dual uf joint ekf joint uf chaotic ar nn dual ekf dual uf ekf dual joint ekf iteration final table compares smoother performance used step em algorithm case network models trained clean time series tested noisy data using either standard kalman smoother linearized backward model eks kalman smoother second nonlinear backward model eks unscented smoother us forward backward smoothed estimation errors reported performance benefits unscented approach clear mackey glass norm mse algorithm eks eks us chaotic ar nn norm mse algorithm eks eks us uf neural network training part dual uf algorithm introduced use uf weight estimation approach also seen new method general problem training neural networks regression classification problems input observed wan merwe nelson state estimation required advantage uf ekf case obvious state transition function linear see equation however pointed earlier observation nonlinear effectively ekf builds approximation expected hessian taking outer products gradient uf however may provide accurate estimate direct approximation expectation hessian performed number preliminary experiments standard benchmark data figure shows mean std learning curves computed experiments different initial weights mackay robot arm mapping dataset note faster convergence lower variance lower final mse performance uf weight training results encouraging study still necessary fully contrast differences uf ekf weight training learning curves uf mean uf std ekf mean eke istd opoch conclusions ekf widely accepted standard tool machine learning community paper presented alternative ekf using unscented filter uf consistently achieves better level accuracy ekf comparable level complexity demonstrated performance gain number dual estimation methods well standard regression modeling acknowledgements work sponsored part nsf grant iri
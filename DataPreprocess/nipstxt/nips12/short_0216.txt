abstract unsupervised learning algorithms designed extract structure data samples reliable robust inference requires guarantee extracted structures typical data source similar structures inferred second sample set data source overfitting phenomenon maximum entropy based annealing algorithms exemplarily studied class histogram clustering models bernstein inequality large deviations used determine maximally achievable approximation quality parameterized minimal temperature monte carlo simulations support proposed model selection criterion finite temperature annealing introduction learning algorithms designed extract structure data two classes algorithms widely discussed literature supervised unsupervised learning distinction two classes depends supervision teacher information either available learning algorithm missing paper applies statistical learning theory problem unsupervised learning particular error bounds protection fitting derived recently developed asymmetric clustering model acm co occurrence data theoretical results show continuation method deterministic annealing yields robustness learning results sense statistical learning theory computational temperature annealing algorithms plays role control parameter regulates complexity learning machine let us assume hypothesis class loss functions given loss functions measure quality structures data complexity controlled coarsening define cover informally inference principle advocated us performs learning two inference steps determine optimal approximation level consistent learning terms large risk deviations ii given optimal approximation level average hypotheses appropriate neighborhood empirical minimizer result inference work supported german israel foundation science research development gif grant model selection clustering uniform convergence bounds procedure single hypothesis set hypotheses set represented either average loss functions alternatively typical member set induction approach named empirical risk approximation era reader note learning algorithm return average structure typical cover sense supposed return hypothesis minimal empirical risk vapnik empirical risk minimization erm induction principle classification regression loss function minimal empirical risk usually structure maximal complexity clustering erm principle necessarily yield solution maximal number clusters erm principle therefore suitable model selection principle determine number clusters stable sample fluctuations era principle approximation accuracy solves problem controlling effective complexity hypothesis class spirit approach similar gibbs algorithm presented example gibbs algorithm samples random hypothesis version space predict label lth data point xt version space defined set hypotheses consistent first given data points approach use alternative definition consistency hypothesis appropriate neighborhood empirical minimizer define version space see also averaging neighborhood yields structure risk equivalent expected risk obtained random sampling set hypotheses exists also tight methodological relationship learning curves learning two class classifiers derived using techniques statistical mechanics empirical risk approximation principle data samples zr analyzed unsupervised learning algorithm elements suitable object resp feature space samples distributed according measure assumed known analysis mathematically precise statement era principle requires several definitions formalize notion searching structure data quality structures extracted data set evaluated empirical risk zr structure given training set function known loss function statistics measures costs processing generic datum model value parameterizes individual loss function denoting set possible parameters loss function minimizes empirical risk denoted argminae relevant quality measure learning expected risk fa optimal structure inferred data argminae distribution assumed decay sufficiently fast bounded rth moments lb denote expectation variance random variable respectively distribution dependent constant era requires learning algorithm determine set hypotheses basis finest consistently learnable cover hypothesis class given learning accuracy subset parameters el ala defined hypothesis class covered function balls index sets fa ih dp jaea em knowledge covering numbers required following analysis weaker type information complete knowledge probability measure see also buhmann andm hem pirical minimizer added cover simplify bounding arguments large deviation theory used determine approximation accuracy learning hypothesis hypothesis class expected risk empirical minimizer exceeds global minimum expected risk ca probability bounded bernstein inequality sup exp complexity lay coarsened hypothesis class small enough guarantee high confidence small deviations large deviation inequality weighs two competing effects learning problem probability large deviation exponentially decreases growing sample size whereas large deviation becomes increasingly likely growing cardinality cover hypothesis class according sample complexity defined iavl og probability deviation empirical risk expected risk bounded pt pp averaging set functions exceed empirical minimizer pp empirical risk yields average hypothesis corresponding statistically significant structure data since definition key task following remains calculate minimal precision function approximation bound cardinality av cover specific learning problems asymmetric clustering model asymmetric clustering model developed analysis resp grouping objects characterized co occurrence objects certain feature values application domains explorerive data analysis approach example texture segmentation statistical language modeling document retrieval denote product space objects xi features yj xi characterized observations zr xi yj sufficient statistics often object feature pair xi yj occurs data set measured set frequencies rli number observations xi total number observations derived measurements frequency observing object xi ij frequency observing feature yj given object xi li rli rli asymmetric clustering model defines generarive model finite mixture component probability distributions feature space cluster conditional distributions qjlv see introduce indicator variables mir membership object xi cluster mi enforces uniqueness constraint assignments maximal standard deviation sup defines scale measure deviations empirical risk expected risk see model selection clustering uniform convergence bounds using variables observed data distributed according generative model xi yjlm mi qjl analysis unknown data source characterized least approximatively empirical data structure xk inferred aim acm analysis group objects xi coded unknown indicator variables uiv estimate cluster prototypical feature distribution qjlv using loss function xi yj logn mir logqjlv maximization likelihood formulated minimization empirical risk rlijh xi yj essential quantity minimized expected risk ptrue xi yj xi yj using maximum entropy principle following annealing equations derived zill miv ij miv qjlv mir may rljli exp ljli log qjlv critical temperature due limited precision observed data natural study histogram clustering learning problem hypothesis class logqjlv vm qylv lv limited number observations results limited precision frequencies jli value qjl excluded since causes infinite expected risk pt yj ix size regul ized hypothesis class upper bounded cardinality complete hypothesis cl divided minimal cardinality function ball centered function cover ia din ity hnction ball radius ff appro mated adopting techniques asymptotic ysis yy ix log dql entropy given zvvz lv zi log exp auxiliary variables lagr ge ameters enforce normalizations qjl choosing qjla qj obtain approximation inte al reader note saddlepoint appro mation buhmann hem usual sense applicable parameter fail parameters since integrand maximal non differentiability point absolute value function therefore expand linear terms integrate piece wise following saddle using abbreviation niv ptrue yj xi log os point approximation integral obtained exp ia lpi pi exp iu entropy evaluated yields combination laplace approximation estimate dinality cover logjam logk zi pnippip pi ni nip second term results second order term taylor expansion ound saddle point serting complexity equation yields equation determines required number samples fixed precision confidence equation defines function relationship precision approximation qu ity fixed sample size confidence sumption precision depends non monotone hion loc rc using abbreviation log avl log minimum function defines compromise uncertainty originating empirical fluctuations loss precision due approximation cover differentiating respect setting result zero de yields upper bound inverse temperature lo cr oc analogous estimates means phase transitions occur acm lowering temperature mixture model data hand partitioned components revealing finer finer details generation process critical opt defines resolution limit details resolved reliable fashion basis sample size given inverse temperature effective cardinality hypothesis class upper bounded via solution fix point equation hand cardinality defines sample size upper bound iterating two steps finally obtain upper bound critical inverse temperature given sample size empirical results evaluation derived theoretical result series monte carlo experiments artificial data performed asymmetric clustering model given number objects number groups size histograms generative model experiments created randomly summarized fig generative model sample sets arbitrary size generated true distributions ptrue yjlxi calculated figure predicted temperatures compared empirically observed critical temperatures estimated basis different samples randomly generated co occurrence data expected risk solid model selection clustering uniform convergence bounds qjl figure generatire acm model monte carlo experiments empirical risk dashed inferred models averaged overfitting sets expected risk rises function inverse temperature figure indicates average minimal expected risk assumed effective number smaller equal number clusters true generatire model predicting right computational temperature therefore also enables data analyst solve cluster validation problem asymmetric clustering model especially sample fluctuations permit estimate five clusters minimal computational temperature prevents inference result hand minimal temperature prevents algorithm infer many clusters would instance overfitting interesting point one note infinite number observations critical inverse temperature reaches finite positive value five effective clusters extracted point conclude case histogram clustering empirical risk approximation solves realizable rules problem model validation choosing right number clusters figure summarizes predictions critical temperature basis empirical distribution ij rather true distribution ptrue xi yj empirical distribution generated training sample set eq used plug estimator histogram depicts predicted inverse temperature average plug estimators equal predicted temperature true distribution estimates biased towards small inverse temperatures due correlations parameter estimates stopping criterion still open question focus ongoing work rigorously bound variance plug estimator empirically observe reduction variance expected risk occurring predicted temperature higher sample sizes conclusions two conditions empirical risk uniformly converge towards expected risk loss functions within pp range global empirical risk minimum considered inference process limits complexity underlying hypothesis class given number samples maximum entropy method widely employed deterministic annealing procedures optimization problems substantiated analysis solutions many clusters clearly overfit data generalize condition hypothesis class divided function balls size forces us stop stochastic search lower bound computational temperature another important result investigation fact choosing right stopping temperature annealing process avoids fitting also solves cluster validation problem realizable case acm possible inference many clusters using empirical risk functional suppressed buhmann held exp io emp io emp io emp inverse temperature io exp io io emp io oooem inverse temperature soo inveme temperature de bruijn asymptotic methods analysis north holland publishing co repr dover amsterdam buhmann empirical risk approximation technical report iai tr institut fiir informatik iii universit bonn haussler kearns schapire bounds sample complexity bayesian learning using information theory vc dimension machine learning haussler kearns seung tishby rigorous learning curve bounds statistical mechanics machine learning haussler opper mutual information metric entropy cumulative relative entropy risk annals statistics december hofmann puzicha jordan learning dyadic data kearns solla cobh editors advances neural information processing systems mit press appear seung sompolinsky tishby statistical mechanics learning examples physical review april van der vaart wellner weak convergence empirical processes springer verlag new york berlin heidelberg vapnik statistical learning theory wiley interscience new york references figure comparison theoretically derived upper bound observed critical temperatures minimum expected risk rs curve depicted plots vertical lines indicate predicted critical temperatures average effective number clusters drawn part part distribution plug estimates shown
abstract formulate model probability distributions image spaces show distribution images factored exactly conditional distributions feature vectors one resolution pyramid level conditioned image information lower resolutions would like factor positions pyramid levels make tractable factoring may miss long range dependencies fix introduce hidden class labels pixel pyramid result hierarchical mixture conditional probabilities similar hidden markov model tree model parameters found maximum likelihood estimation using em algorithm obtained encouraging preliminary results problems detecting various objects sar images target recognition optical aerial images introduction many approaches object recognition images estimate pr class image contrast model probability distribution images promage many attractive features could use object recognition usual way training distribution object class using bayes rule get pr classlimage pr mage class pr class pr mage clearly many benefits model distribution images since kind data analysis task approached using knowledge distribution data classification could attempt detect unusual examples reject rather trusting classifier output could also compress interpolate suppress noise extend resolution fuse multiple images etc many image analysis algorithms use probability concepts treat distribution images zhu wu mumford computing maximum entropy distribution given set statistics features seems work well textures clear well model appearance structured objects several algorithms modeling distributions features extracted image instead image markov random field mrf models example line development see unfortunately tend expensive computationally de bonet viola flexible histogram approach features extracted multiple image scales resulting feature vectors treated set independent hierarchical image probability hip models gaussian feature pyramid pyramid go subsampled figure pyramids feature notation samples drawn distribution model distribution feature vectors parzen windows given good results feature vectors neighboring pixels treated independent fact share exactly components lower resolutions fix might want build model features one pixel one pyramid level condition features several child pixels next higher resolution pyramid level multiscale stochastic process msp methods exactly luettgen willsky example applied scale space auto regression ar model texture discrimination use quadtree quadtree like organization pixels image pyramid model features pyramid stochastic process coarse fine levels along tree variables process hidden observations sums hidden variables plus noise gaussian distributions limitation msp models result also model probability observations tree image methods seem well suited modeling texture unclear might build models capture appearance structured objects argue presence objects images make local conditioning like flexible histogram msp approaches inappropriate following present model probability distributions images try move beyond texture modeling hierarchical image probability hip model similar hidden markov model tree learned em algorithm preliminary tests model classification tasks performance comparable algorithms coarse fine factoring image distributions goal write image distribution form similar pr pr fo pr fl set feature images pyramid level expect short range dependencies captured model distribution individual feature vectors long range dependencies captured somehow low resolution large scale structures affect finer scales conditioning fact prove coarse fine factoring like correct image build gaussian pyramid repeatedly blur subsample gaussian filter call th level original image io figure gaussian level extract set feature images ft sub sample get feature images gr note images gt dimensions denote set images containing images gr denote mapping suppose io invertible think change vari spence parra ables distribution space expressions two different coordinate systems related multiplying jacobian case get pr io pr since go factor pr get pr io pr go pr invertible simply repeat change variable factoring procedure get general result valid pr doubt rather mild restrictions make change variables valid restriction invertible strong many feature sets known exist wavelet transforms images know ways condition relaxed work needed need hidden variables sake tractability want factor pr ii positions something like pr il pr gl ft feature vectors position dependence gl expresses persistence image structures across scale edge usually detectable several neighboring pyramid levels flexible histogram msp methods share structure may plausible strong influence gl argue factorization conditioning enough capture properties real images objects world cause correlations non local dependencies images example presence particular object might cause certain kind texture visible level usually local features contain enough information infer object presence entire image ii layer might thus gl influenced ii local feature vector similarly objects create long range dependencies example object class might result kind texture across large area image object class always present distribution may factor objects always present inferred lower resolution information presence texture one location affects probability presence elsewhere introduce hidden variables represent non local information captured local features also constrain variability features next finer scale denoting collectively assume conditioning allows distributions feature vectors factor general distribution images becomes pr gt ft pr pr il written absolutely general need specific particular would like preserve conditioning higher resolution information coarser resolution information ability factor positions hierarchical image probability hip models al al ai figure tree structure conditional dependency hidden variables hip model subsampling two sometimes called quadtree structure first model chosen following structure hip model pr oc pr gt ft pr lat ao ar zeit position level attach hidden discrete index label resulting label image level dimensions images since codes non local information think labels segmentation classification th pyramid level conditioning al mean conditioned parent pixel parent child relationship follows sub sampling operation example sub sample two direction get gt ft condition variable level al location level figure gives dependency graph hidden variables tree structure probabilistic tree discrete variables sometimes referred belief network conditioning child labels parents information propagates though layers areas image accumulating information along way sake simplicity chosen pr al normal mean mat covariance eat also constrain mat eat diagonal em algorithm thanks tree structure belief network hidden variables relatively easy train em algorithm expectation step summing performed directly chosen densely connected structure child several parents would need either approximate algorithm monte carlo techniques expectation weighted probability label parent child pair labels given image computed fine coarse fine procedure working leaves root back leaves method based belief propagation care efficient algorithm worked omit details due space constraints compute expectations normal distribution makes step tractable simply compute updated gat eat mat pr combinations various expectation values proportionality factor includes pr al il model pr pr factor equation read quantities al spence parra hip hpniq ane rocareas ooofigure examples aircraft rois right values jack knife study detection performance hip hpnn models figure sar images three types vehicles detected experiments applied hip problem detecting aircraft aerial photograph logan airport simple template matching algorithm used select forty candidate aircraft twenty false positives figure ten plane examples used training one hip model ten negative examples used train another thesmall number examples performed jack knife study ten random splits data features used filter kernels polynomials third order multiplying gaussians hip pyramid used subsampling three direction test set roc area hip mean az hpnn algorithm gave mean individual values shown figure compared hpnn given larger set aircraft images including different set features subsampling two also performed experiment three target classes mstar public targets data set compare results flexible histogram approach de bonet et al trained three hip models one target vehicles bmp btr figure trained model ten images class one image ten aspect angles spaced approximately apart trained one model ten images target whereas de bonet et al trained one model per image first tried discriminating vehicles one class objects thresholding log pr class model objects used tests objects taken test data two vehicle classes plus seven vehicle classes hierarchical image probability hip models roc using pr target bmp az az btr az false roc uam pr targel pr target jj bmp vs az oll bmp vsbtr az os figure roc curves vehicle detection sar imagery roc curves thresholding hip likelihood desired class roc curves inter class discrimination using ratios likelihoods given hip models image seven classes bmp test images btr test images test images resulting roc curves shown figure tried discriminating pairs target classes using hip model likelihood ratios log pr classl log pr class could use extra seven vehicle classes resulting roc curves shown figure performance comparable flexible histogram approach conditional distributions features test hip model fit image distribution computed several distributions features gl conditioned parent feature fl empirical computed distributions particular parent child pair features shown figure conditional distributions examined similar appearance fit empirical distributions well buccigrossi simoncelli reported bow tie shape conditional distributions variety features want point conditional distributions naturally obtained mixture gaussian distributions varying scales zero means present hip model learns conditionals effect describing features non stationary gaussian variables conclusion developed class image probability models call hierarchical image probability hip models justify showed image distributions exactly represented products pyramid levels distributions sub sampled feature images conditioned coarser scale image information argued hidden variables needed capture long range dependencies allowing us factor distributions position current model hidden variables act indices mixture somewhat involved pr gt ft pr gt ft pr summed eat pr gt ft pr gt ft pr ft spence parra conditional dislirbution data conditional distirbution hip model feature layer feature layer figure empirical hip estimates distribution feature conditioned parent feature ft components resulting model somewhat like hidden markov model tree early results classification problems showed good performance acknowledgements thank jeremy de bonet john fisher kindly answering questions work experiments supported united states government
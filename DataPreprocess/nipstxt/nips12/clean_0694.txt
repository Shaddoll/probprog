abstract propos new markov chain mont carlo algorithm gener stochast dynam method algorithm perform explor state space use intrins geometr structur facilit effici sampl complex distribut appli bayesian learn neural network algorithm found perform least well best state art method consum consider less time introduct bayesian framework predict made integr function interest posterior paramet distribut latter normal product prior distribut likelihood sinc problem integr complex calcul analyt approxim need earli work bayesian learn nonlinear model buntineand weigend mackay use gaussian approxim posterior paramet distribut howev gaussian approxim may poor especi complex model multi modal charact posterior distribut hybrid mont carlo hmc duan et al introduc neural network commun neal deal success multi modal distribut time consum one main caus hmc ineffici anisotrop charact posterior distribut densiti chang rapidli direct remain almost constant other present novel algorithm overcom problem use intrins geometr structur model space hybrid mont carlo markov chain mont carlo mcmc gilk et al approxim valu manifold stochast dynam bayesian learn mean success state ergod markov chain invari distribut addit ergod invari anoth qualiti would like markov chain rapid explor state space first two qualiti rather easili attain achiev rapid explor state space often nontrivi state art mcmc method capabl sampl complex distribut hybrid mont carlo duan et al algorithm express term sampl canon distribut state physic system defin term energi function oc exp allow use dynam method momentum variabl introduc dimension canon distribut phase space defin oc exp hamiltonian repres total energi kinet energi due momentum defin mi momentum compon mi mass associ th compon differ compon given differ weight sampl canon distribut done use stochast dynam method andersen task split two subtask sampl uniformli valu fix total energi sampl state differ valu first task done simul hamiltonian dynam system dqi pi dr opi dpi oh oe dr oqi qi differ energi level obtain occasion stochast gibb sampl geman geman momentum sinc independ may updat without refer draw valu probabl densiti proport exp case easili done sinc pi independ gaussian distribut practic hamiltonian dynam cannot simul exactli approxim discret use finit time step one common approxim leapfrog discret neal hybrid mont carlo method stochast dynam transit use gener candid state metropoli algorithm metropoli et al elimin certain note probabl densiti nowher zero put form simpli defin log log conveni zlochin baram drawback stochast dynam systemat error due leapfrog discret sinc metropoli algorithm ensur everi transit keep canon distribut invari howev empir comparison uncorrect stochast dynam hmc applic bayesian learn neural network neal show appropri discret stepsiz notabl differ two method modif propos horowitz instead gibb sampl momentum replac time co sin small angl distribut accord keep canon distribut invari scheme call momentum persist improv rate explor riemannian geometri riemannian manifold amari set equip metric tensor posit semidefinit matrix defin inner product infinitesim increment let us denot entri gi entri inner product natur give us norm ii jeffrey prior defin densiti function denot determin hamiitonian dynam manifold riemannian manifold dynam take gener form one describ section metric tensor mass set one hamiltonjan given dynam govern follow set differenti equat chavel oe qi gi oqj fj qiqj dr christoffel symbol given og og gi oq oq dq rr relat lp manifold stochast dynam bayesian learn riemannian geometri function regress log likelihood proport empir error simpli euclidean distanc target point candid function evalu sampl therefor natur distanc measur model euclidean seminorm ii fox fo ii xi xi result metric tensor vof zi xi jr denot gradient jacobian matrix bayesian geometri bayesian approach would suggest inclus prior assumpt paramet manifold geometri exampl priori log posterior written logp oix xi ok invers nois varianc therefor natur metric model space xi zi metric tensor gb jt extend jacobian kronek delta note gb henc prior becom vaguer approach nonbayesian paradigm hand bayesian geometri approach euclidean geometri paramet space qualiti would like bayesian geometri prior strong comparison likelihood exact form littl import definit appli log concav prior distribut invers hessian log prior logp replac framework restrict regress gener distribut class natur use fisher inform matrix metric tensor amari bayesian metric tensor becom gb logp zlochin baram manifold stochast dynam mention energi landscap mani regress problem anisotrop degrad perform hmc two aspect dynam may optim effici explor posterior distribut suggest studi gaussian diffus hwang et al result differenti equat stiff gear lead larg discret error turn necessit small time step impli comput burden high problem disappear instead euclidean hamiltonian dynam use hmc simul dynam manifold equip metric tensor gb propos previou section context regress definit gb jt obtain altern deq equat matrix form deq roj dr rrq canon distribut oc exp condit distribut given zero mean gaussian covari matrix margin distribut proport exp equival multipli prior jeffrey prior sampl canon distribut two fold simul hamiltonian dynam one time step use leapfrog discretis replac use momentum persist unlik hmc case momentum perturb distribut accord actual weight multipli matric may chosen differ specifi improv numer stabil empir comparison robot arm problem compar perform manifold stochast dynam msd algorithm standard hmc comparison carri use mackay robot arm problem common benchmark bayesian method neural network mackay neal robot arm problem concern map yl co co el sin sin el independ gaussian nois variabl standard deviat dataset use neal mackay contain exampl train set test set fact sinc actual prior weight unknown truli bayesian approach would use non inform prior rr paper kept modifi prior product rr zero mean gaussian manifold stochast dynam bayesian learn msd figur averag run autocorrel input hidden left hiddento output right weight hmc leapfrog step per iter msd singl leapfrog step per iter horizont axi give lag measur number iter use neural network two input unit one hidden layer contain tanh unit two linear output unit hyperparamet set correct valu chosen algorithm compar msd two version hmc leapfrog step per iter henceforth refer hmc hmc msd run singl leapfrog step per iter three algorithm momentum resampl use persist co singl iter hmc requir float point oper flop hmc requir flop msd requir flop henc comput load msd one third hmc time lower hmc discret stepsiz hmc chosen keep reject rate equival criterion averag error hamiltonian around use msd three sampl algorithm run time time iter first sampl discard order allow algorithm reach region high probabl result one appropri measur rate state space explor weight autocorrel neal shown figur behavior msd clearli superior hmc anoth valu interest total squar error test set predict test set made follow subsampl paramet vector wag gener take everi twentieth sampl vector start predict valu zlochin baram averag empir function distribut subsampl total squar error normal respect varianc test case follow statist run averag standard deviat hmc hmc msd averag error hmc high indic algorithm fail reach region high probabl error hmc msd compar standard deviat msd twice low hmc mean estim obtain use msd reliabl conclus describ new algorithm effici sampl complex distribut appear bayesian learn non linear model empir comparison show algorithm achiev result superior best achiev exist algorithm consider smaller comput time
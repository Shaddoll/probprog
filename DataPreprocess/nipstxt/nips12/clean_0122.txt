abstract first show represent sharp posterior probability distributions using real valued coefficients broadly tuned basis functions show precise times spikes used convey real valued coefficients basis functions quickly accurately finally describe simple simulation spiking neurons learn model image sequence fitting dynamic generafive model population codes energy landscapes perceived object represented brain activities many neurons general consensus activities individual neurons combine represent multiple properties object start focussing case single object multiple instantiation parameters position velocity size orientation assume neuron ideal stimulus space instantiation parameters activation rate probability activation falls monotonically directions actual stimulus departs ideal semantic problem define exactly instantiation parameters represented activities many neurons specified hinton rumelhart mcclelland consider binary neurons receptive fields convex instantiation space assume object present activates neurons whose receptive fields instantiation parameters lie consequently known one object present parameter values object must lie within feasible region formed intersection receptive fields active neurons called conjunctive distributed representation assuming receptive field occupies small fraction whole space interesting property type coarse coding bigger receptive fields accurate representation however large receptive fields lead loss resolution several objects present simultaneously sensory input noisy impossible infer exact parameters objects makes sense perceptual system represent probability distribution across parameters rather single best estimate feasible region full probability distribution essential correctly combining inforspiking boltzmann machines figure energy landscape onedimensional space neuron adds dimple dotted line energy landscape solid line corresponding probability density dimples overlap corresponding probability density becomes sharper since dimples decay zero location sharp probability peak affected distant dimples multimodal distributions represented marion different times different sources one obvious way represent distribution anderson van essen allow neuron represent fairly compact probability distribution space instantiation parameters treat activity levels neurons unnormalized mixing proportions semantics disjunctive distributed representation precise percepts allows impossible represent distributions sharper individual receptive fields high dimensional spaces individual fields must broad order cover space disjunctive representations used kohonen self organizing map restricted low dimensional latent spaces disjunctive model viewed attempt approximate arbitrary smooth probability distributions adding together probability distributions contributed active neuron coarse coding suggests multiplicative approach addition done domain energies negative log probabilities active neuron contributes energy landscape whole space instantiation parameters activity level neuron multiplies energy landscape landscapes neurons population added figure example neuron full covariance gaussian tuning function energy landscape parabolic bowl whose curvature matrix inverse covariance matrix activity level neuron scales inverse covariance matrix instantiation parameters real numbers required span space means inverse covariance matrices real valued activities neurons sufficient represent arbitrary full covariance gaussian distributions space instantiation parameters treating neural activities multiplicative coefficients additive contributions energy landscapes number advantages unlike disjunctive codes vague distributions represented low activities significant biochemical energy required distributions quite sharp central operation bayesian inference combine prior term likelihood term combine two conditionally independent likelihood terms trivially achieved adding two energy landscapes thank zoubin ghahramani pointing another important operation convolving probability distribution gaussian noise difficult non linear operation energy landscape hinton brown representing coefficients basis functions perform perception video rates probability distributions instantiation parameters need represented frames per second seems difficult using relatively slow spiking neurons requires real valued multiplicative coefficients basis functions communicated accurately quickly using none spikes trick realise spike arrives another neuron produces postsynaptic potential smooth function time perspective postsynaptic neuron spike convolved smooth temporal function adding number smooth functions together appropriate temporal offsets possible represent smoothly varying sequence coefficient values basis function makes possible represent temporal evolution probability distributions shown figure ability vary location spike single dimension time thus allows real valued control representation probability distributions multiple spatial dimensions encoded value time time neuron neuron figure two spiking neurons centered represent time varying mean standard deviation single spatial dimension spikes first convolved temporal kernel resulting activity values treated exponents gaussian distributions centered ratio activity values determines mean sum activity values determines inverse variance method used two spatial dimensions time flows top bottom spike makes contribution energy landscape resembles hourglass thin lines waist hourglass corresponds time spike strongest effect post synaptic population moving hourglasses time possible get whatever temporal cross sections desired thick lines provided temporal sampling rate comparable time course effect spike proposed use spike timing convey real values quickly accurately require precise coincidence detection sub threshold oscillations modifiable time delays paraphernalia invoked explain brain could make effective use single real valued degree freedom timing spike hopfield coding scheme proposed would far convincing could show learned could demonstrate effective simulation two ways design learning algorithm spiking neurons could work relatively low dimensional space instantiation parameters design learning produce right representations interactions representations space could treat space implicit emergent property network design learning algorithm optimize spiking boltzmann machines objective function much higher dimensional space neural activities hope create representations understood using implicit space instantiation parameters chose latter approach learning algorithm restricted boltzmann machines hinton describes learning algorithm probabilistic generative models composed number experts expert specifies probability distribution visible variables experts combined multiplying distributions together renormalizing ilmpm dlom dlo ilmpm cilom data vector discrete space parameters individual model pm dlom probability model index possible vectors data space coding scheme described product experts spike expert first summarize product experts learning rule restricted boltzmann machine rbm consists layer stochastic binary visible units connected layer stochastic binary hidden units intralayer connections extend rbm deal temporal data rbm hidden unit expert specifies uniform distribution states visible units weight visible unit specifies log odds visible unit multiplying together distributions specified different hidden units achieved adding log odds inference rbm much easier causal belief net explaining away hidden states sj conditionally independent given visible states si distribution sj given standard logistic function sj wijsi conversely hidden states rbm marginally dependent easy rbm learn population codes units may highly correlated hard causal belief nets one hidden layer generative model causal belief net assumes marginal independence rbm trained following gradient log likelihood data awij sisj sisj expected value data clamped visible units hidden states sampled conditional distribution given data expected value sisj prolonged gibbs sampling alternates sampling conditional distribution hidden states given visible states vice versa learning rule work well sampling noise estimate swamps gradient far effective maximize difference log likelihood data log likelihood one step reconstructions data produced first picking binary hidden states conditional distribution given data picking binary visible states conditional distribution given hidden states gradient log hinton brown likelihood one step reconstructions complicated changing weight changes probability distribution reconstructions ol oq ol cqwij sisj sisj owij oq distribution one step reconstructions training data equilibrium distribution stationary distribution prolonged gibbs sampling fortunately cumbersome third term sufficiently small ignoring prevent vector weight changes positive cosine true gradient difference log likelhoods following simple learning rule works much better eq awij sisj sisj restricted boltzmann machines time using restricted boltzmann machine represent time spatializing taking visible unit hidden unit replicating time constraint weight wij replica replica depend implement desired temporal smoothing also force weights smooth function shape temporal kernel shown figure remaining degree freedom weights replicas replicas scale temporal kernel scale learned replicas visible hidden units still form bipartite graph probability distribution hidden replicas inferred exactly without considering data lies future width temporal kernel one problem restricted boltzmann machine spatialize time hidden units one time step memory states previous time steps see data add undirected connections hidden units different time steps architecture would return fully connected boltzmann machine hidden units longer conditionally independent given data useful trick borrowed elman nets allow hidden units see previous states treat observations like data cannot modified future hidden states thus hidden states may still inferred independently without resorting gibbs sampling connections hidden layer weights also follow time course temporal kernel connections act predictive prior hidden units important note forward connections required network model sequence purposes extrapolating future figure form temporal kernel spiking boltzmann machines probability sj given states visible units sj er wijhi hi convolution history visible unit temporal kernel tand convolution hidden unit history computed similarly learning weights follows immediately formula inference positive phase visible units clamped time step posterior hidden units conditioned data computed assume zero boundary conditions time negative phase sample posterior hidden units compute distribution visible units time step given hidden unit states phase correlations hidden visible units computed learning rule results trained network sequence synthetic images gaussian blob moving circular path following diagrams display time sequence images matrix row matrix represents single image pixels stretched vector scanline order column time course single pixel intensity pixel represented area white patch used hidden units figure shows segment time steps time series used training sequence period blob time steps figure shows trained model reconstructs data sample hidden layer units trained model possible forecasting clamping visible layer units segment sequence iterative gibbs sampling generate future points sequence figure shows given time steps series model predict reasonably far future pattern dies one problem simulations treating real valued intensities images probabilities works blob images values viewed probabilities pixels binary image true natural images discussion initial simulations used causal sigmoid belief network sbn rather restricted boltzmann machine inference sbn much difficult rbm requires gibbs sampling severe approximations even temporal kernel used ensure replica hidden unit one time computing conditional probability distribution visible units given hidden states done similar fashion caveat weights direction must symmetric thus convolution done using reverse kernel hinton brown figure original data reconstruction data prediction data given time steps sequence black line indicates prediction begins connections replicas visible units different times posterior distribution hidden units still depends data far future gibbs sampling made sbn simulations slow sampling noise made learning far less effective rbm although rbm simulations seem closer biological plausibility suffer major problem apply learning procedure necessary reconstruct data hidden states know without interfering incoming datastream simulations simply ignored problem allowing visible unit observed value reconstructed value time acknowledgements thank zoubin ghahramani peter dayan rich zemel terry sejnowski radford neal helpful discussions research funded grants gatsby foundation nserc
abstract new decomposition algorithm training regression support vector machines svm presented algorithm builds basic principles decomposition proposed osuna et al addresses issue optimal working set selection new criteria testing optimality working set derived based criteria principle maximal inconsistency proposed form approximately optimal working sets experimental results show superior performance new algorithm comparison traditional training regression svm without decomposition similar results previously reported decomposition algorithms pattern recognition svm new algorithm also applicable advanced svm formulations based regression density estimation integral equation svm introduction increasing interest applications support vector machines svm largescale problems ushers new requirements computational complexity training algorithms requests recently made algorithms capable handling problems containing examples training svm constitutes quadratic programming problem typical svm package uses shelf optimization software obtain solution number variables optimization problem equal number training data points pattern recognition svm twice number regression svm speed general purpose optimization methods insufficient problems containling thousand examples motivated quest special purpose training algorithms take advantage particular structure svm training problems main avenue research svm training algorithms decomposition key idea decomposition due osuna et al freeze small number optimization variables solve sequence small fixed size problems set variables whose values optimized current iteration called working set complexity optimizing working set assumed constant time improved decomposition algorithm regression support vector machines order decomposition algorithm successful working set must selected smart way fastest known decomposition algorithm due joachims based zoutendijk method feasible directions proposed optimization community early however joachims algorithm limited pattern recognition svm makes use labels current article presents similar algorithm regression svm new algorithm utilizes slightly different background optimization theory karush kuhn tucker theorem used derive conditions determining whether given working set optimal conditions become algorithm termination criteria alternative osuna criteria also used joachims without modification used conditions individual points advantage new conditions knowledge hyperplane constant factor cases difficult compute required investigation new termination conditions allows form strategy selecting optimal working set new algorithm applicable pattern recognition svm provably equivalent joachims algorithm one also interpret new algorithm sense method feasible directions experimental results presented last section demonstrate superior performance new method comparison traditional training regression svm general principles regression svm decomposition original decomposition algorithm proposed pattern recognition svm extended regression svm sake completeness repeat main steps extension aim providing terse streamlined notation lay ground working set selection given training data size training regression svm amounts solving following quadratic programming problem variables maximize td subject el basic idea decomposition split variable vector working set fixed size non working set containing rest variables corresponding parts vectors also bear subscripts matrix partitioned dbb dbn vn requirement th element training data either included omitted working set values variables non working set frozen iteration optimization performed respect variables working set optimization working set also quadratic program seen arranging terms objective function equality constraint rule facilitates formulation sub problems solved iteration laskov dropping terms independent objective quadratic program sub problem formulated follows resulting maximize wb vdnb bdbb subject basic decomposition algorithm chooses first working set random proceeds iteratively selecting sub optimal working sets optimizing solving quadratic program subsets size optimal precise formulation termination conditions developed following section optimality working set order maintain strict improvement objective function working set must sub optimal optimization classical karush kuhn tucker kkt conditions necessary sufficient optimality quadratic program use conditions applied standard form quadratic program described standard form quadratic program requires constraints equality type except non negativity constraints cast ression svm quadratic program standard form slack variables corresponding box constraints following matrices introduced vector length vector length zero element vector reflects fact slack variable equality constraint must zero matrix notation constraints problem compactly expressed tz notation karush kuhn tucker theorem stated follows theorem karush kuhn tucker theorem primal vector solves quadratic problem satisfies exists dual vector ur ii ii ch ii ew urz follows karush kuhn tucker theorem satisfying conditions system inequalities inconsistent solution problem optimal since objective function sub problem obtained merely arranging terms objective function initial problem conditions guarantee sub problem optimal thus main strategy identifying sub optimal working sets enforce inconsistency system satisfying conditions improved decomposition algorithm regression support vector machines let us analyze inequalities inequality one following forms vi la cfii yi ctj ctj kij consider values cti possible take ctl case si complementarity condition vi inequality becomes ta cti complementarity condition ri inequality becomes tz tz cti complementarity condition vi ri inequality becomes similar reasoning ct inequality yields following results ct ct ct tz one see free variable system inequality restricts certain interval real line intervals denoted la sets rest exposition subset inequalities inconsistent intersection corresponding sets empty provides lucid rule determining optimality working set sub optimal intersection sets points empty sub optimal working set also denoted inconsistent following summarizes rules calculation sets taking account regression svm ic li ifcti ct cti ct ifcti ai cti ct ifcti ct lo laskov maximal inconsistency algorithm inconsistency working set iteration guarantees convergence decomposition rate convergence quite slow arbitrary inconsistent working sets chosen natural heuristic select maximally inconsistent working sets hope choice would provide greatest improvement objective function notion maximal inconsistency easy define let gap smallest right boundary largest left boundary sets elements training set max ti min ti left right boundaries respectively possibly minus plus infinity set li convenient require largest possible inconsistency gap maintained pairs points comprising working set obvious implementation strategy select elements largest values elements smallest values maximal inconsistency strategy summarized algorithm algorithm maximal inconsistency svm decomposition algorithm let list samples compute li according rules elements select elements largest values left pass select elements smallest values right pass optimize working set although motivation provided maximal inconsistency algorithm purely heuristic algorithm rigorously derived similar fashion joachims algorithm zoutendijk feasible direction problem details derivation cannot presented due space constraints relationship refer algorithms feasible direction algorithms experimental results experimental evaluation new algorithm performed modified kdd cup data set original data set available http www ics uci edu kdd databases kddcup kddcup html following modifications made obtain pure regression problem character fields eliminated numeric fields controln odatedw tcode dob elimitated remaining features labels scaled initial subsets training database different sizes selected evaluation scaling properties new algorithm training times algorithms without decomposition numbers support vectors including bounded support vectors experimental scaling factors displayed table improved decomposition algorithm regression support vector machines table training time sec number svs kdd cup problem examples dcmp dcmp total sv bsv scaling factor sv scaling factor table training time sec number svs kdd cup problem reduced examples dcmp dcmp total sv bsv feature space scaling factor sv scaling factor experimental scaling factors obtained fitting lines log log plots running times sample sizes number examples number unbounded support vectors respectively experiments run sgi octane mhz clock ram rbf kernel termination accuracy working set size cache size samples used similar experiment performed reduced feature set consisting first features selected full size data set experiment illustrates behavior algorithms large number support vectors bounded results presented table discussion comes surprise decomposition algorithm outperforms conventional training algorithm order magnitude similar results well established pattern recognition svm remarkable co incidence scaling factors maximal inconsistency algorithm joachims algorithm scaling factors range believe however important performance measure sv scaling factor results suggest factor consistent even problems significantly different compositions support vectors experiments investigate properties measure finally would like mention methods proposed order speed training svm although experimental results reported methods regard training regression svm chunking iterates laskov training data accumulating support vectors adding chunk new data changes solution occur main problem method percentage support vectors high essentially solves problem almost size sequential minimal optimization smo proposed platt easily extendable regression svm employs idea similar decomposition always uses working set size working set solution calculated hand without numerical optimization number heuristics applied order choose good working set difficult draw comparison working set selection mechanisms smo feasible direction algorithms experimental results joachims suggest smo slower another advantage feasible direction algorithms size working set limited smo practical experience shows optimal size working set lastly traditional optimization methods newton conjugate gradient methods modified yield complexity number detected support vectors considerable improvement methods complexity total number training samples real challenge lies attaining sub complexity experimental results suggest feasible direction algorithms might attain complexity complexity fully understood theoretical point view specifically convergence rate dependence number support vectors needs analyzed main direction future research feasible direction svm training algorithms references smola sch kopf tutorial support vector regression neurocolt technical report nc tr osuna freund girosi improved training algorithm support vector machines proceedings ieee nnsp amelia island fl joachims making large scale svm learning practical advances kernel methods support vector learning schslkopf burges smola eds mit press osuna support vector machines training applications ph dissertation operations research center mit boot quadratic programming algorithms anomalies applications north holland publishing company amsterdam vapnik estimation dependencies based empirical data springer verlag platt fast training support vector machines using sequential minimal optimization advances kernel methods support vector learning schslkopf burges smola eds mit press kaufman solving quadratic programming problem arising supportvector classification advances kernel methods support vector learning schslkopf burges smola eds mit press
abstract describe framework interpreting support vector machines svms maximum posterjori map solutions inference problems gaussian process priors provide intuitive guidelines choosing good svm kernel also assign evidence maximization optimal values parameters noise level cannot determined unambiguously properties map solution alone cross validation error illustrate using simple approximate expression svm evidence determined error bars svm predictions also obtained support vector machines probabilistic flamework support vector machines svms recently subject intense research activity within neural networks community tutorial introductions overviews recent developments see one open questions remains set tunable parameters svm algorithm methods choosing width kernel function noise parameter controls closely training data fitted proposed see also recently effect overall shape kernel function remains imperfectly understood error bars class probabilities svm predictions important safety critical applications example also difficult obtain paper suggest probabilistic interpretation svms could used tackle problems shows svm kernel defines prior functions input space avoiding need think terms high dimensional feature spaces also allows one define quantities evidence likelihood set hyperparameters kernel amplitude etc give simple approximation evidence maximized set hyperparameters evidence sensitive values individually contrast properties cross validation error deterministic solution depends product cko therefore used assign unambiguous value error bars derived sollich focus two class classification problems suppose given set training examples xi yi binary outputs yi corresponding two classes basic svm idea map inputs onto vectors high dimensional feature space ideally feature space problem linearly separable suppose first true among decision hyperplanes qb separate training examples obey yi xi qb xi dx dx set training inputs svm solution chosen one largest margin largest minimal distance training examples equivalently one specifies margin one minimizes squared length weight vector lw subject constraint yi xi problem linearly separable slack variables introduced measure much margin constraints violated one writes yi xi control amount slack allowed penalty term added objective function penalty coefficient training examples yi xi hence incur penalty others contribute yi xi gives svm optimization problem find minimize shifted hinge loss interpret svms probabilistically one regard defining negative log posterior probability parameters svm given training set first term gives prior exp llwl gaussian prior components uncorrelated unit variance chosen gaussian prior variance fiat prior implied recovered letting oe latent variable values qb rather individually appear second data dependent term makes sense express prior directly distribution joint gaussian distribution components covariances given qb qb svm prior therefore simply gaussian process gp functions covariance function qb zero mean correspondence svms gps noted number authors second term becomes negative log likelihood define probability obtaining output given rkllx exp cl yo set exp ensure probabilities never add value larger one likelihood complete data set rli yi xi xi input distribution remains essentially arbitrary point however likelihood function normalized llx exp cl exp cl probabilistic setting actually makes sense keep finite small training sets yi equal nonzero probability probabilistic methods support vector machines except remedy write actual probability model dio af posterior probability oid dio independent normalization factor af construction map value therefore svm solution simplest choice af normalizes independent af fdoo fdxo conceptually corresponds following procedure sampling first sample gp prior data point sample assign outputs probability respectively remaining probability know class probability restart whole process sampling new smallest inside gap functions many values gap less likely survive dataset required size built reflected dependent factor effective prior follows correspondingly likelihood ylx ylx xlo normalized input density influenced function reduced uncertainty gaps summarize eqs define probabilistic data generation model whose map solution argmax oid given data set identical standard svm effective prior gp prior modified data set size dependent factor likelihood defines conditional output distribution also input distribution relative arbitrary relevant properties feature space encoded underlying gp prior covariance matrix equal kernel log posterior model fdxdx il yio xi const lnp transformation differentiating non training inputs one sees maximum standard form iotiyik xi yio xi one respectively call training inputs xi last group marginal form subset support vectors xi sparseness svm solution often number support vectors comes fact hinge loss constant contrasts uses gp models classification see instead likelihood sigmoidal often logistic transfer function nonzero gradient everywhere used moreover noise free limit sigmoidal transfer function becomes step function map values tend trivial solution illuminates alternative point view margin shift hinge loss imt ortant svms within probabilistic framework main effect kernel svm classification change properties underlying gp prior true smaller actually higher gap model makes less intuitive sense sollich figure samples svm priors input space unit square plots samples underlying gaussian process prior greyscale plots represent output distributions obtained used likelihood model greyscale indicates probability black white exponential ornstein uhlenbeck kernel covariance function exp giving rough decision boundaries length scale reduced amplitude note sample prior corresponding new kernel grey uncertainty gaps given roughly io regions definite outputs black white widened first row squared exponential rbf kernel exp yielding smooth decision boundaries changing holding fixed taking new sample shows parameter sets typical length scale decision regions polynomial kernel absence clear length scale widely differing magnitudes bottom left top right corners square make kernel less plausible probabilistic point view probabilistic methods support vector machines fig illustrates samples three different types kernels effect kernel smoothness decision boundaries typical sizes decision regions uncertainty gaps clearly seen prior knowledge properties target available probabilistic framework therefore provide intuition suitable choice kernel note samples fig rather effective prior one finds however dependent factor nn change properties prior qualitatively evidence error bars beyond providing intuition svm kernels probabilistic framework discussed also makes possible apply bayesian methods svms example one define evidence likelihood data given model specified hyperparameters parameters defining follows fdoq dio factor naive evidence derived unnormalized likelihood model correction factor ensures normalized data sets crucial order guarantee optimization log evidence gives optimal hyperparameter values least average opper private communication clearly general depend separately actual vm solution hand map values seen depend product ck properties deterministically trained svm alone test cross validation error cannot therefore used determine resulting class probabilities unambiguously outline simple approximation naive evidence derived given integral log integrand additive constant integrating gaussian distributed dx intractable integral xi remains however progress made expanding log integrand around maximum xi non marginal training inputs equivalent laplace approximation first terms expansion quadratic deviations maximum give simple gaussian integrals remaining xi leading terms log integrand vary linearly near maximum couplings xi appear next quadratic order discarding terms subleading integral factorizes xi evaluated end result calculation yic io xi yio xi ln det lmkra first three terms represent maximum log integrand ln dlo last one comes integration fluctuations note contains information marginal training inputs era corresponding submatrix lra diagonal matrix entries aquantitative changes arise function values discouraged large tends increase size decision regions narrow uncertainty gaps verified comparing samples ollich ix figure toy example evidence maximization left target latent function solid line svm rbf kernel exp cko trained dashed line training examples circles keeping cko constant evidence top right evaluated function using note normalization factor shifts maximum towards larger values naive evidence bottom right class probability target solid prediction evidence maximum dashed target generated ai ai given sparseness svm solution matrices reasonably small making determinants amenable numerical computation estimation eq diverges ai one marginal training inputs approximation retaining linear terms log integrand breaks therefore adopt simple heuristic replacing det lmkm det lmkm prevents spurious singularities identity matrix choice also keeps evidence continuous training inputs move set marginal inputs hyperparameters varied fig shows simple application evidence estimate given data set evidence evaluated function kernel amplitude varied simultaneously cko hence svm solution remained unchanged data set generated artificially probability model true value known spite rather crude approximation maximum full evidence identifies quite close truth approximate class probability prediction value also plotted fig overestimates noise target somewhat note obtained simply inserting map values proper bayesian treatment average posterior distribution oid course taken leave future work normalization factor estimated assumed uniform input density qlx example sampling gp prior unknown empirical training input distribution used proxy one samples instead multivariate gaussian xi covariance matrix xi xj gave similar values example even subset training inputs used probabilistic methods support vector machines summary described probabilistic framework svm classification gives intuitive understanding effect kernel determines gaussian process prior importantly also allows properly normalized evidence defined optimal values hyperparameters noise parameter corresponding error bars derived future work include comprehensive experimental tests simple laplacetype estimate naive evidence given comparison wi th approaches include variational methods recent experiments gaussian approximation posterior oid example seem promising improvement possible dropping restriction factor analysed covariance form one easily shows optimal gaussian covariance matrix parameterized diagonal matrix also interesting compare laplace gaussian variational results evidence cavity field approach acknowledgements pleasure thank tommi jaakkola manfred opper matthias seeger chris williams ole winther interesting comments discussions royal society financial support dorothy hodgkin research fellowship
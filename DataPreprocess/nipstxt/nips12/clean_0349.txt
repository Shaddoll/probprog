abstract describ framework interpret support vector machin svm maximum posterjori map solut infer problem gaussian process prior provid intuit guidelin choos good svm kernel also assign evid maxim optim valu paramet nois level cannot determin unambigu properti map solut alon cross valid error illustr use simpl approxim express svm evid determin error bar svm predict also obtain support vector machin probabilist flamework support vector machin svm recent subject intens research activ within neural network commun tutori introduct overview recent develop see one open question remain set tunabl paramet svm algorithm method choos width kernel function nois paramet control close train data fit propos see also recent effect overal shape kernel function remain imperfectli understood error bar class probabl svm predict import safeti critic applic exampl also difficult obtain paper suggest probabilist interpret svm could use tackl problem show svm kernel defin prior function input space avoid need think term high dimension featur space also allow one defin quantiti evid likelihood set hyperparamet kernel amplitud etc give simpl approxim evid maxim set hyperparamet evid sensit valu individu contrast properti cross valid error determinist solut depend product cko therefor use assign unambigu valu error bar deriv sollich focu two class classif problem suppos given set train exampl xi yi binari output yi correspond two class basic svm idea map input onto vector high dimension featur space ideal featur space problem linearli separ suppos first true among decis hyperplan qb separ train exampl obey yi xi qb xi dx dx set train input svm solut chosen one largest margin largest minim distanc train exampl equival one specifi margin one minim squar length weight vector lw subject constraint yi xi problem linearli separ slack variabl introduc measur much margin constraint violat one write yi xi control amount slack allow penalti term ad object function penalti coeffici train exampl yi xi henc incur penalti other contribut yi xi give svm optim problem find minim shift hing loss interpret svm probabilist one regard defin neg log posterior probabl paramet svm given train set first term give prior exp llwl gaussian prior compon uncorrel unit varianc chosen gaussian prior varianc fiat prior impli recov let oe latent variabl valu qb rather individu appear second data depend term make sens express prior directli distribut joint gaussian distribut compon covari given qb qb svm prior therefor simpli gaussian process gp function covari function qb zero mean correspond svm gp note number author second term becom neg log likelihood defin probabl obtain output given rkllx exp cl yo set exp ensur probabl never add valu larger one likelihood complet data set rli yi xi xi input distribut remain essenti arbitrari point howev likelihood function normal llx exp cl exp cl probabilist set actual make sens keep finit small train set yi equal nonzero probabl probabilist method support vector machin except remedi write actual probabl model dio af posterior probabl oid dio independ normal factor af construct map valu therefor svm solut simplest choic af normal independ af fdoo fdxo conceptu correspond follow procedur sampl first sampl gp prior data point sampl assign output probabl respect remain probabl know class probabl restart whole process sampl new smallest insid gap function mani valu gap less like surviv dataset requir size built reflect depend factor effect prior follow correspondingli likelihood ylx ylx xlo normal input densiti influenc function reduc uncertainti gap summar eq defin probabilist data gener model whose map solut argmax oid given data set ident standard svm effect prior gp prior modifi data set size depend factor likelihood defin condit output distribut also input distribut rel arbitrari relev properti featur space encod underli gp prior covari matrix equal kernel log posterior model fdxdx il yio xi const lnp transform differenti non train input one see maximum standard form iotiyik xi yio xi one respect call train input xi last group margin form subset support vector xi spars svm solut often number support vector come fact hing loss constant contrast use gp model classif see instead likelihood sigmoid often logist transfer function nonzero gradient everywher use moreov nois free limit sigmoid transfer function becom step function map valu tend trivial solut illumin altern point view margin shift hing loss imt ortant svm within probabilist framework main effect kernel svm classif chang properti underli gp prior true smaller actual higher gap model make less intuit sens sollich figur sampl svm prior input space unit squar plot sampl underli gaussian process prior greyscal plot repres output distribut obtain use likelihood model greyscal indic probabl black white exponenti ornstein uhlenbeck kernel covari function exp give rough decis boundari length scale reduc amplitud note sampl prior correspond new kernel grey uncertainti gap given roughli io region definit output black white widen first row squar exponenti rbf kernel exp yield smooth decis boundari chang hold fix take new sampl show paramet set typic length scale decis region polynomi kernel absenc clear length scale wide differ magnitud bottom left top right corner squar make kernel less plausibl probabilist point view probabilist method support vector machin fig illustr sampl three differ type kernel effect kernel smooth decis boundari typic size decis region uncertainti gap clearli seen prior knowledg properti target avail probabilist framework therefor provid intuit suitabl choic kernel note sampl fig rather effect prior one find howev depend factor nn chang properti prior qualit evid error bar beyond provid intuit svm kernel probabilist framework discuss also make possibl appli bayesian method svm exampl one defin evid likelihood data given model specifi hyperparamet paramet defin follow fdoq dio factor naiv evid deriv unnorm likelihood model correct factor ensur normal data set crucial order guarante optim log evid give optim hyperparamet valu least averag opper privat commun clearli gener depend separ actual vm solut hand map valu seen depend product ck properti determinist train svm alon test cross valid error cannot therefor use determin result class probabl unambigu outlin simpl approxim naiv evid deriv given integr log integrand addit constant integr gaussian distribut dx intract integr xi remain howev progress made expand log integrand around maximum xi non margin train input equival laplac approxim first term expans quadrat deviat maximum give simpl gaussian integr remain xi lead term log integrand vari linearli near maximum coupl xi appear next quadrat order discard term sublead integr factor xi evalu end result calcul yic io xi yio xi ln det lmkra first three term repres maximum log integrand ln dlo last one come integr fluctuat note contain inform margin train input era correspond submatrix lra diagon matrix entri aquantit chang aris function valu discourag larg tend increas size decis region narrow uncertainti gap verifi compar sampl ollich ix figur toy exampl evid maxim left target latent function solid line svm rbf kernel exp cko train dash line train exampl circl keep cko constant evid top right evalu function use note normal factor shift maximum toward larger valu naiv evid bottom right class probabl target solid predict evid maximum dash target gener ai ai given spars svm solut matric reason small make determin amen numer comput estim eq diverg ai one margin train input approxim retain linear term log integrand break therefor adopt simpl heurist replac det lmkm det lmkm prevent spuriou singular ident matrix choic also keep evid continu train input move set margin input hyperparamet vari fig show simpl applic evid estim given data set evid evalu function kernel amplitud vari simultan cko henc svm solut remain unchang data set gener artifici probabl model true valu known spite rather crude approxim maximum full evid identifi quit close truth approxim class probabl predict valu also plot fig overestim nois target somewhat note obtain simpli insert map valu proper bayesian treatment averag posterior distribut oid cours taken leav futur work normal factor estim assum uniform input densiti qlx exampl sampl gp prior unknown empir train input distribut use proxi one sampl instead multivari gaussian xi covari matrix xi xj gave similar valu exampl even subset train input use probabilist method support vector machin summari describ probabilist framework svm classif give intuit understand effect kernel determin gaussian process prior importantli also allow properli normal evid defin optim valu hyperparamet nois paramet correspond error bar deriv futur work includ comprehens experiment test simpl laplacetyp estim naiv evid given comparison wi th approach includ variat method recent experi gaussian approxim posterior oid exampl seem promis improv possibl drop restrict factor analys covari form one easili show optim gaussian covari matrix parameter diagon matrix also interest compar laplac gaussian variat result evid caviti field approach acknowledg pleasur thank tommi jaakkola manfr opper matthia seeger chri william ole winther interest comment discuss royal societi financi support dorothi hodgkin research fellowship
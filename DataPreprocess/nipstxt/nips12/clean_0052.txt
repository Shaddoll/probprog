abstract introduce novel method constructing language models avoids problems associated recurrent neural networks method creating prediction fractal machine pfm briefly described experiments presented demonstrate suitability pfms language modeling pfms distinguish reliably minimal pairs behavior consistent hypothesis wellformedness graded absolute discussion potential offer fresh insights language acquisition processing follows introduction cognitive linguistics seen development recent years two important related trends firstly widespread renewal interest statistical graded nature language showing traditional nothing notion well formedness may present accurate picture congruity utterances represented internally secondly analysis state space trajectories artificial neural networks anns provided new insights types processes may account ability learning devices acquire represent language without appealing traditional linguistic concepts despite remarkable advances come connectionist research common use recurrent networks simple recurrent networks srns especially study language recurrent neural networks suffer particular problems make imperfectly suited language tasks vast majority work field employs small networks datasets usually artificial although many interesting linguistic issues may thus tackled real progress evaluating potentials state trajectories graded grammaticality uncover underlying processes responsible overt linguistic phenomena must inevitably limited whilst experimental tasks remain small nevertheless certain obstacles scaling networks trained back propagation bp networks tend towards ever graded grammaticalif prediction fractal machines longer training times sizes input set network increase although real time recurrent learning rtrl back propagation time potentially better modeling temporal dependencies training times longer still scaling also difficult due potential catastrophic interference lack adaptivity stability problems include rapid loss information past events distance present increases dependence learned state trajectories training data also upon vagaries initial weight vectors making analysis difficult types learning device also suffer problems standard markov models require allocation memory every gram large values impractical variable length markov models memory efficient become unmanageable trained large data sets two important related concerns cognitive linguistics thus find method allows language models scaled similar spirit recurrent neural networks encounter problems scale use method evince new insights graded grammaticality state trajectories arise given genuinely large naturally occurring data sets accordingly present new method generating state trajectories avoids problems previously studied financial prediction task method creates fractal map training data state machines built resulting models known prediction fractal machines pfms useful properties state trajectories fractal representation fast computationally efficient generate accurate well understood may inferred even large vocabularies training sets catastrophic interference lack adaptivity stability problem given way representations built demonstrating topic future work training times significantly less recurrent networks experiments described smallest models took minutes build largest ones took around three hours comparison anns took longer day train little loss information course input sequence allowing finite precision computer scalability pfm taken advantage training large corpus naturally occurring text enabled assessment potential new insights might arise use method truly large scale language tasks prediction fractal machines pfms brief description method creating pfm given interested readers consult since space constraints preclude detailed examination key idea behind predictive model transformation symbol sequences alphabet tagset points hypercube dimensionality hypercube large enough symbol identified umque vertex particular assignment symbols vertices arbitrary transformation crucial property symbol sequences sharing suffix context mapped close specifically longer common suffix shared two sequences smaller euclidean distance point representations transformation used study corresponds iterative function system parfitt dorffner consisting affine maps ti tie ti tjfori given sequence symbols alphabet construct point representation sl sl hypercube note common center iterative function systems literature refers either symbol map depending upon context pfms constructed point representations subsequences appearing training sequence first slide window length training sequence position transform sequence length appearing window point set points obtained sliding whole training sequence partitioned several classes means vector quamization euclidean space class represented particular codebook vector number codebook vectors required chosen experimentally since quantization classes group points lying close together sequences point representations class potentially share long suffixes quantization classes may treated prediction contexts corresponding predictive symbol probabilities computed sliding window training sequence counting quantization class often sequence mapped class followed particular symbol test mode upon seeing new sequence symbols transformation performed closest quantization center found corresponding predictive probabilities used predict next symbol experimental comparison pfms recurrent networks performance pfm compared rtrl trained recurrent network next tag prediction task sixteen grammatical tags sentence start character used models trained concatenated sequence tags top three quarters sub corpora university pennsylvania brown corpus remainder used create test data follows large training corpus naturally occurring data contexts cases one possible correct continuation simply counting correctly predicted symbols insufficient assess performance since fails count correct responses targets extent models distinguished grammatical axed ungrammatical utterances therefore additionally measured generating minimal pairs comparing negative log likelihoods nlls per symbol respect model likelihood computed sliding test sequence window position determining probability symbol appears immediately beyond processing progresses probabilities multiplied negative natural logarithm taken divided number symbols significant differences nlls http www ldc upenn edu graded grammaticality prediction fractal machines much harder achieve members minimal pairs grammatical random sequences therefore good measure model validity minimal pairs generated theoretically motivated manipulations tend longer ungrammatical given small tagset removal grammatical sub classes necessarily also removes large amount information manipulations therefore performed switching positions two symbols sentence test sets symbols switched could distance apart within sentence long resulting sentence ungrammatical surface instantiations changing little possible make sentence ungrammatical goal retained task distinguishing grammatical ungrammatical sequences difficult possible test data consisted paired grammatical ungrammatical test sets around tags plus ungrammatical meaningless test set containing codes listed several times used measure baseline performance ten st order randomly initialised networks trained epochs using rtrl networks consisted input output layer units corresponding tags hidden layers units context layer units connected first hidden layer second hidden layer used increase flexibility maps hidden representations recurrent portion tag activations output layer logistic sigmoid activation function used learning rate momentum set training sequence presented rate one tag per clock tick pfms derived clustering fractal representation training data ten times various numbers codebook vectors experiments performed using pfms neural networks former case experience choosing appropriate numbers codebook vectors initially lacking type data results follow given averages either neural networks else pfms derived given number codebook vectors networks correctly predicted next tags grammatical ungrammatical test sets respectively pfms matched performance around codebook vectors respectively exceeded higher numbers vectors respectively vectors networks generated mean nlls per symbol grammatical ungrammatical test sets respectively difference meaningless test set difference nlls grammatical meaningless data pfms matched difference nlls codebook vectors nll grammatical nll ungrammatical difference nll meaningless data codebook vectors difference nlls grammatical meaningless data difference nlls grammatical ungrammatical grammatical meaningless data sets became even larger increased numbers codebook vectors difference performance grammatical ungrammatical test sets thus highly significant cases models distinguished grammatical conclusion supported fact mean nlls meaningless test set always noticeably higher minimal pair sets parfitt tiho dorffner discussion pfms exceeded performance networks larger numbers codebook vectors possible networks hidden nodes would also better terms ease use however well scaling potential pfms certainly superior great advantage representations created dependable see section making hypothesis creation testing rapid also straightforward speed pfms may trained made possible make statistically significant observations large number clustering runs introduction graded wellformedness spoken productive new hypotheses nature language use minimal pairs designed make clear cut distinction grammatical ungrammatical utterances appears leave issue one side reality results rather pertinent use likelihood measure might indeed imply brown corpus consists subcorpora representative different discourse types fiction government documents whereas traditional notions gramrnaticality would lead us treat ungramrnatical sentences minimal pair test sets equally ungrammatical nlls experiments tell different story grammatical versions consistently lower associated nll higher probability ungrammatical versions difference much smaller meaningless data either grammatical ungramrnatical data supports concept graded grammaticality nlls meaningless data might seen sort benchmark measure lesser degrees ungrammaticality note incidentally pfms appear associate meaningless data significantly higher nll networks even though difference nlls grammatical ungrammatical data suggestive pfms greater powers discrimination grades wellformedness recurrent networks used research needed ascertain validity moreover nll varied grammatical ungrammatical test sets also sentence sentence word word discourse style discourse style increased often dramatically manipulated portion ungrammatical sentence encountered words grammatical sentences exhibited similar effect thus subsequence well formed utterance occurs rarely never training set high associated nll way ungrammatical one likely happen even large corpora since grammatical structures rare consistent recent findings human sentence processing well formedness linked conformity expectation measured gloze scores interesting also remarkable variation nll discourse styles although mean nll across discourse styles test sets lower grammatical ungrammatical versions cannot guaranteed grammatical version one test set lower nll ungrammatical version another indeed grammatical ungrammatical nlls interleave may observed figure shows nlls three discourse styles lie bottom middle top range even interestingly nlls grammatical versions discourse styles ordered according lie within range becomes clear nll predictor discourse style styles linguists class formal graded grammaticalif prediction fractal machines nils ociated grammatical ungrammatical versions discourse types learned text grammatical learned text ungrammatical romanfie liction gramma cal romantic fiction ungrammatical science liction grammatical science fiction ungrammatical codebook vectors figure nlls minimal pair test sets containing different discourse styles suggest grades wellformedness based upon prototypicality learned government document test sets lowest nlls three press test sets clustering fiction test sets exemplifying creative language use clustering high end similarly learned government test sets lowest nlls conforms intuition usage lies closest grammatically prototypical even though training set test sets fiction thus might expected contribute prototype suggests usage varies significantly across fiction test sets conclusion work use pfms language modeling early stage results date show lot offer much larger project planned examine allen seidenberg hypothesis graded grammaticality wellformedness applies syntax also language subdomains semantics integral part use larger corpora tagsets identification vertices semantic syntactic features rather atomic symbols identifying possibilities combining pfms anns example means bypassing normal method creating state space trajectories subject current study acknowledgments work supported austrian science fund fwf within research project adaptive information systems modeling economics management science sfb austrian research institute artificial intelligence supported austrian federal ministry science transport parfitt tit dorffner
abstract present variational bayesian method model selection families kernels classifiers like support vector machines gaussian processes algorithm needs user interaction able adapt large number kernel parameters given data without sacrifice training cases validation opens possibility use sophisticated families kernels situations small standard kernel classes clearly inappropriate relate method work done gaussian processes clarify relation support vector machines certain gaussian process models introduction bayesian techniques widely successfully used neural networks statistics community appealing conceptual simplicity generality consistency solve learning problems paper present new method applying bayesian methodology support vector machines briefly review gaussian process support vector classification section clarify relationship pointing common roots although focus classification straightforward apply methods regression problems well section introduce algorithm show relations existing methods finally present experimental results section close discussion section let measure space ir xn tn xi ti noisy sample latent function tly denotes noise distribution given points wish predict minimize error probability difficult estimate probability generarive bayesian methods attack problem placing stochastic process prior space latent functions seeger compute posterior predictive distributions ly dy xi likelihood dly rli ti normalization constant tlx obtained averaging lx gaussian process gp spline smoothing models use gaussian process prior seen function set random variables finite corresponding variables jointly gaussian see introduction gp determined mean function positive definite covariance kernel gaussian process classification gpc amounts specifying available prior knowledge choosing class kernels vector hyperparameters hyperprior usually choices guided simple attributes smoothness trends differentiability general approaches kernel design also considered class classification common noise distribution binomial one ty exp logistic function logit log target distribution noise model integral analytically tractable range approximative techniques based laplace approximations markov chain monte carlo variational methods mean field algorithms known follow laplace approach gpc approximate posterior yld gaussian distribution argmaxp yld posterior mode vy logp evaluated easy show predictive distribution gaussian mean variance lg covariance matrix xi xj ij xi prime denotes transposition final discriminant therefore linear combination xi discriminative approach prediction problem choose loss function approximation misclassification loss ty search discriminant minimizes points interest see support vector classification svc uses insensitive loss svc loss ty ui upper bound misclassification loss reproducing kernel hilbert space rkhs kernel hypothesis space indeed support vector models laplace method gaussian processes special cases spline smoothing models rkhs aim minimize functional ti yi denotes norm rkhs shown minimizer written maximizes ti yi ay ic facts found terms depending log posterior gp framework choose log consider gps mean function follows denotes indicator function set bayesian model selection support vector machines absorb svc loss transformed dual problem via iccp vector dual variables efficiently solved using quadratic programming techniques excellent reference note svc loss cannot written negative log noise distribution cannot reduce svc special case gaussian process classification model although generarive model svc given easier less problematic regard svc efficient approximation proper gaussian process model various models proposed see work simply normalize svc loss pointwise use gaussian process model normalized vc loss ty log exp exp note close approximation unnormalized svc loss reader might miss svm bias parameter dropped clarity straightforward apply semiparametric extension gp models variational method kernel classification real bayesian way deal hyperparameters average lx posterior oid order obtain predictive distribution lx approximated markov chain monte carlo methods simply argmaxp old latter approach called maximum posteriori map justified limit large often works well practice basic challenge map calculate evidence fp dy exp ti yi dy plan attack variational approach let density model class chosen approximate posterior yld logp log yio dy ult call ep log epoog variational free energy second term well known kullback leibler divergence posterior nonnegative equals zero iff yld almost everywhere respect distribution thus upper bound logp changing decrease enlarges evidence decreases divergence posterior approximation favourable idea introduced ensemble learning successfully applied mlps latter work also introduced model class use namely class gaussians mean factor analyzed covariance diagonal positive elements hinton athis random effects model improper prior works placing flat improper prior bias parameter average different discriminants given ensemble although danger overfitting use full covariances would render optimization difficult time memory consuming seeger van camp used diagonal covariances would setting choosing small able track important correlations components posterior using mn parameters represent agreed criterion gradients respect parameters easily efficiently computed except generic term sum one dimensional gaussian expectations depending actual either analytically tractable approximated using quadrature algorithm example expectation normalized svc loss decomposed expectations unnormalized svc loss log see end section former computed analytically latter expectation handled replacing log piecewise defined tight bound integral solved analytically gpc loss cannot solved analytically experiments approximated gaussian quadrature optimize using nested loop algorithm follows inner loop run optimizer minimize fixed used conjugate gradients optimizer since number parameters rather large outer loop optimizer minimizing chose quasi newton method since dimension usually rather small gradients costly evaluate use resulting minimizer two different ways natural discard plug original architecture predict using mode approximation true posterior mode benefitting kernel adapted given data particularly interesting support vector machines due sparseness final kernel expansion typically small fraction components weight vector xp non zero corresponding datapoints termed support vectors allows efficient predictions large number test points however also retain use gaussian approximation posterior use variance approximative predictive distribution ix derive error bars predictions although interpretation figures somewhat complicated case kernel discriminants like svm whose loss function correspond noise distribution relations methods let us look alternative ways maximize loss twice differentiable everywhere progress made replacing second order taylor expansion around mode integrand known laplace approximation used maximize approximately however technique cannot used nondifferentiable losses insensitive type nevertheless svc loss evidence approximated laplacelike fashion interesting compare results work approximation evaluated efficiently continuous nondifferentiabilities cannot ignored since probability one nonzero number sit exactly maxgin locations although continuity accomplished modification see bayesian model selection support vector machines difficult optimize dimension small opper winther use mean field ideas derive approximate leave one test error estimator quickly evaluated suffers typical noisiness cross validation scores kwok applies evidence framework support vector machines technique seems restricted kernels finite eigenfunction expansion see details interesting compare variational method laplace method variational technique let differentiable suppose given restrict approximate replacing ti yi expansion og ti yi ti yi ti pi yy oy fi posterior mean change criterion fappro say easy show gaussian approximation posterior employed laplace method namely fi diag minimizes fappro full covariances used plugging minimizer pp end evidence approximation maximized laplace method latter variational technique since approximation loss function upper bound works differentiable loss functions upper bound loss function quadratic polynomial add variational parameters bound parameters method becomes broadly similar lower bound algorithm indeed since fixed variational parameters polynomials easily solve mean covariance former parameters essential ones however quadratic upper bound poor functions like svc loss cases bound expected tighter experiments tested variational algorithm number datasets uci machine learning repository delve archive university torontos leptograpsus crabs pima indian diabetes wisconsin breast cancer ringnorm twonorm waveform class descriptions may found web case normalized whole set zero mean unit variance input columns picked training set random used rest testing chose ir well known squared exponential kernel see wd parameters constrained positive chose representation oi use prior see comment end section comparison trained gaussian process classifier laplace method also without hyperprior support vector machine using fold cross validation select free parameters latter case constrained scale parameters wi equal infeasible adapt hyperparameters data using crossvalidation dropped parameter allowing bias parameter mentioned within variational method use posterior mode ssee http cs utoronto ca delve http ics uci edu mlearn lrepository html seeger name train test var gp gp var svm svm lin size size lapl cv discr crabs pima wdbc twonorm ringnorm waveform table number test errors various methods well mean prediction tested methods error bars computed baseline method linear discriminant trained minimize squared error table shows test errors different methods attained results show new algorithm performs equally well methods considered course regarded combination much effort necessary produce took us almost whole day lot user interactions cross validation model selection rule thumb lot support vectors upper bound indicate large parameter failed least two sets start coarse grids sweep several stages refinement effect known automatic relevance determination ard see nicely observed datasets monitoring length scale parameters wi indeed variational svc algorithm almost completely ignored driving length scales small values dimensions crabs pima waveform wdbc detected dimension particularly important regard separation harmony gp laplace method thus sensible parameterized kernel family together method bayesian kind allows us gain additional important information dataset might used improve experimental design results experiments methods tested hyperpriors well detailed analysis experiments found discussion shown perform model selection support vector machines using approximative bayesian variational techniques method applicable wide range loss functions able adapt large number hyperparameters given data allows use sophisticated kernels bayesian techniques like automatic relevance determination see possible using common model selection criteria like cross validation since method fully automatic easy non experts use evidence computed training set training data sacrificed validation refer topics paper investigated much greater detail pressing issue unfortunate scaling method training set aside opens possibility comparing svms fullyautomatic methods within delve project see section bayesian model selection support vector machines size currently currently explorin applicability powerful approximations might bring us much closer desired scaling see also another interesting issue would connect method work use generarive models derive kernels situations standard kernels applicable reasonable acknowledgments thank chris williams amos storkey peter sollich carl rasmussen helpful inspiring discussions work partially funded scholarship dr erich milllet foundation grateful division informatics supporting visit edinburgh chris williams making possible references david barber christopher bishop ensemble learning multi layer networks advances nips number pages mit press mark gibbs bayesian gaussian processes regression classification phd thesis university cambridge geoffrey hinton van camp keeping neural networks simple minimizing description length weights proceedings th annual conference computational learning theory pages tommi jaakkola marina meila tony jebara maximum entropy discrimination advances nips number mit press tommi jaakkola david haussler exploiting generarive models discriminarive classifiers advances nips number james tin tau kwok integrating evidence framework support vector machine submitted esann radford neal monte carlo implementation gaussian process models bayesian classification regression technical report department statistics university toronto january manfred opper ole winther gp classification svm mean field results leave one estimator advances large margin classifiers mit press matthias seeger bayesian methods support vector machines gaussian processes master thesis university karlsruhe germany available http www dai ed ac uk seeger john skilling maximum entropy bayesian methods cambridge university press peter sollich probabilistic methods support vector machines advances nips number mit press vladimir vapnik statistical learning theory wiley grace wahba spline models observational data cbms nsf regional conference series siam grace wahba support vector machines reproducing kernel hilbert spaces randomized gacv technical report university wisconsin christopher williams prediction gaussian processes linear regression linear prediction beyond jordan editor learning graphical models kluwer christopher williams david barber bayesian classification gaussian processes ieee trans pami rathe running time essentially laplace method thus comparable fastest known bayesian gp algorithm
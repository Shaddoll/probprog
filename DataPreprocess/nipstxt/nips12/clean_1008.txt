abstract propose analyze class actor critic algorithms simulation based optimization markov decision process parameterized family randomized stationary policies two time scale algorithms critic uses td learning linear approximation architecture actor updated approximate gradient direction based information provided critic show features critic span subspace prescribed choice parameterization actor conclude discussing convergence properties open problems introduction vast majority reinforcement learning rl neuro dynamic programming ndp methods fall one following two categories actor methods work parameterized family policies gradient performance respect actor parameters directly estimated simulation parameters updated direction improvement possible drawback methods gradient estimators may large variance furthermore policy changes new gradient estimated independently past estimates hence learning sense accumulation consolidation older information critic methods rely exclusively value function approximation aim learning approximate solution bellman equation hopefully prescribe near optimal policy methods indirect sense try optimize directly policy space method type may succeed constructing good approximation value function yet lack reliable guarantees terms near optimality resulting policy actor critic methods aim combining strong points actor criticonly methods critic uses approximation architecture simulation learn value function used update actor policy parameters actor critic algorithms direction performance improvement methods long gradient based may desirable convergence properties contrast criticonly methods convergence guaranteed limited settings hold promise delivering faster convergence due variance reduction compared actor methods hand theoretical understanding actor critic methods limited case lookup table representations policies paper propose actor critic algorithms provide overview convergence proof algorithms based important observation since number parameters actor update relatively small compared number states critic need attempt compute approximate exact value function high dimensional object fact show critic ideally compute certain projection value function onto low dimensional subspace spanned set basis functions completely determined parameterization actor finally analysis suggests td algorithms algorithms extended case arbitrary state action spaces long certain ergodicity assumptions satisfied close section noting ideas similar presented simultaneous independent work sutton et al markov decision processes parameterized family rsp consider markov decision process finite state space finite action space let given cost function randomized stationary policy rsp mapping assigns state probability distribution action space consider set randomized stationary policies ip parameterized terms vector pair lu denotes probability taking action state encountered policy corresponding let pxy denote probability next state given current state current action note rsp sequence states xn state action pairs xn un markov decision process form markov chains state spaces respectively make following assumptions family policies ip map twice differentiable bounded first second derivatives furthermore exists valued function mapping bounded first bounded derivatives fixed markov chains xn xn un irreducible aperiodic stationary probabilities respectively rsp reference assumption note whenever nonzero vin consider average cost function given konda tsitsiklis interested minimizing let vs lt differential cost function defined solution poisson equation vs ps epxv vs intuitively vs viewed disadvantage state expected excess cost top average cost incurred start state plays role similar played familiar value function arises total discounted cost markov decision problems finally every define function qs qs epxv vs recall following result stated different versions result established theorem ris qs stands ith component quantity qs formula interpreted expected excess cost incurred certain renewal period markov chain xn rsp ps estimated means simulation leading actoronly algorithms provide alternative interpretation formula theorem inner product thus derive different set algorithms readily generalize case infinite space well define inner product two real valued functions ql viewed vectors lai ql rio ql notation rewrite formula oia qs let ils denote norm induced inner product lsllai let denote span vectors lsll set functions form scalars note although gradient depends function vector possibly high dimensional space lt lsllai dependence inner products vectors thus instead learning function would suffice learn projection qs subspace indeed let iis projection operator defined since ilsq arg min ilq oils enough compute projection qs onto actor critic algorithms actor critic algorithms view actor critic algorithms stochastic gradient algorithms parameter space actor actor parameter vector job critic compute approximation projection ii onto actor uses approximation update policy approximate gradient direction analysis shows precisely td algorithms try compute projection exact value function onto subspace spanned feature vectors allows us implement critic using td algorithm note however types critics possible based batch solution least squares problems long aim computing projection note minor differences common usage td context need projection functions rather value functions easily achieved replacing markov chain xt markov chain xn un difference assume control policy feature vectors fixed algorithms control policy well features need change actor updates parameters shown need pose problems long actor parameters updated slower time scale ready describe two actor critic algorithms differ far critic updates concerned variants critic td algorithm linearly parameterized approximation architecture function form denotes parameter vector critic features used critic dependent actor parameter vector chosen span ai denoted contains note formula still holds ii redefined projection onto long contains straightforward choice would let nevertheless allow possibility properly contains critic uses features actually necessary added flexibility may turn useful number ways possible certain values features either close zero almost linearly dependent values operator ii becomes ill conditioned algorithms become unstable might avoided using richer set features second algorithm propose td critic compute approximate rather exact projection use additional features result reduction approximation error along parameter vector critic stores auxiliary parameters scalar estimate average cost vector represents sutton eligibility trace actor critic updates take place course simulation single sample path controlled markov chain let rk zk parameters critic let parameter vectpr actor time let state action pair time let xk new state obtained action applied new action generated according rsp corresponding actor parameter vector critic carries update similar average cost temporal difference method konda andj tsitsimis positive stepsize parameter two variants critic use different ways updating zk td critic let state zk xk uk otherwise td critic zk otzk xk uk actor finally actor updates parameter vector letting ok ok rk xk uk xk uk positive stepsize rk normalization factor satisfying lipschitz continuous exists presented algorithms two many variations instance one could also consider episodic problems one starts given initial state runs process random termination time time process reinitialized objective minimizing expected cost termination setting average cost estimate unnecessary removed critic update formula critic parameter reinitialized time entered one would obtain method closely related williams reinforce algorithm method involve value function learning observations one episode affect critic parameter another episode contrast approach observations past episodes affect current critic parameter sense critic learning advantageous long slowly changing observations recent episodes carry useful information function current policy convergence actor critic algorithms since actor critic algorithms gradient based one cannot expect prove convergence globally optimal policy within given class rsp best one could hope convergence va zero practical terms usually translate convergence local minimum actually td critic generally converge approximation desired projection value function corresponding convergence result necessarily weaker guaranteeing va becomes small infinitely often let us introduce assumptions actor critic algorithms define matrix assume uniformly positive definite exists rrg qllrll assume stepsize sequences positive nonincreasing satisfy ec stands either ffk also assume note last assumption requires actor parameters updated time scale slower critic theorem actor critic algorithm td critic lim inf iiv furthermore bounded lim iivx theorem every exists liminfk llvx sufficiently close note theoretical guarantees appear stronger case td critic however expect td perform better practice much smaller variance parameter rk similar issues arise considering actor algorithms experiments reported indicate introducing forgetting factor result much faster convergence little loss performance provide overview proofs theorems since ffk size actor updates becomes negligible compared size critic updates therefore actor looks stationary far critic concerned thus analysis td critic analysis td critic used appropriate modifications conclude critic approximation qo asymptotically correct denotes value critic converges actor parameters fixed update actor rewritten kek ek error becomes asymptotically negligible point standard proof techniques stochastic approximation algorithms used complete proof conclusions key observation paper actor critic methods actor parameterization critic parameterization need chosen konda andj tsitsiklis independently rather appropriate approximation architecture critic directly prescribed parameterization used actor capitalizing observation presented class actor critic algorithms aimed combining advantages actor critic methods contrast existing actor critic methods algorithms apply high dimensional problems rely lookup table representations mathematically sound sense possess certain convergence properties acknowledgments research partially supported nsf grant ecs afosr grant
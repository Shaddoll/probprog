abstract propos analyz class actor critic algorithm simul base optim markov decis process parameter famili random stationari polici two time scale algorithm critic use td learn linear approxim architectur actor updat approxim gradient direct base inform provid critic show featur critic span subspac prescrib choic parameter actor conclud discuss converg properti open problem introduct vast major reinforc learn rl neuro dynam program ndp method fall one follow two categori actor method work parameter famili polici gradient perform respect actor paramet directli estim simul paramet updat direct improv possibl drawback method gradient estim may larg varianc furthermor polici chang new gradient estim independ past estim henc learn sens accumul consolid older inform critic method reli exclus valu function approxim aim learn approxim solut bellman equat hope prescrib near optim polici method indirect sens tri optim directli polici space method type may succeed construct good approxim valu function yet lack reliabl guarante term near optim result polici actor critic method aim combin strong point actor criticonli method critic use approxim architectur simul learn valu function use updat actor polici paramet actor critic algorithm direct perform improv method long gradient base may desir converg properti contrast criticonli method converg guarante limit set hold promis deliv faster converg due varianc reduct compar actor method hand theoret understand actor critic method limit case lookup tabl represent polici paper propos actor critic algorithm provid overview converg proof algorithm base import observ sinc number paramet actor updat rel small compar number state critic need attempt comput approxim exact valu function high dimension object fact show critic ideal comput certain project valu function onto low dimension subspac span set basi function complet determin parameter actor final analysi suggest td algorithm algorithm extend case arbitrari state action space long certain ergod assumpt satisfi close section note idea similar present simultan independ work sutton et al markov decis process parameter famili rsp consid markov decis process finit state space finit action space let given cost function random stationari polici rsp map assign state probabl distribut action space consid set random stationari polici ip parameter term vector pair lu denot probabl take action state encount polici correspond let pxi denot probabl next state given current state current action note rsp sequenc state xn state action pair xn un markov decis process form markov chain state space respect make follow assumpt famili polici ip map twice differenti bound first second deriv furthermor exist valu function map bound first bound deriv fix markov chain xn xn un irreduc aperiod stationari probabl respect rsp refer assumpt note whenev nonzero vin consid averag cost function given konda tsitsikli interest minim let vs lt differenti cost function defin solut poisson equat vs ps epxv vs intuit vs view disadvantag state expect excess cost top averag cost incur start state play role similar play familiar valu function aris total discount cost markov decis problem final everi defin function qs qs epxv vs recal follow result state differ version result establish theorem ri qs stand ith compon quantiti qs formula interpret expect excess cost incur certain renew period markov chain xn rsp ps estim mean simul lead actoronli algorithm provid altern interpret formula theorem inner product thu deriv differ set algorithm readili gener case infinit space well defin inner product two real valu function ql view vector lai ql rio ql notat rewrit formula oia qs let il denot norm induc inner product lsllai let denot span vector lsll set function form scalar note although gradient depend function vector possibl high dimension space lt lsllai depend inner product vector thu instead learn function would suffic learn project qs subspac inde let ii project oper defin sinc ilsq arg min ilq oil enough comput project qs onto actor critic algorithm actor critic algorithm view actor critic algorithm stochast gradient algorithm paramet space actor actor paramet vector job critic comput approxim project ii onto actor use approxim updat polici approxim gradient direct analysi show precis td algorithm tri comput project exact valu function onto subspac span featur vector allow us implement critic use td algorithm note howev type critic possibl base batch solut least squar problem long aim comput project note minor differ common usag td context need project function rather valu function easili achiev replac markov chain xt markov chain xn un differ assum control polici featur vector fix algorithm control polici well featur need chang actor updat paramet shown need pose problem long actor paramet updat slower time scale readi describ two actor critic algorithm differ far critic updat concern variant critic td algorithm linearli parameter approxim architectur function form denot paramet vector critic featur use critic depend actor paramet vector chosen span ai denot contain note formula still hold ii redefin project onto long contain straightforward choic would let nevertheless allow possibl properli contain critic use featur actual necessari ad flexibl may turn use number way possibl certain valu featur either close zero almost linearli depend valu oper ii becom ill condit algorithm becom unstabl might avoid use richer set featur second algorithm propos td critic comput approxim rather exact project use addit featur result reduct approxim error along paramet vector critic store auxiliari paramet scalar estim averag cost vector repres sutton elig trace actor critic updat take place cours simul singl sampl path control markov chain let rk zk paramet critic let paramet vectpr actor time let state action pair time let xk new state obtain action appli new action gener accord rsp correspond actor paramet vector critic carri updat similar averag cost tempor differ method konda andj tsitsimi posit stepsiz paramet two variant critic use differ way updat zk td critic let state zk xk uk otherwis td critic zk otzk xk uk actor final actor updat paramet vector let ok ok rk xk uk xk uk posit stepsiz rk normal factor satisfi lipschitz continu exist present algorithm two mani variat instanc one could also consid episod problem one start given initi state run process random termin time time process reiniti object minim expect cost termin set averag cost estim unnecessari remov critic updat formula critic paramet reiniti time enter one would obtain method close relat william reinforc algorithm method involv valu function learn observ one episod affect critic paramet anoth episod contrast approach observ past episod affect current critic paramet sens critic learn advantag long slowli chang observ recent episod carri use inform function current polici converg actor critic algorithm sinc actor critic algorithm gradient base one cannot expect prove converg global optim polici within given class rsp best one could hope converg va zero practic term usual translat converg local minimum actual td critic gener converg approxim desir project valu function correspond converg result necessarili weaker guarante va becom small infinit often let us introduc assumpt actor critic algorithm defin matrix assum uniformli posit definit exist rrg qllrll assum stepsiz sequenc posit nonincreas satisfi ec stand either ffk also assum note last assumpt requir actor paramet updat time scale slower critic theorem actor critic algorithm td critic lim inf iiv furthermor bound lim iivx theorem everi exist liminfk llvx suffici close note theoret guarante appear stronger case td critic howev expect td perform better practic much smaller varianc paramet rk similar issu aris consid actor algorithm experi report indic introduc forget factor result much faster converg littl loss perform provid overview proof theorem sinc ffk size actor updat becom neglig compar size critic updat therefor actor look stationari far critic concern thu analysi td critic analysi td critic use appropri modif conclud critic approxim qo asymptot correct denot valu critic converg actor paramet fix updat actor rewritten kek ek error becom asymptot neglig point standard proof techniqu stochast approxim algorithm use complet proof conclus key observ paper actor critic method actor parameter critic parameter need chosen konda andj tsitsikli independ rather appropri approxim architectur critic directli prescrib parameter use actor capit observ present class actor critic algorithm aim combin advantag actor critic method contrast exist actor critic method algorithm appli high dimension problem reli lookup tabl represent mathemat sound sens possess certain converg properti acknowledg research partial support nsf grant ec afosr grant
abstract support vector machin svm state art techniqu regress classif combin excel generalis properti spars kernel represent howev suffer number disadvantag notabl absenc probabilist output requir estim trade paramet need utilis mercer kernel function paper introduc relev vector machin rvm bayesian treatment generalis linear model ident function form svm rvm suffer none disadvantag exampl demonstr compar generalis perform rvm requir dramat fewer kernel function introduct supervis learn given set exampl input vector xn along correspond target tn latter might real valu regress class label classif train set wish learn model depend target input object make accur predict previous unseen valu real world data presenc nois regress class overlap classif impli princip model challeng avoid fit train set success approach supervis learn support vector machin svm make predict base function form wnk xn wn model weight kernel function key featur svm classif case target function attempt minimis number error made train set simultan maximis margin two class featur space implicitli defin kernel effect prior avoid fit lead good generalis furthermor result spars model depend subset kernel function associ train exampl xn lie either margin wrong side state art result report mani task svm appli relev vector machin howev support vector methodolog exhibit signific disadvantag predict probabilist regress svm output point estim classif hard binari decis ideal desir estim condit distribut tlx order captur uncertainti predict regress may take form error bar particularli crucial classif posterior probabl class membership necessari adapt vari class prior asymmetr misclassif cost although rel spars svm make liber use kernel function requisit number grow steepli size train set necessari estim error margin trade paramet regress insensit paramet gener entail cross valid procedur wast data comput kernel function must satisfi mercer condit paper introduc relev vector machin rvm probabilist spars kernel model ident function form svm adopt bayesian approach learn introduc prior weight govern set hyperparamet one associ weight whose probabl valu iter estim data sparsiti achiev practic find posterior distribut mani weight sharpli peak around zero furthermor unlik support vector classifi nonzero weight rvm associ exampl close decis boundari rather appear repres prototyp exampl class term exampl relev vector defer principl automat relev determin ard motiv present approach compel featur rvm capabl generalis perform compar equival svm typic utilis dramat fewer kernel function furthermor rvm suffer none limit svm outlin next section introduc bayesian model initi regress defin procedur obtain hyperparamet valu thu weight section give brief exampl applic rvm regress case develop theori classif case section exampl rvm classif given section conclud discuss relev vector regress given dataset input target pair xn follow standard formul assum gaussian af mean distribut given model defin svm likelihood dataset written tlw ra exp tl tn wo wn design matrix nm xn xm nl maximum likelihood estim gener lead sever overfit encod prefer smoother function defin ard gaussian prior weight wle wi tip vector hyperparamet introduct individu hyperparamet everi weight key featur model ultim respons sparsiti properti posterior weight obtain bay rule wlt exp ty tbt defin diag al also treat hyperparamet may estim data integr weight obtain margin likelihood evid hyperparamet tla lb exp ideal bayesian infer defin hyperprior integr hyperparamet howev marginalis cannot perform close form adopt pragmat procedur base mackay optimis margin likelihood respect essenti type maximum likelihood method equival find maximum lt assum uniform thu improp hyperprior make predict base use maximis valu note optimis hyperparamet valu maximis cannot obtain close form consid two altern formula iter estim first consid weight hidden variabl em approach give new er ii second direct differenti rearrang give new defin quantiti cqz interpret measur well determin paramet data gener latter updat observ exhibit faster converg nois varianc method lead estim new lit uli practic estim find mani ai approach infin wijt becom infinit peak zero impli correspond kernel function prune space preclud detail explan occur occam penalti paid smaller valu ai due appear determin margin likelihood ai lesser penalti paid explain data increas nois case ai relev vector machin exampl relev vector regress synthet exampl sinc function function sinc xl sin commonli use illustr support vector regress place classif margin insensit region introduc tube around function within error penalis case support vector lie edg outsid region exampl use linear spline kernel approxim sinc base uniformli space nois free sampl utilis support vector comparison approxim function relev vector model utilis kernel case nois varianc fix alon estim approxim function plot figur left requir relev vector largest error compar sv case figur right illustr case gaussian nois standard deviat ad target approxim use relev vector nois automat estim use lo figur relev vector approxim sinc nois free data left ad gaussian nois right estim function drawn solid line relev vector shown circl ad nois case right true function shown dash benchmark tabl illustr regress perform popular benchmark dataset friedman three synthet function result averag randomli gener train set size exampl test set boston hous dataset averag randomis train test split predict error obtain number kernel function requir support vector regress svr relev vector regress rvr given error kernel dataset svr rvr svr rvr friedman friedman friedman boston hous tip relev vector classif extend relev vector approach case classif desir predict posterior probabl class membership given input generalis linear model appli logist sigmoid function write likelihood tlw ii xn tn xn howev cannot integr weight obtain margin likelihood analyt utilis iter procedur base mackay current fix valu find probabl weight wmp locat posterior mode equival standard optimis regularis logist model use effici iterativelyreweight least squar algorithm find maximum comput hessian wmp vlogp ct wmr tb bnn xn xn negat invert give covari gaussian approxim posterior weight hyperparamet et updat use note nois varianc procedur repeat suitabl converg criteria satisfi note bayesian treatment multilay neural network gaussian approxim consid weak method posterior mode unrepres overal probabl mass howev rvm note wle log concav hessian neg definit everywher give us consider confid gaussian approxim exampl rvm classif synthet exampl gaussian mixtur data first utilis artifici gener data two dimens order illustr graphic select relev vector class denot sampl singl gaussian overlap small degre class sampl mixtur two gaussian relev vector classifi compar support vector counterpart use gaussian kernel valu svm select use fold crossvalid train set result typic dataset exampl given figur test error rvm svm compar remark featur contrast complex classifi support vector machin utilis kernel function compar relev vector method also notabl relev vector distanc decis boundari space given analysi observ seen consist hyperparamet updat equat qualit explan output basi function lie near decis boundari poor indic class membership basi function natur penalis bayesian framework relev vector machin svm error vector figur result train function ident svm left rvm right classift typic synthet dataset decis boundari shown dash relev support vector shown circl emphasis dramat reduct complex rvm model real exampl tabl give error complex result pima indian diabet handwritten digit dataset former task recent use illustr bayesian classif relat gaussian process gp techniqu utilis author split data train test exampl quot result gp case latter dataset popular support vector benchmark compris train exampl along exampl test set svm result quot error kernel dataset svm gp rvm svm gp rvm pima indian term predict accuraci rvm margin superior pima set outperform svm digit data howev consist exampl paper rvm classifi utilis mani fewer kernel function strikingli rvm achiev state art perform diabet dataset kernel note reduc set method exist subsequ prune support vector model reduc requir number kernel expens increas error see exampl result data discuss exampl paper effect demonstr relev vector machin attain compar regress appar superior level generalis accuraci well establish support vector approach time utilis dramat fewer kernel function impli consider tip save memori comput practic implement importantli also benefit absenc addit nuisanc paramet set apart need choos type kernel associ paramet fact case kernel paramet obtain improv term accuraci sparsiti result benchmark given section optimis margin likelihood respect multipl input scale paramet gaussian kernel furthermor may also exploit bayesian formal guid choic kernel note present methodolog applic arbitrari basi function limit exampl use mercer kernel svm advantag rvm classifi standard formul probabilist generalis linear model impli extend multipl class case st aightforward principl manner without need train heurist combin multipl dichotom classifi standard practic svm furthermor estim posterior probabl class membership major benefit convey principl measur uncertainti predict essenti wish allow adapt vari class prior along incorpor asymmetr misclassif cost howev must note princip disadvantag relev vector method complex train phase necessari repeatedli comput invert hessian matrix requir storag comput larg dataset make train consider slower svm current memori constraint limit us train exampl develop approxim method handl larger dataset employ handwritten digit databa note case bayesian method gener strongest data scarc spars result classifi induc bayesian framework present compel motiv appli relev vector techniqu larger dataset acknowledg author wish thank chri bishop john platt bernhard schslkopf help discuss jp sequenti minim optimis code
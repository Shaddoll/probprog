abstract support vector machine svm state art technique regression classification combining excellent generalisation properties sparse kernel representation however suffer number disadvantages notably absence probabilistic outputs requirement estimate trade parameter need utilise mercer kernel functions paper introduce relevance vector machine rvm bayesian treatment generalised linear model identical functional form svm rvm suffers none disadvantages examples demonstrate comparable generalisation performance rvm requires dramatically fewer kernel functions introduction supervised learning given set examples input vectors xn along corresponding targets tn latter might real values regression class labels classification training set wish learn model dependency targets inputs objective making accurate predictions previously unseen values real world data presence noise regression class overlap classification implies principal modelling challenge avoid fitting training set successful approach supervised learning support vector machine svm makes predictions based function form wnk xn wn model weights kernel function key feature svm classification case target function attempts minimise number errors made training set simultaneously maximising margin two classes feature space implicitly defined kernel effective prior avoiding fitting leads good generalisation furthermore results sparse model dependent subset kernel functions associated training examples xn lie either margin wrong side state art results reported many tasks svms applied relevance vector machine however support vector methodology exhibit significant disadvantages predictions probabilistic regression svm outputs point estimate classification hard binary decision ideally desire estimate conditional distribution tlx order capture uncertainty prediction regression may take form error bars particularly crucial classification posterior probabilities class membership necessary adapt varying class priors asymmetric misclassification costs although relatively sparse svms make liberal use kernel functions requisite number grows steeply size training set necessary estimate error margin trade parameter regression insensitivity parameter generally entails cross validation procedure wasteful data computation kernel function must satisfy mercer condition paper introduce relevance vector machine rvm probabilistic sparse kernel model identical functional form svm adopt bayesian approach learning introduce prior weights governed set hyperparameters one associated weight whose probable values iteratively estimated data sparsity achieved practice find posterior distributions many weights sharply peaked around zero furthermore unlike support vector classifier nonzero weights rvm associated examples close decision boundary rather appear represent prototypical examples classes term examples relevance vectors deference principle automatic relevance determination ard motivates presented approach compelling feature rvm capable generalisation performance comparable equivalent svm typically utilises dramatically fewer kernel functions furthermore rvm suffers none limitations svm outlined next section introduce bayesian model initially regression define procedure obtaining hyperparameter values thus weights section give brief examples application rvm regression case developing theory classification case section examples rvm classification given section concluding discussion relevance vector regression given dataset input target pairs xn follow standard formulation assume gaussian af mean distribution given modelled defined svm likelihood dataset written tlw ra exp tl tn wo wn design matrix nm xn xm nl maximum likelihood estimation generally lead severe overfitting encode preference smoother functions defining ard gaussian prior weights wle wi tipping vector hyperparameters introduction individual hyperparameter every weight key feature model ultimately responsible sparsity properties posterior weights obtained bayes rule wlt exp ty tbt defined diag al also treated hyperparameter may estimated data integrating weights obtain marginal likelihood evidence hyperparameters tla lb exp ideal bayesian inference define hyperpriors integrate hyperparameters however marginalisation cannot performed closed form adopt pragmatic procedure based mackay optimise marginal likelihood respect essentially type maximum likelihood method equivalent finding maximum lt assuming uniform thus improper hyperprior make predictions based using maximising values note optimising hyperparameters values maximise cannot obtained closed form consider two alternative formulae iterative estimation first considering weights hidden variables em approach gives new er ii second direct differentiation rearranging gives new defined quantities cqz interpreted measure well determined parameter data generally latter update observed exhibit faster convergence noise variance methods lead estimate new lit uli practice estimation find many ai approach infinity wijt becomes infinitely peaked zero implying corresponding kernel functions pruned space precludes detailed explanation occurs occam penalty paid smaller values ai due appearance determinant marginal likelihood ai lesser penalty paid explaining data increased noise case ai relevance vector machine examples relevance vector regression synthetic example sinc function function sinc xl sin commonly used illustrate support vector regression place classification margin insensitive region introduced tube around function within errors penalised case support vectors lie edge outside region example using linear spline kernels approximation sinc based uniformly spaced noise free samples utilises support vectors comparison approximate function relevance vector model utilising kernel case noise variance fixed alone estimated approximating function plotted figure left requires relevance vectors largest error compared sv case figure right illustrates case gaussian noise standard deviation added targets approximation uses relevance vectors noise automatically estimated using lo figure relevance vector approximation sinc noise free data left added gaussian noise right estimated functions drawn solid lines relevance vectors shown circled added noise case right true function shown dashed benchmarks table illustrates regression performance popular benchmark datasets friedman three synthetic functions results averaged randomly generated training sets size example test set boston housing dataset averaged randomised train test splits prediction error obtained number kernel functions required support vector regression svr relevance vector regression rvr given errors kernels dataset svr rvr svr rvr friedman friedman friedman boston housing tipping relevance vector classification extend relevance vector approach case classification desired predict posterior probability class membership given input generalise linear model applying logistic sigmoid function writing likelihood tlw ii xn tn xn however cannot integrate weights obtain marginal likelihood analytically utilise iterative procedure based mackay current fixed values find probable weights wmp location posterior mode equivalent standard optimisation regularised logistic model use efficient iterativelyreweighted least squares algorithm find maximum compute hessian wmp vlogp ct wmr tb bnn xn xn negated inverted give covariance gaussian approximation posterior weights hyperparameters et updated using note noise variance procedure repeated suitable convergence criteria satisfied note bayesian treatment multilayer neural networks gaussian approximation considered weakness method posterior mode unrepresentative overall probability mass however rvm note wle log concave hessian negative definite everywhere gives us considerably confidence gaussian approximation examples rvm classification synthetic example gaussian mixture data first utilise artificially generated data two dimensions order illustrate graphically selection relevance vectors class denoted sampled single gaussian overlaps small degree class sampled mixture two gaussians relevance vector classifier compared support vector counterpart using gaussian kernel value svm selected using fold crossvalidation training set results typical dataset examples given figure test errors rvm svm comparable remarkable feature contrast complexity classifiers support vector machine utilises kernel functions compared relevance vector method also notable relevance vectors distance decision boundary space given analysis observation seen consistent hyperparameter update equations qualitative explanation output basis function lying near decision boundary poor indicator class membership basis functions naturally penalised bayesian framework relevance vector machine svm error vectors figure results training functionally identical svm left rvm right classifters typical synthetic dataset decision boundary shown dashed relevance support vectors shown circled emphasise dramatic reduction complexity rvm model real examples table give error complexity results pima indian diabetes handwritten digit datasets former task recently used illustrate bayesian classification related gaussian process gp technique utilised authors split data training test examples quote result gp case latter dataset popular support vector benchmark comprising training examples along example test set svm result quoted errors kernels dataset svm gp rvm svm gp rvm pima indians terms prediction accuracy rvm marginally superior pima set outperformed svm digit data however consistent examples paper rvm classifiers utilise many fewer kernel functions strikingly rvm achieves state art performance diabetes dataset kernels noted reduced set methods exist subsequently pruning support vector models reduce required number kernels expense increase error see example results data discussion examples paper effectively demonstrated relevance vector machine attain comparable regression apparently superior level generalisation accuracy well established support vector approach time utilising dramatically fewer kernel functions implying considerable tipping saving memory computation practical implementation importantly also benefit absence additional nuisance parameters set apart need choose type kernel associated parameters fact case kernel parameters obtained improved terms accuracy sparsity results benchmarks given section optimising marginal likelihood respect multiple input scale parameters gaussian kernels furthermore may also exploit bayesian formalism guide choice kernel noted presented methodology applicable arbitrary basis functions limited example use mercer kernels svm advantage rvm classifier standard formulation probabilistic generalised linear model implies extended multiple class case st aightforward principled manner without need train heuristically combine multiple dichotomous classifiers standard practice svm furthermore estimation posterior probabilities class membership major benefit convey principled measure uncertainty prediction essential wish allow adaptation varying class priors along incorporation asymmetric misclassification costs however must noted principal disadvantage relevance vector methods complexity training phase necessary repeatedly compute invert hessian matrix requiring storage computation large datasets makes training considerably slower svm currently memory constraints limit us training examples developed approximation methods handling larger datasets employed handwritten digit databas note case bayesian methods generally strongest data scarce sparseness resulting classifier induced bayesian framework presented compelling motivation apply relevance vector techniques larger datasets acknowledgements author wishes thank chris bishop john platt bernhard schslkopf helpful discussions jp sequential minimal optimisation code
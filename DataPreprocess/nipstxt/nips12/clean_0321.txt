abstract article study effects introducing structure input distribution data learnt simple perceptron determine learning curves within framework statistical mechanics stepwise generalization occurs function number examples distribution patterns highly anisotropic although extremely simple model seems capture relevant features class support vector machines recently shown present behavior introduction new approach learning recently proposed alternative feedforward neural networks support vector machines svm instead trying learn non linear mapping input patterns internal representations like multilayered perceptrons svms choose priori non linear kernel transforms input space high dimensional feature space binary classification tasks like considered present paper svms look linear separation optimal margin feature space main advantage svms learning becomes convex optimization problem difficulties many local minima hinder process training multilayered neural networks thus avoided one questions raised approach svms overfit data spite extremely large dimensions feature spaces considered two recent theoretical papers studied family svms tools statistical mechanics predicting typical properties limit large dimensional spaces papers considered mappings generated polynomial kernels specifically quadratic ones input vectors transformed dimensional feature vectors precisely mapping xlx xkx studied function number quadratic features xlx xnx considered leading different results mappings particular cases quadratic kernels particular case learning quadratically separable tasks mapping generalization error decreases lower bound number examples proportional followed decrease number examples increases proportionally dimension feature risau gusman gordon space fact behavior specific svms also arises typical case gibbs learning defined quadratic feature spaces increasing training set size quadratic components discriminating surface learnt linear ones case learning linearly separable tasks quadratic feature spaces effect overfitting harmless slows decrease generalization error training set size case mapping overfitting dramatic generalization error given training set size increases number features aim present paper understand influence mapping scalingfactor generalization performance svms end worth remark features may obtained compressing quadratic subspace fixed factor order mimic contraction consider linearly separable task input patterns highly anisotropic distribution variance one subspace much smaller orthogonal directions show simple toy model generalization error function training set size exhibits cross two different behaviors rapid decrease corresponding learning components uncompressed space followed slow improvement mainly components compressed space learnt latter would correspond highly stylized model learning scaled quadratic features svm mapping paper organized follows short presentation model describe main steps statistical mechanics calculation order parameters caracterizing properties learning process defined evolution function training set size analyzed two regimes generalization error described determine training set size per input dimension crossover function pertinent parameters finally discuss results relevance understanding generalization properties svms model consider problem learning binary classification task examples training data set contains dimensional patterns sign given teacher weights wn without loss generality consider normalized teachers assume components input patterns independent identically distributed random variables drawn zero mean gaussian distribution variance along nc directions unit variance nu remaining ones nc nu ra exp ff exp take without loss generality case may deduced former straightforward rescaling nc nu hereafter subspace dimension nc variance called compressed subspace corresponding orthogonal subspace dimension nu nc called uncompressed subspace study typical generalization error student perceptron learning classification task using tools statistical mechanics pertinent cost function understanding stepwise generalization svm toy model number misclassified patterns weight vectors version space correspond vanishing cost choosing random posteriori distribution po exp limit called gibbs learning eq equivalent inverse temperature statistical mechanics formulation cost energy function assume priori distribution weights uniform hypersphere radius po normalization constant leading order term hypersphere surface dimensional space partition function ensuring correct normalization wl dw po exp general properties student related free energy lnz limit training set size per input dimension pin constant properties student weights become independant particular training set deduced averaged free energy per degree freedom calculated using replica trick lnz va lim lnzn va overline represents average composed patterns selected according case gibbs learning typical behavior intensive quantity obtained zero temperature limit oc limit errorfree solutions vanishing cost non vanishing posterior probability thus gibbs learning corresponds picking random student version space vector classifies correctly training set probability proportional case isotropic pattern distribution corresponds properties cost function extensively studied case patterns drawn two gaussian clusters symmetry axis clusters different teacher axis recently addressed consider problem instead single direction along patterns distribution contracted expanded finite fraction compressed dimensions case properties student perceptron may expressed terms following order parameters satisfy corresponding extremum conditions free energy ua aw risau gusman gordon waw nc aw wi indicates average posterior replica indices subcripts stand compressed uncompressed respectively notice impose typical squared norm student components compressed subspace equal corresponding teacher norm ien order parameters learning curves assuming order parameters invariant permutation replicas drop replica indices equations expect hypothesis replica symmetry consistent like cases perceptrons learning realizable tasks problem thus reduced determination five order parameters meaning becomes clearer consider following combinations qc qu rc ru kc qx wi qc qu typical overlaps components two student vectors compressed uncompressed subspaces respectively similarly rc ru corresponding overlaps typical student teacher terms set parameters typical generalization error eg arccos rc ruv given general solution extremum conditions depends three parameters problem namely nc nc interesting case one teacher anisotropy consistent pattern distribution nc case easy show qc rc qu thus nuru ncrc ff nu nu ru rc given following equations rc exp rt nc nu tv understanding stepwise generalization svm toy model rg figure order parameters generalization error case nc curves case spherically distributed patterns shown comparison inset shows first step learning plateau see text ac au rc ru dt recover equations corresponding gibbs learning isotropic pattern distributions order parameters represented function figure particular choice nc ru grows much faster rc meaning easier learn components uncompressed space result therefore generalization error presents cross two behaviors small ru ra nu nc nu nc ra overlap gibbs learning isotropic distribution learning anisotropic distribution faster learning isotropic one anisotropy large increases like ra effective training set size per input dimension increasing intermediate regime ru increases rc nu nc corresponding generalization error seems reach plateau corresponding rc asymptotic behavior independent details distribution like crossover two regimes occurs nu nc nc cases also interest corresponds teacher weights components compressed subspace whereas risau gusman gordon figure generalization errors function different teachers case nc curve spherically distributed patterns included comparison inset shows large alpha behaviors corresponds teacher orthogonal compressed subspace components uncompressed subspace correspond respectively tasks either uncompressed compressed components irrelevant patterns classification figure show generalization error curves including generalization error egg uniform distribution comparison behaviour eg sensitive value teacher compressed subspace learning difficult consequently egg expected contrary components uncompressed space relevant classification task subspace learning easy eg egg crossover regimes already discussed curves merge asymptotic regime may seen inset figure discussion analyzed typical learning behavior toy perceptton model allows clarify aspects generalization high dimensional feature spaces particular captures element essential obtain stepwise learning shown stem compression high order features components compressed space difficult learn compressed thus understanding stepwise generalization svm toy model training set large enough mainly latter learnt results allow understand importance scaling high order features svms kernels fact svms one choose priori kernel maps input space feature space high order features conveniently compressed hierarchical learning occurs low order features learnt first higher order features learnt training set large enough cases higher order features irrelevant likely hinder learning process interesting behavior allows avoid overfitting computer simulations currently progress svms generated quadratic kernels without scaling show behavior consistent theoretical predictions may understood present toy model
abstract provide abstract characterization boosting algorithms gradient decsent cost functionals inner product function space prove convergence functional gradient descent algorithms quite weak conditions following previous theoretical results bounding generalization performance convex combinations classifiers terms general cost functions margin present new algorithm doom ii performing gradient descent optimization cost functions experiments several data sets uc irvine repository demonstrate doom ii generally outperforms adaboost especially high noise situations overfitting behaviour adaboost predicted cost functions introduction considerable interest recently voting methods pattern classification predict label particular example using weighted vote set base classifiers recent theoretical results suggest effectiveness algorithms due tendency produce large margin classifiers loosely speaking combination classifiers correctly classifies training data large margin error probability small gave improved upper bounds misclassification probability combined classifier terms average training data certain cost function margins paper also described doom algorithm directly minimizing margin cost function adjusting weights associated boosting algorithms gradient descent base classifier base classifiers suppiled doom doom exhibits performance improvements adaboost even using base hypotheses provides additional empirical evidence margin cost functions appropriate quantities optimize paper present general class algorithms called anyboost gradient descent algorithms choosing linear combinations elements inner product function space minimize cost functional normal operation weak learner shown equivalent maximizing certain inner product prove convergence anyboost weak conditions section show general class algorithms includes special cases nearly existing voting methods section present experimental results special case anyboost minimizes theoretically motivated margin cost functional experiments show new algorithm typically outperforms adaboost especially true label noise addition theoretically motivated cost functions provide good estimates error adaboost sense used predict overfitting behaviour anyboost let denote examples space measurements typically ir space labels usually discrete set subset ir let denote class functions base hypotheses mapping lin denote set linear combinations functions let inner product lin lin cost functional lin aim find function lin minimizing proceed iteratively via gradient descent procedure suppose lin wish find new add cost el decreases small value viewed function space terms asking direction el rapidly decreases desired direction simply negative functional derivative vc oc al vc oc indicator function since restricted choosing new function general possible choose vc instead search greatest inner product vc choose maximize vc motivated observing first order el vc hence greatest reduction cost occur maximizing vc reasons become obvious later algorithm chooses attempting maximize vc described weak learner preceding discussion motivates algorithm anyboost iterative algorithm finding linear combinations base hypotheses minimize cost functional note allowed base hypotheses take values arbitrary set restricted form cost inner product specified step sizes appropriate choices mason baxter bartlett andm frean things made apply algorithm concrete situations note also algorithm terminates vc ft ft weak learner returns base hypothesis ft longer points downhill direction cost function thus algorithm terminates first order step function space direction base hypothesis returned would increase cost algorithm anyboost require inner product space containing functions mapping set class base classifiers differentiable cost functional lin weak learner accepts lin returns large value vc let fo fort ototdo let ft vu ft ft return ft end choose wt let ft ft wtq lfiq end return ft gradient descent view voting methods restrict attention base hypotheses mapping inner product xi xi lin xl yl set training examples generated according unknown distribution aim find lin pr sgn minimal sgn sgn otherwise words sgn minimize misclassification probability margin example defined yf consider margin cost functionals defined differentiable real valued function margin definitions quick calculation shows ra vc rn yif xi yif xi since positive margins correspond examples correctly labelled sgn negative margins incorrectly labelled examples sensible cost function boosting algorithms gradient descent table existing voting methods viewed anyboost margin cost functions algorithm cost function step size adaboost yf line search arc yf confidenceboost yf line search logitboost ln yf newton raphson margin monotonically decreasing hence yif xi always positive dividing eim yif xi see finding maximizing vc equivalent finding minimizing weighted error yir xi eim yif xi many successful voting methods appropriate choice margin cost function step size specific cases anyboost algorithm see table detailed analysis found full version paper convergence anyboost section provide convergence results anyboost algorithm quite weak conditions cost functional prescriptions given step sizes wt results convergence guarantees practice almost always smaller necessary hence fixed small steps form line search used following theorem proof omitted see supplies specific step size anyboost characterizes limiting behaviour step size theorem let lin lower bounded lipschitz differentiable cost functional exists vc vc ii lin let fo sequence combined hypotheses generated anyboost algorithm using step sizes vc ft ft wtq lllf anyboost either halts round ft ftq ft converges finite value case limt ivc ft ft next theorem proof omitted see shows weak learner always find best weak hypothesis ft round anyboost cost functional convex accumulation point sequence ft generated anyboost step sizes global minimum cost ease exposition assumed rather terminating ft ft anyboost simply continues return ft subsequent time steps theorem let lin convex cost functional properties theorem let ft sequence combined hypotheses generated anyboost algorithm step sizes given assume weak hypothesis class negation closed round mason baxter bartlett andm frean anyboost algorithm finds function ft maximizing vc ft ft accumulation point sequence ft satisfies supfe vc inf experiments adaboost perceived resistant overfitting despite fact produce combinations involving large numbers classifiers however recent studies shown case even base classifiers simple decision stumps overfitting attributed use exponential margin cost functions recall table results showed overfitting may avoided using margin cost functionals form qualitatively similar tanh ayif xi adjustable parameter controlling steepness margin cost function tanh theoretical analysis apply must convex combination base hypotheses rather general linear combination henceforth referred normalized sigmoid cost functional anyboost cost functional inner product referred doom ii implementation doom ii use fixed small step size experiments details algorithm reader referred full version paper compared performance doom ii ariaboost selection nine data sets taken uci machine learning repository various levels label noise applied simplify matters binary classification problems considered experiments axis orthogonal hyperplanes also known decision stumps used weak learner full details experimental setup may found summary experimental results shown figure improvement test error exhibited doom ii ariaboost shown data set noise level doom ii generally outperforms adaboost improvement pronounced presence label noise effect using normalized sigmoid cost function rather exponential cost function best illustrated comparing cumulative margin distributions generated adaboost doom ii figure shows comparisons two data sets label noise applied given margin value curve corresponds proportion training examples margin less equal value curves show trying increase margins negative examples adaboost willing sacrifice margin positive examples significantly contrast doom ii gives examples large negative margin order reduce value cost function given adaboost suffer overfitting guaranteed minimize exponential cost function margins cost function certainly relate test error value proposed cost function correlate adaboost test error figure shows variation normalized sigmoid cost function exponential cost function test error adaboost two uci data sets rounds strong correlation normalized sigmoid cost adaboost test error data sets minimum boosting algorithms gradient descent sonar cleve ionosphere vote credit breazt cancer data set noise noise ama md ans hypol splice figure summary test error advantage standard error bars doom ii adaboost varying levels noise nine uci data sets breast cancer wisconsin noise dooivl ii noise adaboost noise doom ii margin splice ilcilse adaboost noise doom ii noi ad oost gin figure margin distributions adaboost doom ii label noise breast cancer splice data sets adaboost test error minimum normalized sigmoid cost nearly coincide showing sigmoid cost function predicts ariaboost start overfit references bartlett sample complexity pattern classification neural networks size weights important size network ieee transactions information theory march breiman bagging predictors machine learning breiman prediction games arcing algorithms technical report department statistics university california berkeley keogh blake merz uci repository machine learning databases http www ics uci edu mlearn mlrepository html dietterich experimental comparison three methods constructing ensembles decision trees bagging boosting randomization technical report computer science department oregon state university mason baxter bartlett andm frean labor adaboost res tor exponential cc st normalized sigmoid cost rounds vote ariaboy st st exponential cost norm dized sigmoid cost oo rounds figure adaboost test error exponential cost normalized sigmoid cost rounds adaboost labor votel data sets costs scaled case easier comparison test error drucker cortes boosting decision trees advances neural information processing systems pages duffy helmbold geometric approach leveraging weak learners computational learning theory dth european conference appear freund adaptive version boost majority algorithm proceedings twelfth annual conference computational learning theory appear freund schapire experiments new boosting algorithm machine learning proceedings thirteenth international conference pages freund schapire decision theoretic generalization line learning application boosting journal computer system sciences august friedman greedy function approximation gradient boosting machine technical report stanford university friedman hastie tibshirani additive logistic regression statistical view boosting technical report stanford university grove schuurmans boosting limit maximizing margin learned ensembles proceedings fifteenth national conference artificial intelligence pages mason bartlett baxter improved generalization explicit optimization margins machine learning appear llew mason jonathan baxter peter bartlett marcus frean functional gradient techniques combining hypotheses alex smola peter bartlett bernard schslkopf dale schurmanns editors large margin classifiers mit press appear quinlan bagging boosting proceedings thirteenth national conference artificial intelligence pages itsch onoda iller soft margins adaboost technical report nc tr department computer science royal holloway university london egham uk schapire freund bartlett lee boosting margin new explanation effectiveness voting methods annals statistics october schapire singer improved boosting algorithms using confidence rated predictions proceedings eleventh annual conference computational learning theory pages
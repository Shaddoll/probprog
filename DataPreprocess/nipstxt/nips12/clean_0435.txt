abstract mani problem correct behavior model depend input output map also properti jacobian matrix matrix partial deriv model output respect input introduc prop algorithm effici gener method comput exact partial deriv varieti simpl function jacobian model respect free paramet algorithm appli parametr feedforward model includ nonlinear regress multilay perceptron radial basi function network introduct let input output twice differenti feedforward model parameter input vector weight vector jacobian matrix defin df dx algorithm introduc use optim function form ev iijv bll user defin constant algorithm call prop use calcul exact valu oeu ow oev ow time time requir calcul normal gradient thu prop suitabl train model specif first deriv implement sever well known algorithm doubl backpropag tangent prop clearli abl optim equat use howev suspect formal use deriv algorithm actual interest allow us modifi prop easili applic wide varieti model type flake pearlmutt object function spend fair portion paper describ mathemat framework later build prop paper divid four section section contain background inform motiv optim properti jacobian import problem section introduc formal contain deriv prop algorithm section contain brief numer exampl prop final section describ work give conclus background motiv previou work concern model unknown function deriv divid work descript prescript perhap best known descript result due white et al show given nois free data multilay perceptron mlp approxim higher deriv unknown function limit number train point goe infin difficulti appli result strong requir amount integr train data requir rare met practic problem specif demonstr princip rathi kuo deco schfirmann show use noisi train data chaotic system lead model accur input output sens inaccur estim quantifi relat jacobian unknown system largest lyapunov expon correl dimens mlp particularli problemat larg weight lead satur particular sigmoid neuron turn result extrem larg first deriv neuron evalu near center sigmoid transit sever method combat type fit propos one earliest method weight decay use penalti term magnitud weight weight decay arguabl optim model output linear weight minim magnitud weight equival minim magnitud model first deriv howev nonlinear case weight decay suboptim perform larg small weight alway correspond larg small first deriv doubl backpropag algorithm add addit penalti term error function equal lcge cga train function result form regular mani way eleg combin weight decay train nois strictli analyt unlik train nois explicitli penal larg first deriv model unlik weight decay doubl backpropag seen special case prop algorithm deriv paper gener problem coerc first deriv model specif valu simard et al introduc tangent prop algorithm use train mlp optic charact recognit insensit small affin transform charact space tangent prop also consid special case prop deriv defin formal prop easili deriv method similar techniqu introduc pearlmutt calcul product hessian mlp arbitrari vector howev pearlmutt use differenti oper appli model weight space use differenti oper defin respect model input space entir deriv present five step first defin auxiliari error differenti funca on jacobian function use mathemat properti simplifi deriv next defin special differenti oper appli auxiliari error function gradient respect weight see result appli differenti oper gradient auxiliari error function equival analyt calcul deriv requir optim equat show exampl techniqu appli mlp final last step complet algorithm present avoid confus refer gener data driven model model alway express vector function refer model input refer vector tunabl paramet model way talk model ignor mechan model work intern complementari gener vector notat notat mlp use scalar symbol howev symbol must refer intern variabl model neuron threshold net input weight etc lead ambigu clear use vector notat input output mlp alway denot respect collect weight includ bias map vector howev use scalar arithmet scalar notat mlp appli auxiliari error function auxiliari error function defin urf note never actual optim respect defin properti utj use deriv shortli note appear taylor expans point input space thu hold weight fix let ax perturb input equat character small chang input model chang valu auxiliari error function set ax rv arbitrari vector small valu rearrang equat form lim rv jr rv final express allow us defin differenti oper next subsect differenti oper let arbitrari twice differenti function oper rv defin differenti flake pearlmutt properti urjv differenti oper obey standard rule differenti oper also yield ident equival see result calcul use calcul oe oto oev oto note equat assum independ calcul oeu oto oev oto actual set valu depend howev deriv still work choic explicitli made way chain rule differenti suppos appli term henc correct analyt solut obtain despit depend optim respect equat use yj wij equat superscript denot layer number start subscript index term particular layer nt number input node layer thu net input come neuron output neuron node layer moreov yp output entir mlp input go mlp feedback equat calcul respect ui jttt optim respect equat use jv jv tlojv ow wj jv method appli mlp readi see techniqu appli specif type model consid mlp layer node defin equat fi nt differenti function jacobjan wij ijxj ox oy yj ow term compon vector equat appli oper feedforward equat yield xi rv xi rv wlij vi term compon vector equat final step appli oper feedback equat yield rv oy oyi owti yj yj ox complet algorithm implement algorithm nearli simpl implement normal gradient descent type variabl use mlp net input neuron output weight threshold partial deriv etc requir extra variabl alloc hold result appli oper origin variabl chang place complet algorithm comput oe ow follow set user specifi vector equat set mlp input valu evalu perform normal feedforward pass use equat set yi ui flake pearlmutt co figur learn deriv show poor approxim function excel approxim deriv perform feedback pass equat note valu term equal jtu set jtu perform rv forward pass equat set gt term perform rv backward pass equat last step valu ot owtij term contain requir result import note time complex thd forward jbackward calcul nearli ident typic output gradient evalu forward backward pass model use similar techniqu use calcul oev ow main differ rv forward pass perform normal forward backward pass determin calcul experiment result demonstr effect gener prop algorithm implement top exist neural network librari way algorithm use larg number architectur includ mlp radial basi function network higher order network train mlp ten hidden tanh node point conjug gradient train exemplar consist input target deriv co co unknown function mlp never see data sin sin model quickli converg solut approxim iter figur show perform mlp never seen data unknown function mlp yield poor approxim function accur approxim function deriv could train output deriv goal illustr prop target deriv alon differenti function jacobian conclus introduc gener method calcul weight gradient function jacobian matrix feedforward nonlinear system method easili appli nonlinear model common use today result algorithm prop easili modifi minim function sever applic domain possibl use includ target known first deriv implement tangent prop doubl backpropag enforc ident sensit auto encod deflat largest eigenvalu minim eigenvalu bound optim determin blind sourc separ build nonlinear control special case prop algorithm alreadi studi great deal unknown optim jacobian chang overal optim problem anecdot evid seem impli optim jacobian lead better gener faster train remain seen prop use nonlinear extens linear method lead superior solut acknowledg thank fran coetze yanni kevrekidi joe ruanaidh luca parra scott rickard justinian rosca patric simard help discuss gwf would also like thank eric baum nec research institut fund time write result
abstract many problems correct behavior model depends input output mapping also properties jacobian matrix matrix partial derivatives model outputs respect inputs introduce prop algorithm efficient general method computing exact partial derivatives variety simple functions jacobian model respect free parameters algorithm applies parametrized feedforward model including nonlinear regression multilayer perceptrons radial basis function networks introduction let input output twice differentiable feedforward model parameterized input vector weight vector jacobian matrix defined df dx algorithm introduce used optimize functions form ev iijv bll user defined constants algorithm call prop used calculate exact value oeu ow oev ow times time required calculate normal gradient thus prop suitable training models specific first derivatives implementing several well known algorithms double backpropagation tangent prop clearly able optimize equations useful however suspect formalism use derive algorithm actually interesting allows us modify prop easily applicable wide variety model types flake pearlmutter objective functions spend fair portion paper describing mathematical framework later build prop paper divided four sections section contains background information motivation optimizing properties jacobian important problem section introduces formalism contains derivation prop algorithm section contains brief numerical example prop finally section describes work gives conclusions background motivation previous work concerning modeling unknown function derivatives divided works descriptive prescriptive perhaps best known descriptive result due white et al show given noise free data multilayer perceptron mlp approximate higher derivatives unknown function limit number training points goes infinity difficulty applying result strong requirements amount integrity training data requirements rarely met practice problem specifically demonstrated principe rathie kuo deco schfirmann showed using noisy training data chaotic systems lead models accurate input output sense inaccurate estimates quantifies related jacobian unknown system largest lyapunov exponent correlation dimension mlps particularly problematic large weights lead saturation particular sigmoidal neuron turn results extremely large first derivatives neuron evaluated near center sigmoid transition several methods combat type fitting proposed one earliest methods weight decay uses penalty term magnitude weights weight decay arguably optimal models output linear weights minimizing magnitude weights equivalent minimizing magnitude model first derivatives however nonlinear case weight decay suboptimal performance large small weights always correspond large small first derivatives double backpropagation algorithm adds additional penalty term error function equal lcge cga training function results form regularization many ways elegant combination weight decay training noise strictly analytic unlike training noise explicitly penalizes large first derivatives model unlike weight decay double backpropagation seen special case prop algorithm derived paper general problem coercing first derivatives model specific values simard et al introduced tangent prop algorithm used train mlps optical character recognition insensitive small affine transformations character space tangent prop also considered special case prop derivation define formalism prop easily derived method similar technique introduced pearlmutter calculating product hessian mlp arbitrary vector however pearlmutter used differential operators applied model weight space use differential operators defined respect model input space entire derivation presented five steps first define auxiliary error differentiating funca ons jacobian function useful mathematical properties simplify derivation next define special differential operator applied auxiliary error function gradient respect weights see result applying differential operator gradient auxiliary error function equivalent analytically calculating derivatives required optimize equations show example technique applied mlp finally last step complete algorithm presented avoid confusion referring generic data driven models model always expressed vector function refers model input refers vector tunable parameters model way talk models ignoring mechanics models work internally complementary generic vector notation notation mlp uses scalar symbols however symbols must refer internal variable model neuron thresholds net inputs weights etc lead ambiguity clear using vector notation input output mlp always denoted respectively collection weights including biases map vector however using scalar arithmetic scalar notation mlps apply auxiliary error function auxiliary error function defined urf note never actually optimize respect define property utj useful derivation shortly note appears taylor expansion point input space thus holding weights fixed letting ax perturbation input equation characterizes small changes input model change value auxiliary error function setting ax rv arbitrary vector small value rearrange equation form lim rv jr rv final expression allow us define differential operator next subsection differential operator let arbitrary twice differentiable function operator rv define differentiable flake pearlmutter property urjv differential operator obeys standard rules differentiation operator also yields identity equivalence see result calculating used calculate oe oto oev oto note equations assume independent calculate oeu oto oev oto actually set value depends however derivation still works choices explicitly made way chain rule differentiation supposed applied terms hence correct analytical solution obtained despite dependence optimize respect equation use yj wij equations superscripts denote layer number starting subscripts index terms particular layer nt number input nodes layer thus net input coming neuron output neuron node layer moreover yp output entire mlp input going mlp feedback equations calculated respect ui jttt optimize respect equation use jv jv tlojv ow wj jv method applied mlps ready see technique applied specific type model consider mlp layers nodes defined equations fi nt differentiating functions jacobjan wij ijxj ox oy yj ow term component vector equation applying operator feedforward equations yields xi rv xi rv wlij vi term component vector equation final step apply operator feedback equations yields rv oy oyi owti yj yj ox complete algorithm implementing algorithm nearly simple implementing normal gradient descent type variable used mlp net input neuron output weights thresholds partial derivatives etc require extra variable allocated hold result applying operator original variable change place complete algorithm compute oe ow follows set user specified vectors equation set mlp inputs value evaluated perform normal feedforward pass using equations set yi ui flake pearlmutter co figure learning derivative showing poor approximation function excellent approximation derivative perform feedback pass equations note values terms equal jtu set jtu perform rv forward pass equations set gt terms perform rv backward pass equations last step values ot owtij terms contain required result important note time complexity thd forward jbackward calculations nearly identical typical output gradient evaluations forward backward passes models used similar technique used calculating oev ow main difference rv forward pass performed normal forward backward passes determined calculated experimental results demonstrate effectiveness generality prop algorithm implemented top existing neural network library way algorithm used large number architectures including mlps radial basis function networks higher order networks trained mlp ten hidden tanh nodes points conjugate gradient training exemplars consisted inputs target derivative cos cos unknown function mlp never sees data sin sin model quickly converges solution approximately iterations figure shows performance mlp never seen data unknown function mlp yields poor approximation function accurate approximation function derivative could trained outputs derivatives goal illustrate prop target derivatives alone differentiating functions jacobian conclusions introduced general method calculating weight gradient functions jacobian matrix feedforward nonlinear systems method easily applied nonlinear models common use today resulting algorithm prop easily modified minimize functionals several application domains possible uses include targeting known first derivatives implementing tangent prop double backpropagation enforcing identical sensitivities auto encoders deflating largest eigenvalue minimizing eigenvalue bounds optimizing determinant blind source separation building nonlinear controllers special cases prop algorithm already studied great deal unknown optimization jacobian changes overall optimization problem anecdotal evidence seems imply optimization jacobian lead better generalization faster training remains seen prop used nonlinear extension linear methods lead superior solutions acknowledgements thank frans coetzee yannis kevrekidis joe ruanaidh lucas parra scott rickard justinian rosca patrice simard helpful discussions gwf would also like thank eric baum nec research institute funding time write results
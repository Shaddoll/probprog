abstract everybody knows neural networks need single layer nonlinear units compute interesting functions show false one employs winner take nonlinear unit boolean function computed single winner takeall unit applied weighted sums input variables continuous function approximated arbitrarily well single soft winner take unit applied weighted sums input variables positive weights needed linear weighted sums may interest point view neurophysiology since synapses cortex inhibitory addition widely believed special microcircuits cortex compute winner take results support view winner take useful basic computational unit neural vlsi wellknown winner take input variables computed efficiently transistors total wire length area linear analog vlsi lazzaro et al show winner take useful special purpose computations may serve nonlinear unit neural circuits universal computational power show multi layer perceptron needs quadratically many gates compute winner take input variables hence winner take provides substantially powerful computational unit perceptron cost implementation analog vlsi complete proofs details results found maass maass introduction computational models involve competitive stages far neglected computational complexity theory although widely used computational brain models artificial neural networks analog vlsi circuit lazzaro et al computes approximate version winner take inputs transistors wires length lateral inhibition implemented adding currents single wire length numerous efficient implementations winner take analog vlsi subsequently produced among circuits based silicon spiking neurons meador hylander indiveri circuits emulate attention artificial sensory processing horiuchi et al indiveri preceding analytical results winner take circuits found grossberg brown analyze section computational power basic competitive computational operation winner take wta section discuss somewhat complex operation winner take wtan also implemented analog vlsi urahama nagao section devoted soft winner take implemented indiveri analog vlsi via temporal coding output results shows winner take surprisingly powerful computational module comparison threshold gates mcculloch pitts neurons sigmoidal gates theoretical analysis also provides answers two basic questions raised neurophysiologists view well known asymmetry excitatory inhibitory connections cortical circuits much computational power neural networks lost positive weights employed weighted linear sums much learning capability lost positive weights subject plasticity restructuring neural circuits digital output investigate section computational power winner take gate computing function wta xl bl wtan bi xi among largest inputs xl precisely bi xj xi holds indices neural computation nner take theorem two layer feedforward circuit analog binary input variables one binary output variable consisting threshom gates perceptrons simulated circuit consisting single winner take gate wtan applied weighted sums input variables positive weights holds digital inputs analog inputs except set inputs measure particular boolean function computed single winner take gate applied positive weighted sums input bits remarks polynomial size integer weights whose size bounded polynomial number linear gates bounded polynomial weights simulating circuit natural numbers whose size bounded polynomial exception set measure result union finitely many hyperplanes one easily show exception set measure theorem necessary circuit structure converted back layer threshold circuit number gates quadratic number weighted sums linear gates relies construction section proof theorem since outputs gates hidden layer assume without loss generality weights cti ct output gate see example siu et al details one first observes suffices use integer weights threshold gates binary inputs one normalize weights values duplicating gates hidden layer thus circuit input ctj gj gn threshold gates hidden layer ctl ct threshold output gate order eliminate negative weights replace gate gj another threshold gate except hyperplane set oj gj except exception set consisting hyperplanes ect gj esj hencec suitable let wj weights threshold gate use last output bit exploit wizi wi zi arbitrary wi zi maass zm gn gi gn arbitrary threshold gates threshold gate weights zm back sn linear gates positive weights sums absolute values weights gates gi gn thus iw lzi iw lzi oj hence wf forj every every ir sn ilz lz implies st output bn winner take gate wta neural computation winner take applied satisfies bn sn note coefficients sums sn positive restructuring neural circuits analog output order approximate arbitrary continuous functions values circuits similar structure preceding section consider variation winner take gate outputs analog numbers whose values depend rank corresponding input linear order input numbers one may argue gate longer winner take gate agreement common terminology refer soft winner take gate gate computes function soft winner take whose ith output roughly proportional rank xi among numbers xl xn precisely parameter set xi xj rounded value outside hence gate focuses inputs xi whose rank among input numbers xn belongs set min ranks linearly scaled theorem circuits consisting single soft winner take gate use first output applied positive weighted sums input variables universal approximators arbitrary continuous functions shown maass actually continuous monotone scaling used instead maass circuit type considered theorem soft winner take gate applied positive weighted sums simple geometrical interpretation point input plane consider relative heights hyperplanes defined positive weighted sums circuit output depends many hyperplanes point lower bound result winner take one easily see wta gate inputs computed layer threshold circuit consisting threshold gates xi xl xj threshold gates threshold gates hence following result provides optimal lower bound theorem feedforward threshold circuit multi layer perceptron computes wta inputs needs least gates conclusions lower bound result theorem shows computational power winner takeall quite large even compared arguably powerful gate commonly studied circuit complexity theory threshold gate also referred mcculloch pitts neuron perceptron neural computation winner take well known minsky papert single threshold gate able compute certain important functions whereas circuits moderate polynomial size consisting two layers threshold gates polynomial size integer weights remarkable computational power see siu et al shown theorem layer hidden layer circuit simulated single winner take gate applied polynomially many weighted sums positive integer weights polynomial size also analyzed computational power soft winner take gates context analog computation shown theorem single soft winner take gate may serve nonlinearity class circuits universal computational power sense approximate continuous functions furthermore novel universal approximators require positive linear operations besides soft winner take thereby showing principle computational power lost biological neural system inhibition used exclusively unspecific lateral inhibition adaptive flexibility lost synaptic plasticity learning restricted excitatory synapses somewhat surprising results regarding computational power universality winner take point opportunities low power analog vlsi chips since winner take implemented efficiently technology
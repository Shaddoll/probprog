abstract gaussian mixtures called radial basis function networks density estimation provide natural counterpart sigmoidal neural networks function fitting approximation cases possible give simple expressions iterative improvement performance components network introduced one time particular mixture density estimation show component mixture estimated maximum likelihood iterative likelihood improvement introduce achieves log likelihood within order log likelihood achievable convex combination consequences approximation estimation using kullback leibler risk also given minimum description length principle selects optimal number components minimizes risk bound introduction density estimation gaussian mixtures provide flexible basis representations densities used model heterogeneous data high dimensions introduce ar index regularity cy density functions respect mixtures densities given family mixture models components shown achieve kullback leibler approximation error bounded every thus manner analogous treatment sinusoidal sigrnoidal networks barron find classes density functions reasonable size networks exponentially large function input dimension achieve suitable approximation estimation error consider parametric family probability density functions parameterized consider class conv density functions mixture representation form fo density functions probability measure main theme paper give approximation estimation bounds arbitrary densities finite mixture densities focus attention densities li barron inside first give approximation error bound finite mixtures arbitrary approximation error measured kullback leibler divergence two densities defined fllg log dx density estimation natural use distance often seen function fitting literature indeed invariant scale transformations transformation variables intrinsic connection maximum likelihood one useful methods mixture density estimation following result quantifies approximation error theorem let qb conv let exists fk component mixture trio fllfk bound og dx log sup characterizes upper bound log ratio densities parameters restricted variable note rate convergence related dimensions behavior constants though depends choices target example may take gaussian location family restrict set cube side length likewise restrict parameters cube da case linear dimension value depends target density suppose finite mixture components equality components disjoint indeed suppose lpiqboi piqbo im piqboi hence bo bo dx cfio dx pio genovese wasserman deal similar setting kullback leibler approximation bound order vr one dimensional mixtures gaussians given general case necessarily competitive optimality result density approximation nearly least good gp mixture density estimation theorem every gp fllf fllgp fck dx particular take infimum gr still obtain bound let flic infgec fllg theory information projection shows exists sequence fk fllfk fll converges function achieves fll note necessarily element developed li building work bell cover consequence theorem fllf fllf smallest limit sequences achieving fllg approaches infimum fllc prove theorem induction following section appealing feature approach provides iterative estimation procedure allows us estimate one component time greedy procedure shown perform almost well full mixture procedures computational task estimating one component considerably easier estimating full mixtures section gives iterative construction suitable approximation section shows mixtures may estimated data risk bounds stated section iterative construction approximation provide iterative construction following fashion suppose discussion approximation given seek component mixture close initialize fx choosing single component minimize fllfx fllq suppose fk let acfo chosen minimize fllf generally let sequence component mixtures fllfk mina fll fk prove sequences fk achieve error bounds theorem theorem familiar iterative hilbert space approximation results jones barron lee bartlett williamson see follow similar strategy use distance measures density approximation involves norms component densities exponentially large dimension naive taylor expansion kullback leibler divergence leads norm approximation weighted reciprocal density difficulty remains zeevi gc meir li challenge us adapt iterative approximation use kullback leibler divergence manner permits constant bound involve logarithm density ratio rather ratio allow manageable constants li barron proof establishes inductive relationship dk dk bounded fllf choosing thereafter easy see induction get establish quadratic upper bound log log three key analytic inequalities regarding logarithm handy us forr ro log ro log log logr log log note application ro ratio densities thus obtain upper bound log ro involving indeed find log ro defined theorem case take taking expectation respect sides acquire quadratic upper bound dk noting also note function greedy algorithm chooses minimize therefore plugging upper bound dk log ro dxp apply respectively get log log aqb log log ro log negative part logarithm proof inequality og done verifying monotone decreasing inequalities shown separately considering cases well limit get inequalities one multiplies respectively takes derivatives obtain suitable monotonicity one moves away ro apply inequality arbitrary density op note ro case plug ro right side expand square get aqb log ro aqb log ro ro log log ro log ro ro xture density estimation ro fk chosen satisfy thus dx log log shown log log thus desired inductive relationship dk therefore case mixture representation form bop outside convex hull take log gr eiz dx given fk analysis yields fllf fllg desired completes proof theorems greedy estimation procedure connection divergence mle helps motivate following estimation procedure fk data xx sampled iterative construction turned sequential maximum likelihood estimation changing mind flirt max ell log ft xi step surprising result resulting estimator log likelihood almost least high log likelihood achieved density gp difference order formally state logfk xi logg xi gp fn empirical distribution xl cx proof result follows proof last section except take ef logg expectation respect instead respect density let look computation step see benefits new greedy procedure bring us acfo chosen maximize log fk xi ac xi simple two component mixture problem one two components fixed achieve bound either chosen iterative maximum likelihood held fixed step equal thus one may replace mle computation kcomponent mixture successive mle computations two component mixtures resulting estimate guaranteed almost least high likelihood achieved mixture density li barron disadvantage greedy procedure may take number steps adequately downweight poor initial choices thus advisable step retune weights convex combinations previous components even perhaps adjust locations components case result previous iterations components provide natural initialization search step good news long given chosen among component mixtures achieve likelihood least large choice achieving max log ak fk xi akqb xi require eloga xi moaxelog xi kqbo xi conclusion follow particular likelihood results risk bound results apply case taken global maximizer likelihood component mixtures well case result greedy procedure risk bounds mle iterative mle metric entropy family controlled obtain risk bound determine precisions coordinates parameter space allowed represented specifically following lipschitz condition assumed sup log logr bo io th coordinate parameter vector note condition satisfied gaussian family restricted cube sidelength location parameter also prescribed cube particular let variance may set state bound risk fk theorem assume condition also assume cube sidelength let fk either maximizer likelihood component mixtures generally sequence density estimates fk satisfying kd fllfk fllc log nnbe bound risk best choice would order roughly leading bound ed fk order within logarithmic factors however best bound occurs cf dlog nabe available value cl unknown importantly chosen merely optimize upper bound risk rather balance whatever approximation estimation sources error actually occur toward end optimize penalized likelihood criterion related minimum description length principle following barron cover let function satisfies log mixture density estimation penalized mle mdl procedure picks minimizing log kdlog nabe fk xi kd fllfi fll log proof risk bounds given li builds general results maximum likelihood penalized maximum likelihood procedures recently dasgupta established randomized algorithm estimating mixtures gaussians case data drawn finite mixture sufficiently separated gaussian components common covariance runs time linear dimension quadratic sample size however present forms algorithm require impractically large sample sizes get reasonably accurate estimates density yet known techniques work general mixtures see iterative likelihood maximization provides better relationship accuracy sample size number components references barron andrew universal approximation bounds superpositions sigmoidal function ieee transactions information theory barron andrew approximation estimation bounds artificial neural networks machine learning genovese chris wasserman larry rates convergence gaussian mixture seive manuscript li jonathan estimation mixture models ph dissertation department statistics yale university bell robert cover thomas game theoretic optimal portfolios management science jones lee simple lemma greedy approximation hilbert space convergence rates projection pursuit regression neural network training annals statistics lee bartlett williamson efficient agnostic learning neural networks bounded fan ieee transactions information theory zeevi assaf meir ronny density estimation convex combinations densities approximation estimation bounds neural networks li jonathan iterative estimation mixture models ph prospectus department statistics yale university barron andrew cover thomas minimum complexity density estimation ieee transactions information theory dasgupta sanjoy learning mixtures gaussians proc ieee conf foundations computer science
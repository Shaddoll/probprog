abstract present monte carlo algorithm learning act partially observable markov decision processes pomdps real valued state action spaces approach uses importance sampling representing beliefs monte carlo approximation belief propagation reinforcement learning algorithm value iteration employed learn value functions belief states finally samplebased version nearest neighbor used generalize across states initial empirical results suggest approach works well practical applications introduction pomdps address problem acting optimally partially observable dynamic environment pomdps learner interacts stochastic environment whose state partially observable actions change state environment lead numerical penalties rewards may observed unknown temporal delay learner goal devise policy action selection maximizes reward obviously pomdp framework embraces large range practical problems past work predominately studied pomdps discrete worlds discrete worlds advantage distributions states called belief states represented exactly using one parameter per state optimal value function finite planning horizon shown convex piecewise linear makes possible derive exact solutions discrete pomdps interested pomdps continuous state action spaces paying tribute fact large number real world problems continuous nature general pomdps solvable exactly little known special cases solved paper proposes approximate approach mc pomdp algorithm accommodate real valued spaces models central idea use monte carlo sampling belief representation propagation reinforcement learning belief space employed learn value functions using sample based version nearest neighbor generalization empirical results illustrate approach finds close optimal solutions efficiently monte carlo pomdps preliminaries pomdps address problem selection actions stationary partially observable controllable markov chains establish basic vocabulary let us define state point time world specific state denoted monte carlo pomdps action agent execute actions denoted observation sensors agent observe noisy projection world state use denote observations reward additionally agent receives rewards penalties denoted simplify notation assume reward part observation specifically use denote function extracts reward observation throughout paper use subscript refer specific point time st refers state time pomdps characterized three probability distributions initial distribution pr zo specifies initial distribution states time next state distribution pt oct vc vct describes likelihood action executed state leads state perceptual distribution pt ct describes likelihood observing world state history sequence states observations simplicity assume actions observations alternated use dt denote history leading time dt ot ot ao oo fundamental problem pomdps devise policy action selection maximizes reward policy denoted cr mapping histories actions assuming actions chosen policy policy induces expected cumulative possibly discounted discount factor reward defined denotes mathematical expectation pomdp problem thus find policy maximizes argmax belief states avoid difficulty learning function unbounded input history arbitrarily long common practice map histories belief states learn mapping belief states actions instead formally belief state denoted probability distribution states conditioned past actions observations ot pr xt dt pr xt ot oo belief computed incrementally using knowledge pomdp defining distributions initially obtain ot pr xt ot oo pr ot pr ot pr ot xt zt pr zt oo zt pr zt oo dzt ct pr oet ct ot dzt thrun lllllllllnillllllllllllllllllllll iiiilll ii ii figure sampling likelihood weighted sampling importance sampling bottom graph samples shown approximate function shown top height samples illustrates importance factors denotes constant normalizer derivations follow directly fact environment stationary markov chain future states observations conditionally independent past ones given knowledge state equation obtained using theorem total probability armed notion belief states policy mapping belief states instead histories actions legitimacy conditioning instead follows directly fact environment markov implies one needs know past make optimal decisions sample representations thus far intentionally left open belief states represented prior work state spaces discrete discrete worlds beliefs represented collection probabilities one state hence beliefs represented exactly interested real valued state spaces general probability distributions realvalued spaces possess infinitely many dimensions hence cannot represented digital computer key idea represent belief states sets weighted samples drawn belief distribution figure illustrates two popular schemes sample based approximation likelihood weighted sampling samples shown bottom figure la drawn directly target distribution labeled figure la importance sampling samples drawn distribution curve labeled figure lb latter case samples annotated numerical importance factor account difference sampling distribution target distribution height bars figure illustrates importance factors importance sampling requires case throughout paper obviously sampling methods generate approximations mild assumptions converge denoting sample set size target distribution rate context pomdps use sample based representations gives rise following algorithm approximate belief propagation equation algorithm particle filter ot times draw random state zt ot monte carlo pomdps sample xt according xt xt set importance factor ot add zt zt toot normalize zt zt return ot algorithm converges arbitrary models arbitrary belief distributions defined discrete continuous mixed continuous discrete state action spaces minor modifications proposed names like particle filters condensation algorithm survival fittest context robotics monte carlo localization projection conventional planning result applying action state zt distribution pr zt rt zt states zt rewards rt next time step operation called projection pomdps state zt unknown instead one compute result applying action belief state result distribution pt rt belief states rewards rt since belief states distributions result projection pomdps technically distribution distributions projection algorithm derived follows using total probability obtain pr pr ot rt dr pr ot ot dt pt dr dot term already derived previous section equation observation reward trivially computed observation second term obtained integrating unknown variables zt zt exploiting markov property pr ot dt pr ot xt pr xt dt dzt pt zt pr zt zt pr zt idt dzt dzt leads following approximate algorithm projecting belief state spirit paper approach uses monte carlo integration instead exact integration represents distributions distributions distributions samples drawn distributions algorithm partide projecfion ot times draw random state zt ot sample next state zt according zt sample observation according ot compute ot particle filter ot add ot ot toot return ot result algorithm sample set belief states rewards drawn desired distribution pv ot rt ot converges probability true posterior thrun learning value functions following rich literature reinforcement learning approach solves pomdp problem value iteration belief space specifically approach recursively learns value function belief states action backing values subsequent belief states ot ot maaxq leaving open moment represented easy seen algorithm particle projection applied compute monte carlo approximation right hand side expression given belief state ot action particle projection computes sample ot expected value right hand side approximated shown sides equal greedy policy argmaxq optimal crq furthermore shown discrete case repetitive application leads optimal value function thus optimal policy approach essentially performs model based reinforcement learning belief space using approximate sample based representations makes possible apply rich bag tricks found literature mdps experiments use online reinforcement learning counter based exploration experience replay determine order belief states updated nearest neighbor return issue represent since operating real valued spaces sort function approximation method called however recall accepts probability distribution sample set input makes existing function approximators neural networks inapplicable current implementation nearest neighbor applied represent specifically algorithm maintains set sample sets belief states annotated action value new belief state encountered value obtained finding nearest neighbors database linearly averaging values sufficiently many neighbors within pre specified maximum distance added database hence database grows time approach uses kl divergence relative entropy distance function technically kl divergence two continuous distributions well defined applied sample sets however cannot computed hence evaluating distance two different sample sets approach maps continuous valued densities using gaussian kernels uses monte carlo sampling approximate kl divergence algorithm fairly generic extension nearest neighbors function approximation density space densities represented samples space limitations preclude us providing detail see experimental results preliminary results obtained world shown two domains one synthetic one using simulator rwi robot synthetic environment figure agents starts lower left corner objective reach heaven either upper left corner lower right stdctly speaking kl divergence distance metric ignored monte carlo pomdps figure environment schematically average performance reward function training episodes black graph corresponds smaller environment steps min grey graph larger environment steps min results plotted function number backups thousands comer opposite location hell agent know location heaven ask priest located upper right comer thus optimal solution requires agent go first priest head heaven state space contains real valued coordinates agent discrete location heaven component unobservable addition knowing location heaven agent also cannot sense real valued coordinates random motion noise injected move agent hits boundary penalized also told boundary hit makes possible infer coordinates along one axis however notice initial coordinates agent known optimal solution takes approximately steps thus successful pomdp planner must capable looking steps ahead use term successful policy refer policy always leads heaven even path suboptimal policy successful agent must learned first move priest information gathering proceed right target location figures show performance results averaged experiments solid black curve diagrams plots average cumulative reward function number training episodes figure function number backups figure successful policy consistently found episodes backups experiments current implementation backups require approximately minutes pentium pc experiments successful policy identified episodes less backups minutes successful policy found learning gradually optimizes path investigate scaling doubled size environment quadrupling size state space making optimal solution steps long results depicted gray curves figures successful policy consistently found episodes backups minutes runs successful policy identified episodes also applied mc pomdps robotic locate retrieve task robot figure find grasp object somewhere vicinity floor table height robot task grasp object using gripper rewarded successfully grasping object penalized unsuccessful grasps moving far away object state space continuous coordinates discrete object height robot uses mono camera system object detection hence viewing object single location insufficient localization moreover initially object might sight robot camera robot must look around first simulation assume general detection error false positive false negative additional gaussian noise object detected correctly robot actions include tums variable angle translations variable distance grasps one two legal heights robot control erroneous variance space rotational space typical belief states range uniformly distributed sample sets initial belief samples narrowly focused specific location thrun figure find fetch task success iteration mobile robot gripper camera holding target object experiments carded simulation three successful runs trajectory projected success rate function number planning steps figure shows rate successful grasps function iterations actions initially robot fails grasp object approximately iterations performance surpasses planning time order hours however robot fails reach part certain initial configurations make impossible succeed object close maximum allowed distance part robot occasionally misses object centimeters figure depicts three successful example trajectories three robot initially searches object moves towards grasps successfully discussion presented monte carlo approach learning act partially observable markov decision processes pomdps approach represents belief distributions using samples drawn distributions reinforcement learning belief space applied learn optimal policies using sample based version nearest neighbor generalization backups performed using monte carlo sampling initial experimental results demonstrate approach applicable real valued domains yields good performance results environments pomdp standards relatively large
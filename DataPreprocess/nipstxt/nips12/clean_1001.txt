abstract consider problem reliably choosing near best strategy restricted class strategies ii partially observable markov decision process pomdp assume given ability simulate pomdp study might called sample complexity amount data one must generate pomdp order choose good strategy prove upper bounds sample complexity showing even infinitely large arbitrarily complex pomdps amount data needed finite depends linearly complexity restricted strategy class ii exponentially horizon time latter dependence eased variety ways including application gradient local search algorithms measure complexity generalizes classical supervised learning notion vc dimension settings reinforcement learning planning introduction much recent attention focused partially observable markov decision processes pomdps exponentially even infinitely large state spaces domains number interesting basic issues arise state space becomes large classical way specifying pomdp tables transition probabilities clearly becomes infeasible intelligently discuss problem planning computing good strategy given pomdp compact implicit representations pomdps strategies pomdps must developed examples include factored next state distributions strategies derived function approximation schemes trend towards compact representations well algorithms planning learning using reminiscent supervised learning researchers long emphasized parametric models decision trees neural networks capture limited structure enjoy number computational information theoretic benefits motivated issues consider setting given generative model throughout use word strategy mean mapping observable histories actions generalizes notion policy fully observable mdp kearns mansour ng simulator pomdp wish find good strategy restricted class strategies ii generative model black box allows us generate experience trajectories different states choosing generative models abstract notion compact pomdp representations sense compact representations typically considered factored next state distributions already provide efficient generative models imagining strategy class ii given compact representation natural limitation strategies bounded memory thus view adopting even though world pomdp may extremely complex assume least simulate sample experience world via generative model try use experience choose strategy simple class ii study following question many calls generative model needed enough data choose near best strategy given class analogous question sample complexity supervised learning harder added difficulty lies reuse data supervised learning every sample provides feedback every hypothesis function namely close restricted lie hypothesis class reuse permits sample complexity bounds far smaller size instance log samples needed choose near best model finite class infinite sample sizes obtained depend measure complexity vc dimension dependence complexity target function size input domain pomdp setting would like analogous sample complexity bounds terms complexity strategy class ii bounds dependence size complexity pomdp unlike supervised learning setting experience reuse immediate pomdps see consider straw man algorithm starting ii uses generative model generate many trajectories thus forms monte carlo estimate clear trajectories much use evaluating different ii since may quickly disagree actions take naive monte carlo method thus gives ii bounds sample complexity rather log irtl finite case paper shall describe trajectory tree method generating reusable trajectories requires generating relatively small number trajectories number independent state space size pomdp depends linearly general measure complexity strategy class ii depends exponentially horizon time latter dependence eased via gradient algorithms williams reinforce baird moore recent vaps local search techniques measure strategy class complexity generalizes notion vc dimension supervised learning settings reinforcement learning planning give bounds recover settings powerful analogous results supervised learning bounds arbitrary infinite strategy classes depend dimension class rather size state space preliminaries begin standard definitions markov decision process mdp tuple ls possibly infinite state set start state al ak actions gives next state distribution upon taking action state reward function gives corresponding rewards assume simplicity rewards deterministic bounded approximate planning large pomdps via reusable trajectories absolute value max partially observable markov decision process pomdp consists underlying mdp observation distributions ols state random observation made adopted common assumption fixed start state limit class strategies entertain may single best strategy class different start states may different best strategies ii also assume given pomdp form generafive model given input state action pair output state drawn according observation drawn according reward gives us ability sample pomdp random access way definition may initially seem unreasonably generous generative model giving us fully observable simulation partially observable process however key point must still find strategy performs well partially observable setting concrete example designing elevator control system may access simulator generates random rider arrival times keeps track waiting time rider number riders waiting every floor every time day however helpful information might designing controller controller must use information floors currently call button pushed observables case readers uncomfortable power provided generative models referred section briefly describe results requiring extremely weak form partially observable simulation time agent seen sequence observations ot chosen actions received rewards time steps prior current one write observable history oo ao observable histories also called trajectories inputs strategies formally strategy stochastic mapping observable histories actions example includes approaches use observable history track belief state strategy class ii set strategies restrict attention case discounted return let discount factor define horizon time log rnax note returns beyond first steps contribute total discounted return also let vmax max bound value function finally pomdp strategy class ii define opt ii support best expected return achievable using ii problem thus following given generative model pomdp strategy class ii many calls generative model must make order enough data choose ii whose performance approaches opt ii also calls make generative model achieve trajectory tree method describe use generative model create reusable trajectories ease exposition assume two actions results generalize easily finite number actions see full paper equivalent definition assume fixed distribution start states since dummy state whose next state distribution action results paper extended without difficulty undiscounted finite horizon setting kearns mansour ng trajectory tree binary tree node labeled state observation pair child two actions additionally link child labeled reward tree depth hr nodes section discuss settings exponential dependence eased trajectory tree built follows root labeled observation two children created calling generative model al gives us two next states reached say respectively two observations made say two rewards received al st label root al child child links children labeled recursively generate two children rewards way node depth deterministic strategy trajectory tree defines path starts root inductively internal node feed observable history along path root node selects moves child current node continues leaf node reached define discounted sum returns along path taken case stochastic defines distribution paths expected return according distribution later also describe another method treating stochastic strategies hence given rn trajectory trees natural estimate ti note tree used evaluate strategy much way single labeled example used evaluate hypothesis supervised learning thus sense trajectory trees reusable goal establish uniform convergence results bound error estimates function sample size number trees section first treats easier case deterministic classes ii section extends result stochastic classes case deterministic ii let us begin stating result special case finite classes deterministic strategies serve demonstrate kind bound seek theorem let ii finite class deterministic strategies arbitrary twoaction pomdp let rn trajectory trees created using generative model resulting estimates vmax log iii log probability ivy holds simultaneously due space limitations detailed proofs results section left full paper try convey intuition behind ideas observe fixed deterministic estimates ti generated rrt different trajectory trees ti independent moreover ti unbiased estimate expected discounted step return turn close observations combined simple chernoff union bound argument sufficient establish theorem rather developing argument instead move straight harder case infinite ii addressing sample complexity supervised learning perhaps important insight even though class may infinite number possible behaviors finite set points often exhaustive precisely boolean functions say set cl ca shattered every possible labelings approximate planning large pomdps via reusable trajectories points realized vc dimension defined size largest shattered set known vc dimension number possible labelings induced set points era much less fact provides key leverage exploited classical vc dimension results concentrate replicating leverage setting possibly infinite set deterministic strategies strategy ii simply deterministic function mapping set observable histories set thus boolean function observable histories therefore write vc ii denote familiar vc dimension set binary functions ii example ii set thresholded linear functions current vector observations particular type memoryless strategy vc simply equals number parameters show intuitively class ii bounded vc dimension cannot induce exhaustive behavior set trajectory trees note ii reward labelings differ rl ti ti give different returns ti must choose different actions node ti words every different reward labeling set trees yields different binary labeling set observable histories trees number different tree reward labelings era developing argument carefully applying classical uniform convergence techniques obtain following theorem full proof theorem let ii class deterministic strategies arbitrary two action pomdp let vc denote vc dimension let trajectory trees created using generative model resulting estimates vmax vc ii log vmax log probability ivy holds simultaneously ii case stochastic ii address case stochastic strategy classes describe approach transform stochastic strategies equivalent deterministic ones operate deterministic versions reducing problem one handled previous section transformation follows given class stochastic strategies ii domain set observable histories first extend domain stochastic strategy ii define corresponding deterministic transformed strategy domain given al pr al otherwise let ii collection transformed deterministic strategies since ii set deterministic boolean functions vc dimension well defined define pseudo dimension original set stochastic strategies ii pvc ii vc transformed strategy class also need transform pomdp augmenting state space informally transitions rewards remain except state transition draw new random variable uniformly independently previous events states form let observed variable whenever original pomdp stochastic strategy would equivalent conventional definition pseudo dimension rl viewed set maps real valued action probabilities kearns mansour ng given history transformed pomdp corresponding deterministic transformed strategy given random variable current state definition easy see exactly chance choosing action node randomization back deterministic case theorem applies vc ii replaced pvc ii vc lit desired uniform convergence result algorithms approximate planning given generative model pomdp preceding section results immediately suggest class approximate planning algorithms generate rn trajectory trees ti tin search ii maximizes ti following corollary uniform convergence results establishes soundness approach corollary let ii class strategies pomdp let number rn trajectory trees given theorem let argmax en policy ii highest empirical return rn trees probability near optimal within ii vn ovt ii suggested maximization computationally infeasible one search local maximum instead uniform convergence assures us trusted estimate true performance course even finding local maximum expensive since trajectory tree size exponential however practice may possible significantly reduce cost search suppose using class possibly transformed deterministic strategies perform greedy local search ii optimize time search evaluate policy currently considering really need look single path length tree corresponding path taken strategy considered thus build trajectory trees lazily incrementally build node tree needed evaluate tr ti current strategy parts tree reached poor policies good search algorithm may never even build parts tree case fixed number trees step local search takes time linear different approach works directly stochastic strategies without requiring transformation deterministic strategies case stochastic strategy defines distribution paths trajectory tree thus calculating tr may general require examining complete trees however view trajectory tree small deterministic pomdp children node tree successor nodes ii tr smoothly parameterized family stochastic strategies algorithms william reinforce used find unbiased estimate gradient tm turn used see also ng jordan preparation assuming much stronger model pomdp deterministic function distributed according distributed uniform gives algorithm enjoys uniform convergence bounds similar presented polynomial rather exponential dependence algorithm samples number vectors defines step monte carlo evaluation trial policy bound number random vectors needed rather total number calls approximate planning large pomdps via reusable trajectories perform stochastic gradient ascent maximize tm moreover fixed number trees algorithms need time per gradient estimate combined lazy tree construction practical algorithm whose per step complexity linear horizon time line thought developed long version papertl random trajectory method using fully observable generative model pomdp shown trajectory tree method gives uniformly good value estimates amount experience linear vc ii exponential turns significantly weaken generative model yet still obtain essentially theoretical results harder case assume generative model provides partially observable histories generated truly random strategy takes action equal probability every step regardless history far furthermore trajectories always begin designated start state ability provided reset pomdp state indeed underlying states may never observed method harder case called random trajectory method seems lead less readily practical algorithms trajectory tree method formal description analysis difficult trajectory trees given long version paper theorem prove amount data needed linear vc ii exponential horizon time averaging appropriately resulting ensemble trajectories generated amount data sufficient yield uniformly good estimates values strategies ii
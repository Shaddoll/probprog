abstract consid problem reliabl choos near best strategi restrict class strategi ii partial observ markov decis process pomdp assum given abil simul pomdp studi might call sampl complex amount data one must gener pomdp order choos good strategi prove upper bound sampl complex show even infinit larg arbitrarili complex pomdp amount data need finit depend linearli complex restrict strategi class ii exponenti horizon time latter depend eas varieti way includ applic gradient local search algorithm measur complex gener classic supervis learn notion vc dimens set reinforc learn plan introduct much recent attent focus partial observ markov decis process pomdp exponenti even infinit larg state space domain number interest basic issu aris state space becom larg classic way specifi pomdp tabl transit probabl clearli becom infeas intellig discuss problem plan comput good strategi given pomdp compact implicit represent pomdp strategi pomdp must develop exampl includ factor next state distribut strategi deriv function approxim scheme trend toward compact represent well algorithm plan learn use reminisc supervis learn research long emphas parametr model decis tree neural network captur limit structur enjoy number comput inform theoret benefit motiv issu consid set given gener model throughout use word strategi mean map observ histori action gener notion polici fulli observ mdp kearn mansour ng simul pomdp wish find good strategi restrict class strategi ii gener model black box allow us gener experi trajectori differ state choos gener model abstract notion compact pomdp represent sens compact represent typic consid factor next state distribut alreadi provid effici gener model imagin strategi class ii given compact represent natur limit strategi bound memori thu view adopt even though world pomdp may extrem complex assum least simul sampl experi world via gener model tri use experi choos strategi simpl class ii studi follow question mani call gener model need enough data choos near best strategi given class analog question sampl complex supervis learn harder ad difficulti lie reus data supervis learn everi sampl provid feedback everi hypothesi function name close restrict lie hypothesi class reus permit sampl complex bound far smaller size instanc log sampl need choos near best model finit class infinit sampl size obtain depend measur complex vc dimens depend complex target function size input domain pomdp set would like analog sampl complex bound term complex strategi class ii bound depend size complex pomdp unlik supervis learn set experi reus immedi pomdp see consid straw man algorithm start ii use gener model gener mani trajectori thu form mont carlo estim clear trajectori much use evalu differ ii sinc may quickli disagre action take naiv mont carlo method thu give ii bound sampl complex rather log irtl finit case paper shall describ trajectori tree method gener reusabl trajectori requir gener rel small number trajectori number independ state space size pomdp depend linearli gener measur complex strategi class ii depend exponenti horizon time latter depend eas via gradient algorithm william reinforc baird moor recent vap local search techniqu measur strategi class complex gener notion vc dimens supervis learn set reinforc learn plan give bound recov set power analog result supervis learn bound arbitrari infinit strategi class depend dimens class rather size state space preliminari begin standard definit markov decis process mdp tupl ls possibl infinit state set start state al ak action give next state distribut upon take action state reward function give correspond reward assum simplic reward determinist bound approxim plan larg pomdp via reusabl trajectori absolut valu max partial observ markov decis process pomdp consist underli mdp observ distribut ol state random observ made adopt common assumpt fix start state limit class strategi entertain may singl best strategi class differ start state may differ best strategi ii also assum given pomdp form generaf model given input state action pair output state drawn accord observ drawn accord reward give us abil sampl pomdp random access way definit may initi seem unreason gener gener model give us fulli observ simul partial observ process howev key point must still find strategi perform well partial observ set concret exampl design elev control system may access simul gener random rider arriv time keep track wait time rider number rider wait everi floor everi time day howev help inform might design control control must use inform floor current call button push observ case reader uncomfort power provid gener model refer section briefli describ result requir extrem weak form partial observ simul time agent seen sequenc observ ot chosen action receiv reward time step prior current one write observ histori oo ao observ histori also call trajectori input strategi formal strategi stochast map observ histori action exampl includ approach use observ histori track belief state strategi class ii set strategi restrict attent case discount return let discount factor defin horizon time log rnax note return beyond first step contribut total discount return also let vmax max bound valu function final pomdp strategi class ii defin opt ii support best expect return achiev use ii problem thu follow given gener model pomdp strategi class ii mani call gener model must make order enough data choos ii whose perform approach opt ii also call make gener model achiev trajectori tree method describ use gener model creat reusabl trajectori eas exposit assum two action result gener easili finit number action see full paper equival definit assum fix distribut start state sinc dummi state whose next state distribut action result paper extend without difficulti undiscount finit horizon set kearn mansour ng trajectori tree binari tree node label state observ pair child two action addit link child label reward tree depth hr node section discuss set exponenti depend eas trajectori tree built follow root label observ two children creat call gener model al give us two next state reach say respect two observ made say two reward receiv al st label root al child child link children label recurs gener two children reward way node depth determinist strategi trajectori tree defin path start root induct intern node feed observ histori along path root node select move child current node continu leaf node reach defin discount sum return along path taken case stochast defin distribut path expect return accord distribut later also describ anoth method treat stochast strategi henc given rn trajectori tree natur estim ti note tree use evalu strategi much way singl label exampl use evalu hypothesi supervis learn thu sens trajectori tree reusabl goal establish uniform converg result bound error estim function sampl size number tree section first treat easier case determinist class ii section extend result stochast class case determinist ii let us begin state result special case finit class determinist strategi serv demonstr kind bound seek theorem let ii finit class determinist strategi arbitrari twoaction pomdp let rn trajectori tree creat use gener model result estim vmax log iii log probabl ivi hold simultan due space limit detail proof result section left full paper tri convey intuit behind idea observ fix determinist estim ti gener rrt differ trajectori tree ti independ moreov ti unbias estim expect discount step return turn close observ combin simpl chernoff union bound argument suffici establish theorem rather develop argument instead move straight harder case infinit ii address sampl complex supervis learn perhap import insight even though class may infinit number possibl behavior finit set point often exhaust precis boolean function say set cl ca shatter everi possibl label approxim plan larg pomdp via reusabl trajectori point realiz vc dimens defin size largest shatter set known vc dimens number possibl label induc set point era much less fact provid key leverag exploit classic vc dimens result concentr replic leverag set possibl infinit set determinist strategi strategi ii simpli determinist function map set observ histori set thu boolean function observ histori therefor write vc ii denot familiar vc dimens set binari function ii exampl ii set threshold linear function current vector observ particular type memoryless strategi vc simpli equal number paramet show intuit class ii bound vc dimens cannot induc exhaust behavior set trajectori tree note ii reward label differ rl ti ti give differ return ti must choos differ action node ti word everi differ reward label set tree yield differ binari label set observ histori tree number differ tree reward label era develop argument care appli classic uniform converg techniqu obtain follow theorem full proof theorem let ii class determinist strategi arbitrari two action pomdp let vc denot vc dimens let trajectori tree creat use gener model result estim vmax vc ii log vmax log probabl ivi hold simultan ii case stochast ii address case stochast strategi class describ approach transform stochast strategi equival determinist one oper determinist version reduc problem one handl previou section transform follow given class stochast strategi ii domain set observ histori first extend domain stochast strategi ii defin correspond determinist transform strategi domain given al pr al otherwis let ii collect transform determinist strategi sinc ii set determinist boolean function vc dimens well defin defin pseudo dimens origin set stochast strategi ii pvc ii vc transform strategi class also need transform pomdp augment state space inform transit reward remain except state transit draw new random variabl uniformli independ previou event state form let observ variabl whenev origin pomdp stochast strategi would equival convent definit pseudo dimens rl view set map real valu action probabl kearn mansour ng given histori transform pomdp correspond determinist transform strategi given random variabl current state definit easi see exactli chanc choos action node random back determinist case theorem appli vc ii replac pvc ii vc lit desir uniform converg result algorithm approxim plan given gener model pomdp preced section result immedi suggest class approxim plan algorithm gener rn trajectori tree ti tin search ii maxim ti follow corollari uniform converg result establish sound approach corollari let ii class strategi pomdp let number rn trajectori tree given theorem let argmax en polici ii highest empir return rn tree probabl near optim within ii vn ovt ii suggest maxim comput infeas one search local maximum instead uniform converg assur us trust estim true perform cours even find local maximum expens sinc trajectori tree size exponenti howev practic may possibl significantli reduc cost search suppos use class possibl transform determinist strategi perform greedi local search ii optim time search evalu polici current consid realli need look singl path length tree correspond path taken strategi consid thu build trajectori tree lazili increment build node tree need evalu tr ti current strategi part tree reach poor polici good search algorithm may never even build part tree case fix number tree step local search take time linear differ approach work directli stochast strategi without requir transform determinist strategi case stochast strategi defin distribut path trajectori tree thu calcul tr may gener requir examin complet tree howev view trajectori tree small determinist pomdp children node tree successor node ii tr smoothli parameter famili stochast strategi algorithm william reinforc use find unbias estim gradient tm turn use see also ng jordan prepar assum much stronger model pomdp determinist function distribut accord distribut uniform give algorithm enjoy uniform converg bound similar present polynomi rather exponenti depend algorithm sampl number vector defin step mont carlo evalu trial polici bound number random vector need rather total number call approxim plan larg pomdp via reusabl trajectori perform stochast gradient ascent maxim tm moreov fix number tree algorithm need time per gradient estim combin lazi tree construct practic algorithm whose per step complex linear horizon time line thought develop long version papertl random trajectori method use fulli observ gener model pomdp shown trajectori tree method give uniformli good valu estim amount experi linear vc ii exponenti turn significantli weaken gener model yet still obtain essenti theoret result harder case assum gener model provid partial observ histori gener truli random strategi take action equal probabl everi step regardless histori far furthermor trajectori alway begin design start state abil provid reset pomdp state inde underli state may never observ method harder case call random trajectori method seem lead less readili practic algorithm trajectori tree method formal descript analysi difficult trajectori tree given long version paper theorem prove amount data need linear vc ii exponenti horizon time averag appropri result ensembl trajectori gener amount data suffici yield uniformli good estim valu strategi ii
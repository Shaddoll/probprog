abstract show recently proposed variant support vector machine svm algorithm known svm interpreted maximal separation subsets convex hulls data call soft convex hulls soft convex hulls controlled choice parameter intersection convex hulls empty hyperplane positioned halfway distance convex hulls measured along normal maximized hyperplane normal similarly determined soft convex hulls position perpendicular distance origin adjusted minimize error sum proposed geometric interpretation svm also leads necessary sufficient conditions existence choice svm solution nontrivial introduction recently sch kopf et al introduced new class svm algorithms called svm regression estimation pattern recognition basic idea remove user chosen error penalty factor appears svm algorithms introducing new variable pattern recognition case adds another degree freedom margin given normal separating hyperplane size margin increases linearly turns adding primal objective function coefficient variable absorbed behaviour resulting svm number margin errors number support vectors extent controlled setting moreover decision function produced svm also produced original svm algorithm suitable choice paper show svm pattern recognition case clear geometric interpretation also leads necessary sufficient conditions existence nontrivial solution svm problem considerations apply feature space mapping data induced kernel adopt usual notation normal separating hyperplane mapped geometric interpretation svm classifiers data denoted xi corresponding labels yi scalars ei positive scalar slack variables svm classifiers svm formulation given follows minimize respect subject el el user chosen parameter decision function whose sign determines label given test point wolfe dual problem maximize ij aiajyiy xi subject oti otiyi eoti given aiyixi sch kopf et al show upper bound fraction margin errors lower bound fraction support vectors quantities approach asymptotically note point feasible point thus solution interest must furthermore vp optimal solution thus assume vp therefore always given constraint fact redundant negative value cannot appear solution problem constraint removed since feasible solution gives lower value thus replace constraints el el reparameterization svm reparameterize primal problem dividing objective function constraints making following substitutions el xa margin error xi defined point see sin fact prove even optimal solution unique global solutions still see burges crisp uniqueness svm solution volume crisp burges gives equivalent formulation minimize ilwll respect subject use decision function formulation exactly equivalent although primal dual appear different dual problem minimize fd aiajyiyjxi xj respect ai subject iyi ai ai given iyixi following refer reparameterized version svm given svm although emphasize describes problem geometric interpretation svm separable case clear optimal separating hyperplane hyperplane bisects shortest vector joining convex hulls positive negative polarity points show geometric interpretation extended case svm separable nonseparable cases separable case start giving analysis separable case convex hulls two classes aixi yl yi ai ai yi ai ai yl finding two closest points written following optimization problem tixi li yl yl see example bennett http www rpi edu dennek svmtalk ps also appear geometric interpretation ofu svm classifiers subject ai ai yi yl taking decision boundary perpendicular bisector line segment joining two closest points means solution oqxi aixi yi yi thus lies along line segment half size midpoint line segment rescaling objective function using class labels yi rewrite subject mjn zaiajyiyjxi xj ij aiyi ai ai associated decision function otiyixi ei otiggi otiyiotj ggi ggj connection svm consider two sets points defined aixi ai yi yi ai aixi yi ai yl following simple proposition proposition convex sets furthermore positions points respect xi depend choice origin proof clearly since ai defined subset ai defined similarly consider two points defined ax points line joining two points written otli ot ggi since satisfy ai axi aa since also axi aa set convex one rescale objective function without changing constraints follows uniqueness solution see also burges crisp uniqueness svm solution volume crisp burges argument similar finally suppose every xi translated xo xi xi xo vi since ai every point also translated amount similarly problem finding optimal separating hyperplane convex sets becomes subject main hw oqajyiyjxi xj ij since eqs identical see svm algorithm fact finding optimal separating hyperplane convex sets note convex sets simply uniformly scaled versions example shown figure xl xl xl xl figure soft convex hull vertices right isosceles triangle various note shape changes set grows constrained boundaries encapsulating convex hull set empty refer formulation given section soft convex hull formulation sets points defined eqs soft convex hulls comparing offsets margin widths natural value offset soft convex hull approach arose asking separating hyperplane lie halfway closest extremities two soft convex hulls different choices amount hyperplanes normal different perpendicular distances origin value general cost term eq minimized compare two values follows kkt conditions svm formulation multiplying yi summing using gives geometn interpretation ofu svm classifiers thus separating hyperplane found svm algorithm sits perpendicular distance yi il away found soft convex hull formulation given choice results lowest value cost ithe soft convex hull approach suggests taking since value ill takes points aixi aixi use kkt conditions compare summing using gives since shows primal soft convex hull formulation substituting svm primal formulation obtain primal formulation soft convex hull problem minimize respect subject yi xi yiyj straightforward check dual exactly moreover summing relevant kkt conditions see note formulation variables retain meaning according choosing section establish results choices using svm formulation first note aiyi implies gives vi thus choosing corresponds choosing results solution dual hence normal choosing note different values still result different values primal variables equalities also show feasible region dual empty hence problem insoluble corresponds requirement however improve upon let number positive negative polarity points let lmin min minimal value still results nonempty feasible region izmin lmin gives condition min define nontrivial solution problem solution following proposition gives conditions existence nontrivial solutions crisp burges proposition value exists result nontrivial solution svm classification problem min proof suppose rn allowable values hence two convex hulls intersect since two convex hulls intersect solution trivial since definition exist feasible points ui otixi oti hence eiotiyixi ei yi tixi ei yi otixi cf suppose clearly nontrivial solution exists since shortest distance two convex sets zero hence corresponding note condition amounts requirement centroid positive examples coincide negative examples note also shows given data set one find lower bound finding largest satisfies cl discussion soft convex hull interpretation suggests appropriate way penalize positive polarity errors differently negative replace sum fact one go introduce every train point svm formulation makes possibility explicit original svm formulation note also fact svm leads values differ would place optimal hyperplane halfway soft convex hulls suggests may principled methods choosing best given problem dictated minimizing sum indeed originally sum term arose attempt approximate number errors train set reasoning sense separates justification example given simple line search could used find value actually minimize number errors train set methods example minimizing estimated bayes error may also prove useful acknowledgments burges wishes thank keasler lawrence nohl lucent technologies support
abstract gaussian processes powerful regression models specified parametrized mean covariance functions standard approaches estimate parameters known name hyperparameters maximum likelihood ml maximum aposterior map approaches paper propose investigate predictive approaches namely maximization geisser surrogate predictive probability gpp minimization mean square error respect gpp referred geisser predictive mean square error gpe estimate hyperparameters also derive results standard cross validation cv error make comparison approaches tested number problems experimental results show approaches strongly competitive existing approaches introduction gaussian processes gps powerful regression models gained popularity recently though appeared different forms literature years used classification also see mackay rasmussen williams rasmussen restrict regression problems neal showed large class neural network models converge gaussian process prior functions limit infinite number hidden units although gps created using infinite networks often gps specified directly using parametric forms mean covariance functions williams rasmussen assume process zero mean let wherex andy represents output corresponding input vector gaussian prior functions given ynlxn exp ly covariance matrix th element denotes parametrized covariance function assuming undararajan keerthi observed output modeled en en zero mean multivariate gaussian covariance matrix independent get tnixn xp ttnc ltn lcnl therefore zero otherwise note new set hyperparameters predictive distribution output test case also gaussian mean variance kn lc tn ay bn lkn vector th bn element given need specify covariance function williams rasmussen found following covariance function work well practice ao al xp xp voexp wp xp xp xp pth component tn input vector wp automatic relevance determination ard parameters note si also parameters positive convenient use logarithmic scale hence given log ao al wl wm question handle sophisticated techniques like hybrid monte carlo hmc methods rasmussen neal available numerically integrate hyperparameters make predictions alternately estimate training data restrict latter approach classical approach assumed deterministic unknown estimate found maximizing likelihood argmax oml vix bayesian approach assumed random prior specified map estimate argmax omp obtained omp tlx motivation predictive distribution lx approximated lx zn background paper propose investigate different predictive approaches estimate hyperparameters training data predictive approaches choosing hyperparameters geisser proposed predictive sample reuse psr methodology applied model selection parameter estimation problems basic idea define partition scheme tz ita partition belonging set partitions representing retained omitted data sets respectively unknown estimated model mj chosen among set models indexed means optimizing predictive measure measures predictive performance omitted observations using retained observations averaged partitions special case leave one strategy note approach independently presented predictive approaches choosing hyperparameters gaussian processes name cross validation cv stone well known examples standard cv error negative average predictive likelihood geisser eddy proposed maximize rii lp mj known geisser surrogate predictive probability gpp synthesizing bayesian psr methodology context parametrized model selection propose maximize ri lx estimate obtained zn removing th sample note nothing predictive distribution evaluated also introduce notion geisser predictive mean square error gpe defined expectation operation defined respect lx propose estimate minimizing gpe expressions gpp gradient objective function corresponding gpp given log lx get cr lt fly matrix obtained removing th column row similarly obtained ci column respectively removing element gradien computed cien ly using following result log fly log lr eli nj theorem objective function gaussian process model given logeii log ii denotes ita diagonal entry denotes th element ltn gradient given og sj ooj eli cii ocn tn sj rj denotes column matrix ii ltn thus using compute gpp gradient give meaningful interpretation different terms shortly expressions cv function gradient define cv function undararajan keerthi mean conditional predictive distribution given using following result compute efficiently theorem cv function gaussian model given qn gradient given oh ooj rj sj rj qlv ii defined theorem expressions gpe gradient gpe function defined lx dy readily simplified fly comparing see cv error minimizes deviation predictive mean gpe takes predictive variance also account gradient written oh lh rg toc aoj ooj ii ei used results il oc oc ii eit oc ei denotes th column vector identity matrix interpretations insight obtained reparametrizing covariance function follows xp xp oexp ao ao al al vo vo let us define ci therefore ci denote th element matrices respectively theorem see rewrite log ii log predictive approaches choosing hyperparameters gaussian processes lt ii denote respectively th column diagonal entry matrix setting derivative respect zero infer noise level pii similarly cv error rewritten pii note dependent ratio hyperparameters al apart ard parameters therefore cannot infer noise level uniquely however estimate ard parameters ratios al estimated parameters use estimate noise level next note noise level preferred gpe criterion zero see first let us rewrite reparametrization pii ii since ii independent follows gpe prefers zero noise level true therefore approach applied either noise level known good estimate available simulation results carried simulation four data sets considered mackay robot arm problem modified version introduced neal used data set mackay inputs outputs examples training set test set data set referred data set table next evaluate ability predictive approaches estimating ard parameters carried simulation robot arm data inputs neal version denoted data set table data set generated adding four inputs two copies two inputs corrupted additive zero mean gaussian noise standard deviation two irrelevant gaussian noise inputs zero mean unit variance williams rasmussen performance measures chosen average test set error normalized true noise level average negative logarithm predictive probability nlpp computed gaussian density function friedman data sets based problem predicting impedance phase respectively four parameters electrical circuit training sets three different sizes signal noise ratio replicated times training set sample size scaled integral squared error ise fv dx varo nlpp computed using data points randomly generated uniform distribution friedman case gpe denoted tables used noise level estimate generated gaussian distribution mean nl true noise level standard deviation nl case cv estimated hyperparameters reparametrized form estimated noise level using case map denoted mp tables used prior sundararajan ands keerthi table results robot arm data sets average normalized test set error tse negative logarithm predictive probability nlpp various methods data set data set tse nlpp tse nlpp ml mp gp cv table results friedman data sets average scaled integral squared error negative logarithm predictive probability given brackets different training sample sizes various methods data set data set ml mp gp cv es given rasmussen gpp approach denoted tables methods conjugate gradient cg algorithm rasmussen used optimize hyperparameters termination criterion relative function error tolerance used constraint maximum number cg iterations set case robot arm data sets algorithm run ten different initial conditions best solution chosen respective best objective function value reported optimization carried separately two outputs results reported average tse nlpp case friedman data sets optimization algorithm run three different initial conditions best solution picked optimization algorithm run one initial condition data sets inputs outputs normalized zero mean unit variance table see performances tse nlpp predictive approaches better ml map approaches data sets case data set observed like ml map methods predictive approaches rightly identified irrelevant inputs performance gpe approach best robot arm data demonstrates usefulness approach good noise level estimate available case friedman data set see table important observation performances ise nlpp gpp cv approaches relatively poor low sample size improve well increases note performances predictive approaches better compared ml map methods starting onwards see nlpp gpe gives best performance performance low sample size also quite good case friedman data set ml map approaches perform better compared predictive approaches except gpe performances gpp cv improve predictive approaches choosing hyperparameters gaussian processes increases close ml map methods next clear map method gives best performance low sample size behavior believe prior plays important role hence useful also note unlike data set performance gpe inferior ml map approaches low sample sizes improves approaches see nlpp increases suggests knowledge noise level alone issue basic issue think predictive approaches estimate predictive performance given model training samples clearly quality estimate become better increases also knowing noise level improves quality estimate discussion simulation results indicate size required get good estimates predictive performance dependent problem sufficiently large find predictive approaches perform better ml map approaches sufficient number samples low evident results friedman data set also map approach best low one would expect performances ml map approaches nearly iv increases comparison existing approaches indicate predictive approaches developed strongly competitive overall cost computing function gradient three predictive approaches mn cost making prediction one required ml map methods proofs results detailed simulation results presented another paper sundararajan keerthi
abstract layered sigmoid belief networks directed graphical models local conditional probabilities parameterised weighted sums parental states learning inference networks generally intractable approximations need considered progress learning networks made using variational procedures demonstrate however variational procedures inappropriate equally important issue inference calculating marginms network introduce alternative procedure based assuming weighted input node approximately gaussian distributed approach goes beyond previous gaussian field assumptions take account correlations parents nodes procedure specialized calculating marginals significantly faster simpler variational procedure introduction layered sigmoid belief networks directed graphical models local conditional probabilities parameterised weighted sums parental states see fig graphical representation distribution set binary variables si typically one supposes states nodes bottom network generated states previous layers whilst principle restriction number nodes layer typically one considers structures similar fan fig higher level layers provide explanation patterns generated lower layers graphical models attractive since correspond layers information processors potentially increasing complexity unfortunately learning inference networks generally intractable approximations need considered progress learning made using variational procedures however another crucial aspect remains inference given evidence none calculate marginal variable conditional evidence assumes found suitable network learning procedure wish present address ncrg aston university birmingham et barber sollich query network whilst variational procedure attractive learning since generally provides bound likelihood visible units demonstrate may always equally appropriate inference problem directed graphical model defines distribution set variables sx sn factorises local conditional distributions sx ip sil ri ri denotes parent nodes node layered network nodes proceeding layer feed node sigmoid belief network local probabilities defined si ri ewijsj oi hi figure layered sigmoid belief network field node defined hi jsj oi wij strength connection node parent node parent set wlj oi bias term gives parent independent bias state node interested inference particular calculating marginals network cases without evidential nodes section describe approximate quantities si discuss section method improve standard variational mean field theory conditional marginals si sk considered section gaussian field distributions coding variables si mean variable mi given probability state using fact local conditional distribution node dependent parents field hi rni si hi hi dhi hi use notation denote average respect distribution many parents node reasonable assumption distribution field hi gaussian hi lui eft gaussian field gf assumption need work mean variance given rjk asjas use notation diagonal terms node covariance matrix rii mi mi contrast previous studies include diagonal terms calculation gaussian fields approximate inference need find correlations parents node easy calculate layered networks considering neither descendant rid si sj mimj fp hj hi hj dh mimj hi hj ni mimj assuming joint distribution hi hj gaussian covariance given need mean ahiahj kwj askas wi wjtr kl kl scheme closed set equations means mi covariance matrix ij solved forward propagation equations start nodes without parents consider next layer nodes repeating procedure full sweep network completed one two dimensional field averages equations computed using gaussian quadrature results extremely fast procedure approximating marginals mi requiring single sweep network approach related common motivating assumption node large number parents used obtain actual bounds quantities interest joint marginals approach give bounds advantage however allows fluctuations fields hi effectively excluded assumed scaling weights wij number parents per node relation variational mean field theory variational approach one fits tractable approximating distribution sbn taking factorised ii rn rni bound lnp sx sn mi lnmi mi mi zmiwijmj oimi ln eh final term causes difficulty even case factorised model formally term graphical structure tractable model one way around around difficulty employ bound associated variational parameters another approach make gaussian assumption field hi section factorised corresponding diagonal correlation matrix gives ln ln barber sollich ui wijmj oi eri wi jmj mj note one dimensional integral smooth function contrast therefore evaluate quantity using gaussian quadrature advantage extra variational parameters need introduced technically assumption gaussian field distribution means longer bound nevertheless practice found little effect quality resulting solution implementation variational approach find optimal parameters mi maximising equation component mi separately cycling nodes parameters mi change repeated times solution highest bound score chosen note equations cannot solved forward propagation alone since final term contains contributions nodes network contrast gf approach section finding appropriate parameters mi variational approach therefore rather slower using gf method arriving equations made two assumptions first intractable distribution well approximated factorised model second field distribution gaussian first step necessary order obtain bound likelihood model although slightly compromised gaussian fielc assumption gf approach dispense assumption effectively factorised network partially interested inference bound model likelihood less relevant gf method may therefore prove useful broader class networks variational approach results unconditional marginals compared three procedures estimating conditional values si nodes network namely variational theory described section diagonal gaussian field theory non diagonal gaussian field theory includes correlation effects parents results small weight values wij shown fig case three methods perform reasonably well although significant improvement using gf methods variational procedure parental correlations important compare figs fig weights biases chosen exact mean variables mi roughly non trivial correlation effects parents note variational mean field theory provides poor solution whereas gf methods relatively accurate effect using non diagonal terms beneficial although dramatically calculating conditional marginals consider calculate conditional marginals given evidential nodes contrast set nodes network output nodes considered evidential write evidence following manner quantities interested conditional marginals bayes rule related joint distribution lie si provided procedure estimating joint marginals obtain conditional marginals without loss generality therefore consider gaussian fields approximate inference erro using factodeed model fit error using gaussian field agonal covariance error using gaussian field non diagonal covariance mean error mean error mean error figure error approximating si network fig averaged nodes network trials weights drawn zero mean unit variance gaussian biases set note different scale use variational procedure factorised section use gaussian field equations assuming diagonal covariance matrix procedure repeated including correlations parents si contains evidential variables desired marginal variable absorbed evidence set convenience split nodes two sets containing evidential clamped nodes remaining free nodes joint evidence given ep ec ec sf sf ues specified gmo belief wo ok si otherwise therefore determined distribution field wk examining see product free nodes defines sbn local probability distributions given original network evidential parental nodes clamped evidence values therefore consistent previous assumptions assume distribution fields hc jointly gaussian find mean covariance matrix distribution repeating calculation section evidential nodes clamped evidence values gaussian determined used determine gaussian averages products sigmoids calculated drawing samples gaussian wish integrate note evidential nodes one two dimensions use gaussian quadrature barber sollich error using factorised model fit error using gaussian field diagonal covariance error using gaussian field non diagonal covariance mean error mean error mean error figure weights set uniformly biases set summed parental weights plus uniform random number root node set probability effect making nodes exact network roughly mean non negligible correlations parental nodes simulations made different layers require correlations fields evaluate inter layer correlations required section able use calculational scheme simply neglect leave study effects assumption future work average factors groups group contains evidential terms particular layer conditional marginal node obtained repeating procedure desired marginal node clamped opposite value using results procedure repeated conditional marginal interested although may seem computationally expensive marginal node computed quickly since equations solved one forward propagation sweep error using factorised model fit error using gaussian field diagonal covariance error using gaussian field non diagonal covariance mean error mean error mean error figure estimating conditional marginal top node state given four bottom nodes state weights drawn zero mean gaussian variance biases set summed parental weights plus uniform random number results simulations results conditional marginals used structure previous experiments shown fig interested calculating probability top node state gaussian fields approximate inference given four bottom nodes state weights chosen zero mean gaussian variance biases set negative half summed parent weights plus uniform random value correlation effects networks strong experiments section although improvement gf theory variational theory seen fig remains clear improvement diagonal terms minimal conclusion despite appropriateness learning variational methods may equally suited inference making tailored methods attractive considered approximation procedure based assuming distribution weighted input node approximately gaussian correlation effects parents node taken account improve gaussian theory although examples gave relatively modest improvements variational mean field theory performs poorly networks strong correlation effects nodes hand one may conjecture gaussian field approach generally perform catastrophically worse factorised variational mean field theory one advantage variational theory presence objective function competing solutions compared however finding optimum solution mean parameters mi function numerically complex since gaussian field theory extremely fast solve interesting compromise might prime variational solution results gaussian field theory acknowledgments db would like thank bert kappen wim wiegerinck stimulating helpful discussions ps thanks royal society financial support
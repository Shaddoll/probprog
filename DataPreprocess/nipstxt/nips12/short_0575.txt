abstract provide analysis turbo decoding algorithm tda setting involving gaussian densities context able show algorithm converges somewhat surprisingly though density generated tda may differ significantly desired posterior density means two densities coincide introduction many applications state system must inferred noisy observations examples include digital communications speech recognition control incomplete information unfortunately problems inference often intractable one must resort approximation methods one approximate inference method recently generated spectacular success certain coding applications turbo decoding algorithm bears close resemblance message passing algorithms developed coding community decades ago shown tda also related well understood exact inference algorithms performance intractable problems applied explained connection several papers developed understanding turbo decoding algorithm exact inference algorithms turbo decoding related variants belief propagation however algorithm designed inference problems graphical models describing conditional independencies form trees whereas graphical models associated turbo decoding possess many loops understand behavior belief propagation presence loops weiss analyzed algorithm cases single loop present ill analyses shed significant light performance tda original coding context include paper develop new line analysis restrictive setting underlying distributions gaussian context inference problems tractable use approximation algorithms tda unnecessary however studying tda context enables streamlined analysis generates new insights behavior particular show algorithm converges mean resulting distribution coincides rusmevichientong roy desired posterior distribution preparing paper became aware two related initiatives involving analysis belief propagation priors gaussian graphs possess cycles weiss freeman studying case graphs possessing cliques size two able show belief propagation converges mean resulting approximation coincides true posterior distribution time frey studied case involving graphical structures generalize employed turbo decoding also conducted empirical study paper organized follows section provide working definition tda section analyze case gaussian densities finally discussion experimental results open issues presented section definition turbo decoding consider random variable taking values distributed according density let yl two random variables conditionally independent given example yl might represent outcomes two independent transmissions signal noisy communication channel observed one might want infer posterior density conditioned obtained first computing densities first conditioned second conditioned ct normalizing operator defined multiplication division carried pointwise unfortunately problem computing generally intractable computational burden associated storing manipulating high dimensional densities appears primary obstacle motivates idea limiting attention densities factor context convenient define operator generates density factors possessing marginals another density particular operator defined ai densities ld one may aim computing proxy unfortunately even problem generally intractable tda viewed iterative algorithm approximating let operators defined analysis turbo decoding gaussian densities density tda applicable cases computation two operations tractable algorithm generates sequences according flq initialized densities factor hope po converges approximation rf gaussian case consider setting joint density yl gaussian context application tda warranted tractable algorithms computing conditional densities priors gaussian objective however provide setting tda analyzed new insights generated proceeding let us define notation facilitate exposition write eg denote gaussian density whose mean vector nd covariance matrix respectively matrix denote diagonal matrix whose entries given diagonal elements diagonal matrices write xii pair nonsingular covariance matrices eu nonsingular let matrix defined ar reduce notation sometimes denote matrix au random variables yl jointly gaussian densities also gaussian let assume symmetric positive definite matrices also assume identity matrix easy show ar well defined following lemma provides formulas means covariances arise multiplying rescaling gaussian densities result follows simple algebra state without proof lemma let positive definite positive definite one immediate consequence lemma expression mean az let denote set covariance matrices diagonal positive definite let denote set gaussian densities covariance matrices following result state without proof lemma set closed tda initialized lemma allows us represent iterates using appropriate mean vectors covariance matrices rusmevichientong roy convergence analysis suitable technical conditions shown sequence mean vectors covariance matrices generates tda converges due space limitations present results pertinent convergence covariance matrices furthermore present certain central components analyses complete results detailed analyses refer reader upcoming full length paper recall tda generates sequences according discussed earlier algorithm initialized elements lemma appropriate sequences mean vectors covariance matrices turns mappings let establish convergence suffices show tn converges following theorem establishes points limit depend initial iterates theorem exists matrix lim tn preliminary lemmas proof theorem relies lemmas present section begin lemma captures important abstract properties function due space constraints omit proof even though nontrivial lemma exists matrix function continuous following lemma establishes convergence sequence covariance matrices initialized identity matrix lemma sequence converges fixed point proof lemma follows monotonicity lemma since bounded matrix sequence converges fact limit fixed point follows continuity lemma let lim matrix plays following special role lemma matrix unique fixed point analysis turbo decoding gaussian densities proof converges monotonic matrix fixed point furthermore lemma matrix fixed point let av lemma vv therefore result follows proof theorem proof convergence follows lemma monotonicity lemma convergence follows fact consequence two previously invoked lemmas together lemma let us address case ofv let defined proof lemma vv vv monotonicity tn vv tn vv follows tn vv converges since continuous limit must unique fixed point established convergence elements satisfying elements convergence follows monotonicity analysis fixed point discussed previous section suitable conditions possess unique fixed point tda converges fixed points let pq eq pq eq denote fixed points respectively based theorem eq eq following lemma provides equation relating means associated fixed points hard show aq ar ar used statement well defined lemma proof follows definitions ql result follows lemma fact alter mean distribution prove central result paper mean density generated tda coincides mean desired posterior density theorem po aq proof lemma ar mean po wll sow expressions equl rusmevichientong roy figure evolution errors multiplying equations lemma appropriate matrices obtain aq eq aq aq follows therefore note zq eq aq lq follows aq et zl discussion experimental results limits convergence tda provide approximation po rf established mean approximation coincides desired density one might expect covariance matrix po approximates even bear relation unfortunately illustrated experimental results section expectations appear inaccurate performed experiments involving dimensional gaussian densities either dimensional instance problem instances sampled randomly fixed distribution due space limitations describe tedious details sampling mechanism figure illustrates evolution certain errors representative runs tda dimensional problems first graph plots relative errors means densities generated iterates tda indicated analysis errors converge zero second chart plots measure relative error covariance versus rf representative runs though covariances converge ultimate errors far zero two analysis turbo decoding gaussian densities figure errors iterations final graphs plot errors means respectively though means converge ultimate errors large figure provides plots sorts errors measured different instances dimensional problems th iteration tda horizontal axes labeled indices problem instances note errors first graph close zero units vertical axis must multiplied errors measured relative terms hand errors graphs vary dramatically intriguing least context gaussian densities tda effectively compute conditional means without accurately approximating conditional densities also interesting note context communications objective choose code word comes close transmitted code one natural way involves assigning code word maximizes conditional density one highest chance correct gaussian case studied corresponds mean quantity computed correctly tda interesting explore generalizations line analysis presented paper classes densities references benedetto montorsi unveiling turbo codes results parallel concatenated coding schemes ieee trans inform theory vol pp mar berrou glavieux thitimajshima near shannon limit error correcting coding urbo codes pvoc int conf corafaun geneva switzerland may pp frey urbo factor analysis appear advances neural information processing ysteras callaget low density parity check codes cambridge mit press kschischang frey iterative decoding compound codes probability propagation graphical models ieee journal selected areas corafaun vol pp feb mceliece mackay cheng turbo decoding instance pearl belief propagation algorithm ieee journal selected areas coramun vol pp feb pearl probab listic reasoning intelligent systems networks plausible inference san mateo ca morgan kaufmann richardson geometry turbo decoding dynamics dec appear ieee trans infowra theory richardson urbanke capacity low density parity check codes message passing decoding submitted ieee trans information theory richardson shokrollahi urbanke design provably good low density parity check codes submitted ieee trans information theory weiss belief propagation revision networks loops november available ftp publications ai mit edu weiss freeman correctness belief propagation gaussian graphical models arbitrary topology appear advances neural information processing systems
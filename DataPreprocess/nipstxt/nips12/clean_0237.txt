abstract generalize recent formalism describe dynamics supervised learning layered neural networks regime data recycling inevitable case noisy teachers theory generates reliable predictions evolution time training generalization errors extends class mathematically solvable learning processes large neural networks situations overfitting occur introduction tools statistical mechanics used successfully last decade study dynamics learning layered neural networks reviews see simplest theories result upon assuming data set much larger number weight updates made rules recycling ensures distribution relevance gaussian unfortunately terms applications terms mathematical interest regime relevant one complications peculiarities dynamics learning arise precisely due data recycling creates system possibility improve performance memorizing answers rather learning underlying rule dynamics learning restricted training sets first studied analytically linear learning rules systems binary weights latter studies ahead time get attention deserved stage even simpler learning dynamics without data recycling yet studied recently attention moved back dynamics learning recycling regime studies aimed developing general theory finding exact solutions special cases general theories published far common yet considered realizable scenario rule learned implementable student overfitting could yet occur next hurdle restricted training sets combined unrealizable rules turned non typical solvable cases involving hebbian rules noisy reverse wedge teachers recently cavity method used build general theory yet batch learning paper generalize general theory launched applies arbitrary learning rules case noisy teachers mirror closely presentation dealing simpler case noise free teachers refer background reading ideas behind formalism coolen mace definitions restrict simplicity perceptrons student perceptron operates linear separation parametrised weight vector sgn aims emulate teacher erating similar rule however characterized variable weight vector drawn random distribution output noise gaussian weight noise ex parameters control amount teacher noise noise free teacher recovered limits student modifies iteratively using examples input vectors drawn random fixed randomly composed training set containing vectors corresponding values teacher outputs choose teacher noise consistent answer given teacher question remain particular question appears learning process thus sgn teacher weight vectors drawn randomly independently generalize training set accordingly bp consistency teacher noise natural terms applications prerequisite overfitting phenomena averages training set denoted lb averages possible input vectors analyze two classes learning rules form aj line aj batch aj line learning one draws step question answer pair random training set batch learning one iterates deterministic map average data training set performance measures training generalization errors defined follows step function et eg introduce macroscopic observables taylored present problem generalizing eliminate technical subtleties assuming number arguments evaluated go infinity limit cx taken derivation macroscopic laws upon generalizing calculations one finds line learning xdydz xo xdydz dt xdydz lvr dx dy dz dx dy dz xp dx dy dz supervised learning restricted training sets complexity problem concentrated green function lim oo ill diy involves conditional average form cwr fdj pt jiq pt yz pt jiq dj pt rlxyz pt weight probability density time solution used generate cx performance measures time et dxdydz xz eg arccos expansion equations powers retaining terms linear gives corresponding equations describing batch learning far analysis exact closure macroscopic laws close macroscopic laws making two key assumptions underlying dynamical replica theory cx macroscopic observables obey closed dynamic equations ii equations self averaging respect specific realization implies probability variations within subshells either absent irrelevant macroscopic laws may thus make simplest choice pt jiq pt jlo hs procedure leads exact laws observables indeed obey closed equations imum engopy approximation ii allows us average macroscopic laws training sets observed simulations proven using formalism assumptions result closure since green function written terms final ingredient dynamical replica theo average fractions replica identity aj jlbl jid lim dj willy problem reduced calculating non trivial integrals averages one finds zly exp sho hands dy dy dydxdz zly write resulting macroscopic laws case output noise following compact way vez zly dx zly zly ry wy ffx rw solution time following form xs coolen mace finding function replica symmetric ansatz requires solving saddle point problem scalar observable two functions zly upon introducing qq fax xly ebxsf dx xly bx fdx xly saddle point equations acquire form xly qq ry qq qq dyds equations determine xly sgucture cogesponding single equation proofs apply solutions xly given physical range unique function given ds qq zly ad working predictions equations generally cpu intensive mainly due functional saddle point equation solved time step however one construct useful approximations theory increasing complexity large approximation giving simplest theory without saddle point equations ii conditionally gaussian approximation dependent moments iii annealed approximation functional saddle point equation benchmark tests limits first show limit theory reduces simple formalism infinite training sets worked noisy teachers upon making ansatz xly xly ryi one finds xly xly ry insertion ansatz followed rearranging terms usage expression shows satisfied remaining equations involve averages gaussian distribution indeed reduce ld ffq ld cttr next turn limit restricted aining sets noise free teachers show eow reproduces focalism following ansatz xly xly zly xly insertion shows solutions fo indeed solve equations giving xly xly leaving us exactly formalism describing case noise free teachers res icted aining sets ap new terms due presence weight decay absent supervised learning restricted training sets ot ot figure line hebbian learning conditionally gaussian approximation versus exact solution left right solid lines approximated theory dashed lines exact result upper curves eg functions time two theories agree lower curves et functions time benchmark tests hebbian learning special case hebbian learning sgn solved exactly time arbitrary providing yet another excellent benchmark theory batch execution hebbian learning macroscopic laws obtained upon expanding retaining terms linear integrations done equations solved explicitly resulting vr ro nvt qo nvt nvt nvt ro rt rt xly ry sgncy av cq results turn follow peffomance mereurns eg arccos affl lylr et erf comprison exact solution calculamd along lines equivalently obtained upon putting shows expressions exact line execution cannot yet solve functional saddle point equmion general however analytical predictions still ex acted om qoe nvt fax xly wr rtl aq comparison results shows expressions thus also eg fully exact time observables involving including training error easily solved equations instead used conditionally gaussian approximation found adequate noiseless hebbian case result shown figure agreement reasonable significantly less apparently teacher noise adds deformation field distribution away gaussian shape coolen mac ooooo figure large approximation versus numerical simulations top row perceptron rule bottom row adatron rule left training errors et generalisation errors functions time lines approximated theory markers simulations circles et squares eg right joint distributions student field teacher noise fdy upper lower histograms simulations lines approximated theory non linear learning rules theory versus simulations case non linear learning rules exact solution known test formalism leaving numerical simulations yardstick evaluated numerically large approximation theory perceptron learning sgn zz adatron learning sgn lzlo zz approximation leads following fully explicit equation field distributions xly dx ly xly wy ryl rw dydx xlylg xp xly dydx xly ap iiy oyax xp xly ix supervised learning restricted training sets short hands fdx xpe xly result comparison shown figure note et increases monotonically eg decreases monotonically noise free formalism large approximation appears capture dominant terms predicting power theory mainly limited numerical constraints instance adatron learning rule generates singularities distributions especially small although predicted theory almost impossible capture numerical solutions discussion shown recent theory describe dynamics supervised learning restricted training sets designed apply data recycling regime arbitrary online batch learning rules large layered neural networks generalized successfully order deal also noisy teachers generalized approach joint distribution fields student clean teacher noisy teacher taken dynamical order parameter addition conventional observables order parameter set derive generalization error eg training error et following prescriptions dynamical replica theory one finds diffusion equation evaluated making replica symmetric ansatz carried several orthogonal benchmark tests theory cx data recycling theory exact ii teacher noise theory reduces iii batch hebbian learning theory exact line hebbian learning theory exact regard predictions eg dependent conditional averages fdz zp zly time crude approximation equations already gives reasonable agreement exact results et non linear learning rules perceptron adatron compared numerical solution simple large aproximation equations numerical simulations found satisfactory agreement paper preliminary presentation results obtained second stage research programme aimed extending theoretical tools arena learning dynamics building ongoing work aimed systematic application theory approximations various types non linear learning rules generalization theory multi layer networks
abstract many classification tasks recognition accuracy low input patrems corrupted noise spatially temporally overlapping propose approach overcoming limitations based model human selective attention model early selection filter guided top attentional control entertains candidate output class sequence adjusts attentional gain coefficients order produce strong response class chosen class one obtains strongest response least modulation attention present simulation results classification corrupted superimposed handwritten digit pattems showing significant improvement recognition rates algorithm also applied domain speech recognition comparable results introduction many classification tasks recognition accuracy low input patterns corrupted noise spatially temporally overlapping approaches proposed make classifiers robust perturbations requiring classifiers low input output mapping sensitivity propose approach based human selective attention people use selective attention focus critical features stimulus suppress irrelevant features seems natural incorporate selective attention mechanism pattern recognition systems noisy real world applications psychologists many years studied mechanisms selective attention however controversy still exists among competing theories models sufficiently well defined apply engineering pattem recognition problems fukushima incorporated selective attention attention switching algorithms neocognitron model demonstrated good recognition performance superimposed digits however neocognitron model many unknown parameters must determined heuristically performance sensitive parameter values also computational requirements prohibitively expensive many realtime applications rao also recently introduced selective attention model based lee mozer kalman filters demonstrated classifications superimposed patterns however model based linear systems nonlinear extension straightforward definitive approach incorporating selective attention pattern recognition propose novel approach show improve recognition accuracy psychological views selective attention modern study selective attention began broadbent broadbent presented two auditory channels subjects one ear asked subjects shadow one channel observed although subjects could recall took place unshadowed channel could often recall last seconds input channel therefore suggested brain briefly stores incoming stimuli stimulus information fades neither admitted conscious mind encoded way would permit later recollection unless attention directed toward view known early filtering early selection model treisman proposed modification view filter merely attenuates input rather absolutely preventing analysis although late selection hybrid views attention proposed clear early selection plays significant role human information processing question attention acts stream processing independent another important issue factors drive attention select one ear one location instead another attention may directed based low level stimulus features amplitude sound color visual stimulus type attentional control often called bottom attention may also directed based expectations object knowledge location critical task relevant information expected type attentional control often called top multilayer perceptron architecture selective attention borrow notion early selection filter top control integrate multilayer perceptron mlp classifier depicted figure dotted box standard mlp classifier attention layer one one connectivity added front input layer although depicted mlp single hidden layer approach applicable general mlp architectures kth element input vector denoted gated kth input mlp attention gain filtering coefficient ak previously first author shown benefit treating ak like ordinary adaptive parameters training present work fix attention gains training causing architecture behave ordinary mlp however allow gains adjusted classification test patterns basic conjecture recognition accuracy may improved attention suppress noise along irrelevant dimensions enhance weak signal along relevant dimensions relevant irrelevant determined topdown control attention essentially use knowledge trained mlp determine input dimensions critical classifying test pattern concrete consider mlp trained classify handwritten digits test pattern presented adjust attentional gains via gradient descent make input good example class possible different output classes choose class strongest response obtained smallest robust pattern recognition via selective attention attentional modulation exact quantitative rule presented conjecture net achieve strong response class making small attentional modulation class likely correct whichever class would selected without applying selective anention xn al yl ym figure mlp architecture selective anention process adjusting attentional gains achieve strong response particular class call attention class proceeds follows first target output vector ts ts tsm defined bipolar binary output representations anention class others second anention gain ak set third anention gain ak adapted minimize error given input xn pre trained frozen synaptic weights update rule based gradient descent algorithm error back propagation th iterative epoch anention gain ak updated ak rl la lb denotes anention output error thej th attribute back propagated error first hidden layer synaptic weight input th neuron first hidden layer finally step size anention gains thresholded lie application selective anention test example summarized follows step apply test input pattern trained mlp compute output values step classes top activation values initialize anention gain ak set target vector apply test pattern anention gains network compute output apply selective anention algorithm eqs adapt anention gains repeat steps anention process converges compute anention measure asymptotic network state lee mozer step select class minimum attention measure recognized class attention measure defined dte xk ak yi di square euclidean distance two input patterns application selective attention eo output error application selective attention di eo normalized number input pixels number output classes respectively superscript attention classes omitted simplicity make measure dimensionless quantity one may normalize eo input energy training output error respectively however affect selection process step one think attended input minimal deformation test input needed trigger attended class therefore euclidean distance good measure classification confidence fact basically quantity minimized rao however mlp classifier model capable nonlinear mapping input output patterns nearest neighbor classifier training data examples could also used find minimum distance class model mlp classifier computes similar function without large memory computational requirements proposed selective attention algorithm tested recognition noisy numeral patterns numeral database consists samples handwritten digits collected people total samples digit encoded binary pixel array roughly pixels black coded white pixels coded four experiments conducted different training sets training patterns one hidden layer mlp trained back propagation numbers input hidden output neurons respectively three noisy test patterns generated training pattern randomly flipping pixel value probability pf test patterns presented network classification figure false recognition rate plotted function number candidates considered attentional manipulation rn note run time algorithm proportional rn increasing rn imply lax classification criterion additional external knowledge playing classification results shown three different pixel inversion probabilities pf considering average black pixels data noisy input patterns pf correspond snr approximately db condition figure false recognition rates four different training sets marked means connected solid curve robust pattern recognition via selective attention standard mlp classifier corresponds rn active output mlp considered candidate response false recognition rate clearly lower attentional manipulation used select response mlp nit appears performance improve considering top three candidates attention switching superimposed patterns suppose superimpose binary input patterns two different handwritten digits using logical operator pixels corresponding black ink logical value use attention recognize two patterns sequence extreme case situation common visual pattem recognition two patterns spatially overlapping explore following algorithm first one pattern recognized selective attention process used section second attention switched recognized pattern remaining pixels image switching accomplished removing attention pixels recognized pattern attentional gain input clamped following switching value first stage selective attention process input attended recognition first pattern gains set third recognition process selective attention performed recognize second pattem proposed selective attention attention switching algorithm tested recognition superimposed numeral data four experiments conducted number candidates pf number candidates pf number candidates pf figure false recognition rates noisy patterns function number top candidates binary pixel training pattems randomly inverted robabilitv pc lee mozer different training sets experiment patterns selected training patterns test patterns generated superimposing pairs patterns different output classes test patterns still binary figure examples selective attention attention switching figure shows six examples selective attention attention switching algorithm action consisting four panels horizontal sequence six examples formed superimposing instances following digit pairs first panel example shows superimposed pattern second panel shows attended input first round classification input continuous values thresholded values facilitate viewing figure third panel shows masking pattern attention switching generated thresholding input pattern fourth panel shows residual input pattern second round classification attended input analog values thresholded shown second rectangles figure shows attention switching done effectively remaining input patterns second classifier quite visible compared performance three different methods first simply selected two mlp outputs highest activity method utilizes neither selective attention second performed attention switching apply selective attention third performed attention switching selective attention table summarizes recognition rates first second patterns read mlp three methods hypothesized attention switching increases recognition rate second pattern selective attention increases recognition rate first second pattem table recognition rates two superimposed numeral pattems first pattern second pattern selective attention switching switching switching selective attention robust pattern recognition via selective attention conclusion paper demonstrated selective attention algorithm noisy superimposed patterns obtains improved recognition rates also proposed simple attention switching algorithm utilizes selective attention framework improve performance superimposed patterns algorithms simple easily implemented feedforward mlps although experiments preliminary suggest attention based algorithms useful extracting recognizing multiple patterns complex background conducted simulation studies supporting conjecture domain speech recognition integrate presentation accepted nips acknowledgements lee acknowledges supports korean ministry science technology thank dr le cun providing handwritten digit database
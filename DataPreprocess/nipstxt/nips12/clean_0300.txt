abstract known decision tree learning viewed form boosting however existing boosting theorems decision tree learning allow binary branching trees generalization multi branching trees immediate practical decision tree algorithms cart implement trade number branches improvement tree quality measured index function give boosting justification particular quantitative trade curve main theorem states essence require improvement proportional log number branches top greedy construction decision trees remains effective boosting algorithm introduction decision trees proved popular tool experimental machine learning popularity stems two basic features constructed quickly seem achieve low error rates practice cases time required tree growth scales linearly sample size efficient tree construction allows large data sets hand although known theoretical handicaps decision tree representations seem practice achieve accuracy comparable learning paradigms neural networks decision tree learning algorithms popular practice seems hard quantify success theoretical model fairly easy see even target function described using small decision tree tree learning algorithms may fail find good approximation kearns mansour used weak learning hypothesis show standard tree learning algorithms perform boosting provides theoretical justification decision tree learning similar boosting multi way branching decision trees justifications given various boosting algorithms adaboost decision tree learning algorithms use top growth process given current tree algorithm selects leaf node extends internal node assigning branching function adding leaf possible output value branching function set branching functions may differ one algorithm another algorithms used practice try keep set branching functions fairly simple example branching function depends single attribute categorical attributes branching according attribute value continuous attributes performs comparison attribute constant course top tree growth fit data easy construct large tree whose error rate training data zero however class splitting functions finite vc dimension possible prove high confidence choice training data trees true error rate bounded error rate training sample iti number leaves rn size training sample fitting avoided requiring top tree growth produce small tree practice usually done constructing large tree pruning away nodes take slightly different approach assume given target tree size consider problem constructing tree iti small possible avoid fitting selecting small target value tree size fundamental question top tree growth select branching function growing given leaf think target size budget four way branch spends tree size budget two way branch four way branch increases tree size roughly amount two twoway branches sufficiently large branch would spend entire tree size budget single step branches spend tree size budget required achieve progress branches spending less budget naively one would expect improvement required roughly linear number new leaves introduced one get return proportional expense however weak learning assumption target tree size define nontrivial game learner adversary learner makes moves selecting branching functions adversary makes moves presenting options consistent weak learning hypothesis prove learner achieve better value game selecting branches get return considerably smaller naive linear return main theorem states essence return need proportional log number branches preliminaries assume set instances unknown target function mapping assume given training set set pairs form let set potential branching functions function finite set rh allow different functions different ranges require rhl tree mansour mcallester tree internal node labeled branching function children corresponding elements set rh define number leaf nodes let set leaf nodes given tree leaf node sample write st denote subset sample reaching leaf define dr fraction sample reaching leaf st define fraction pairs fix st training error denoted tel ldt min weak learning hypothesis boosting view top decision tree learning form boosting boosting describes general class iterative algorithms based weak learning hypothesis classical weak learning hypothesis applies classes boolean functions let subset branching functions rh classical weak learning hypothesis states distribution exists prt fix algorithms designed exploit particular hypothesis classes boolean functions proved quite useful practice kearns mansour show key using weak learning hypothesis decision tree learning use index function min defined note conditions imply sample let qw fraction pairs let ta decision tree consisting single internal node branching function plus leaf member iral let iw ta denote value ta measured respect sample let denote qw iw th quantity reduction index sample achieved introducing single branch also note dta reduction leaf replaced branch kearns mansour prove following lemma lemma kearns mansour assuming weak learning hypothesis taking sample exists qw lemma motivates following definition definition say satisfies weak tree growth hypothesis sample exists qw lemma states essence classical weak learning hypothesis implies weak tree growth hypothesis index function empirically however weak tree growth hypothesis seems hold variety index functions already used tree growth prior work kearns mansour ginni index used cart entropy qlog log used long empirically observed possible make steady progress reducing choices difficult make steady progress reducing define simple binary branching procedure given training set target tree size algorithm grows tree iti algorithm boosting multi way branching decision trees denotes trivial tree whose root leaf node tt denotes result replacing leaf branching function new leaf element rh iti argmax argmax end ff define quantity ii note ii zia inn theorem kearns mansour yt satisfy weak tree growth hypothesis binary branching procedure produces tree lti proof proof induction number iterations procedure initial tree immediately satisfies condition assume condition satisfied begining iteration prove remains satisfied tt end iteration since leaf selected procedure sti qt weak tree growth assumption function selected procedure property st yi tt ta st btyi qt implies iti statement main theorem construct tree growth algorithm selects multi way branching functions many weak learning hypotheses weak tree growth hypothesis viewed defining game learner adversary given tree adversary selects set branching functions allowed leaf tree subject constraint leaf adversary must provide binary branching function yi learner selects leaf branching function replaces tt adversary selects new set options leaf subject weak tree growth hypothesis proof theorem implies even adversary reassign options every move exists learner strategy binary branching procedure guaranteed achieves final error rate iti course optimal play adversary game provide single binary option leaf however practice adversary make mistakes provide options learner exploited achieve even lower error rates objective construct strategy learner exploit multi way branches provided adversary first say branching function acceptable tree target size mansour mcallester either iral iti irads ra also define quantity noted also noted nk hence nk small nk hence define following multi branch tree growth procedure argmax argmaxa acceptable tt end run multi branch tree growth procedure called boosting iteration branching function selected property st iral qt weak tree growth hypothesis implies lahl qt therefore weak tree growth hypothesis implies every run multi branch growth procedure bootsing run bootsing exploiting mutli way branches even weak tree growth hypothesis fails following main theorem paper theorem produced boosting run multi branch tree growth procedure lt proof theorem prove main theorem need concept visited weighted tree vwtree short vw tree tree node rn assigned rational weight integer visitation count define following vw tree growth procedure procedure tree consisting single root node weight visitation count tree tt ok result inserting new leaves leaf ith new leaf weight wi new leaves visitation count rational number number steps repeat following argmaxt vl vl optionally tt ov wv vt wt first prove analog theorem procedure vw tree define iti lsi vt define tsi vt wt lemma vw procedure maintains invariant iti proof proof induction number iterations algorithm result immediate initial tree since assume iti start iteration show remains true end iteration boosting multi way branching decision trees associate leaf vt subleaves weight vt wt vt iti total number subleaves total weight subleaves therefore must exist subleaf whose weight least hence must exist leaf satisfying vt wt vt iti therefore relation must hold leaf selected procedure let tree resulting incrementing vt vt wt vt wt vt wt vt wt vt wt iti finally procedure grows new leaves increase remains hence invariant maintained internal node tree let denote set nodes children vw tree called locally well formed every internal node ic ic vw tree called globally safe maxt vt wt vt minm vt wt vt denotes set internal nodes lemma locally well formed globally safe vw tree possible output vw growth procedure therefore lti proof since locally well formed use template making nondeterministic choices vw growth procedure process guaranteed produce provided growth procedure never forced visit node corresponding leaf global safety condition guarantees unfinished internal node weight least large leaf node give way mapping trees vw trees specifically tree define vw result assigning node weight qm internal node visitation count equal number children leaf node visitation count equal following lemmas lemma grown boosting run multi branch procedure vw locally well formed proof note children internal node derived selecting branching function node since run boosting irhl qt therefore th irl implies th multiplying transforming result weights tree vw gives desired result following lemma suffices theorem lemma grown boosting run multi branch procedure vw globally safe proof first note following invariant boosting run multi branch procedure max wt min wt vw vw mansour mcallester proof simple induction boosting tree growth using fact procedure always expands leaf node maximal weight must show every internal node every leaf wl number children note reduces wl follows invariant assume without loss generality also since suffices show wt let internal node children let tree time rn selected expansion let wt maximum weight leaf final tree definition acceptability condition last iterations performing binary branching binary expansion reduces index least times weight selected node since sequence nodes selected multi branch procedure non increasing weights iteration weight selected node least wt since least binary expansions expansion reduces least wl acceptability condition written kit yields wl klt yields wl desired
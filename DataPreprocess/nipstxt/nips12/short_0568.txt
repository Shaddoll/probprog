abstract fishers linear discriminant analysis lda classical multivariate technique dimension reduction classification data vectors transformed low dimensional subspace class centroids spread much possible subspace lda works simple prototype classifier linear decision boundaries however many applications linear boundaries adequately separate classes present nonlinear generalization discriminant analysis uses kernel trick representing dot products kernel functions presented algorithm allows simple formulation em algorithm terms kernel functions leads unique concept unsupervised mixture analysis supervised discriminant analysis semi supervised discriminant analysis partially unlabelled observations feature spaces introduction classical linear discriminant analysis lda projects data vectors belong different classes dimensionai space way ratio group scatter within group scatter sw maximized lda formally consists eigenvalue decomposition sb leading called canonical vatlares contain whole class specific information dimensional subspace canonical variates ordered decreasing eigenvalue size indicating first variates contain major part information consequence procedure allows low dimensional representations therefore visualization data besides interpreting lda technique dimensionality reduction also seen multi class classification method set linear discriminant functions define partition projected space regions identified class membership new observation assigned class centroid closest projected space overcome limitation linear decision functions attempts made incorporate nonlinearity classical algorithm hastie et al introduced called model flexible discriminant analysis lda reformulated framework linear regression estimation generalization method given using nonlinear regression techniques proposed regression techniques implement idea using nonlinear mappings transform input data new space linear regression performed real world nonlinear discriminant analysis using kernel functions applications approach deal numerical problems due dimensional explosion resulting nonlinear mappings recent years approaches avoid explicit mappings using kernel functions become popular main idea construct algorithms afford dot products pattern vectors computed efficiently high dimensional spaces examples type algorithms support vector machine kernel principal component analysis paper show possible formulate classical linear regression therefore also linear discriminant analysis exclusively terms dot products therefore kernel methods used construct nonlinear variant discriminant analysis call technique kernel discriminant analysis kda contrary similar approach published recently algorithm real multi class classifier inherits classical lda convenient property data isualization review linear discriminant analysis assumption data centered xi scatter matrices sw defined nj sb ej el nj number patterns belong class lda chooses transformation matrix maximizes objective function ivrssvl ivtswv columns optimal generalized eigenvectors correspond nonzero eigenvalues ssvi hi swvi shown standard lda algorithm restated exclusively terms dot products input vectors final equation eigenvalue equation terms dot product matrices size since solution high dimensional generalized eigenvalue equations may cause numerical problems may large real world applications present improved algorithm reformulates discriminant analysis regression problem moreover version allows simple implementation em algorithm feature spaces linear regression analysis section give brief review linear regression analysis use building block lda task linear regression analysis approximate regression function linear function yla basis sample yn xn let denote vector yn denote data matrix rows input vectors using quadratic loss function optimal parameters chosen minimize average squared residual ilu denotes vector ones denotes ridge type penalty matrix el penalizes coefficients assuming data centered xi parameters regression function given yi uy xrx ei xry roth steinhage lda optimal scoring section lda problem linked linear regression using framework penalized optimal scoring give overview detailed derivation considering problem classes data vectors class memberships represented categorical response variable levels useful code responses terms indicator matrix zi th data vector belongs class otherwise point optimal scoring turn categorical variables quantitative ones assigning scores classes score vector assigns real number th level vector represents vector scored training data regressed onto data matrix simultaneous estimation scores regression coefficients constitutes optimal scoring problem minimize criterion asr fi llzo still constraint according given score minimizing fi given rios xrx fi rzo partially minimized criterion becomes minasr fi zt fl xtx fl denotes regularized hat smoother matrix minimizing constraint zo performed following procedure choose initial matrix satisfying constraint ztzoo set zoo run multi response regression onto xb matrix regression coefficients eigenanalyze obtain optimal scores update matrix regression coefficients bw matrix ofeigenvectors shown final matrix diagonal scale matrix equivalent matrix lda vectors see ridge regression using dot products penalty matrix assures penalized covariance matrix xtx ei symmetric nonsingular matrix therefore eigenvectors ai accomplished positive eigenvalues following equations hold aia first equation implies first leading eigenvectors ai eigenvalues expansion terms input vectors note number nonzero eigenvalues unpenalized covariance matrix xtx together follows general case dimensionality may extend fi written sum two terms expansion terms vectors xi coefficients ai similar expansion terms remaining eigenvectors ixi jaj xtoi jaj fi however last term dropped since every eigenvector aj orthogonal every vector xi influence value regression function problem penalized linear regression therefore stated minimizing nonlinear discriminant analysis using kernel functions ilu stationary vector determined xx let dot product matrix defined kij rit rj let given test point dot product vector kt defined kt xxt notation regression function test point xt reads equation requires dot products apply kernel trick final equation constant term also found ders et al restated ridge regression dual variables optimized resulting criterion function lagrange multiplier technique note derivation direct generalization standard linear regression formalism leads natural way cl general regression functions including constant term lda using dot products setting xtc using notation section given score optimal vector given otos xx zo analogous partially minimized criterion becomes minasr zr fi zo ot fl xx xx ei minimize constraint llzoll procedure described section used fl substituted fl matrix rows input vectors projected onto column vectors given xb ei zoow note dot product matrix needed calculate kernel trick main idea constructing nonlinear algorithms apply linear methods space observations feature space related former nonlinear mapping assuming mapped data centered xi presented algorithms remain formally unchanged dot product matrix computed kij qb xi qb xj shown assumption dropped writing instead mapping xi qb xi qb xi computation dot products feature spaces done efficiently using kernel functions xi xj choices exists mapping feature space acts dot product among possible kernel functions radial basis function rbf kernels form exp ull em algorithm feature spaces lda derived maximum likelihood method normal populations different means common covariance matrix see coding class membership observations matrix section lda maximizes complete data log likelihood function roth steinhage concept generalized case group membership nc observations known em algorithm provides convenient method maximizing likelihood function missing data step set pki prob xi class fzik class membership xi observed otherwise cfi xi exp xi xi step set rk pki ilk rk pkixi pki xi xi ak idea behind approach even unclassified observation used estimation given proper weight according posterior probability class membership step seen weighted mean covariance maximum likelihood estimates weighted augmented problem augment data replicating observations times th replication observation weights pti maximization likelihood function achieved via weighted augmented lda turns necessary explicitly replicate observations run standard lda optimal scoring version lda described section allows implicit solution augmented problem still uses observations instead using response indicator matrix one uses blurred response matrix whose rows consist current class probabilities observation step used multiple linear regression followed eigen decomposition detailed derivation given since shown optimal scoring problem solved feature spaces using kernel functions also case whole em aigorithm step requires differences mahalonobis distances supplied kda iterated application step observation classified class highest probability leads unique framework pure mixture analysis nc pure discriminant analysis nc semisupervised models discriminant analysis partially unclassified observations nc feature spaces experiments waveform data illustrate kda popular simulated example taken used three class problem variables learning set consisted observations per class test set size results given table table results waveform data values averages simulations entries line taken qda quadratic discriminant analysis fda flexible discriminant analysis mda mixture discriminant analysis technique training error test error lda qda fda best model parameters mda best model parameters kda rbf kernel nonlinear discriminant analysis using kernel functions bayes risk problem kda outperforms nonlinear versions discriminant analysis reaches bayes rate within error bounds indicating one cannot expect significant improvement using classifiers figure demonstrates data visualization property kda since class problem dimensionality projected space equals data visualized without loss information left plot one see projected learn data class centroids right plot shows test data class centroids learning set figure data visualization kda left learn set right test set demonstrate effect using unlabeled data classification repeated experiment waveform data using labeled observations per class compared classification results test set size using labeled data error rate results em model considers test data incomplete measurements iterative maximization likelihood function error rate using rbf kernel obtained following mean error rates simulations classification performance could drastically improved including unlabelled data learning process object recognition tested kda mpi chair database consists regular spaced views form upper viewing hemisphere different classes chairs training set random views class test set available images downscaled pixels use additional edge detection patterns view classification results several classifiers given table table test error rates support vector machine multi layer perceptron oriented filter taken svm mlp kda rbfkernel kdapoly kernel comparison computational performance also trained svm light implementation data experiment classes kda algorithm showed significantly faster svm using rbfkernel kda times faster polynomial kernel kda times faster vm light discussion paper present nonlinear version classical linear discriminant analysis main idea map input vectors high even infinite dimensional feature space apply lda enlarged space restating lda way dot products input vectors needed makes possible use kernel representations dot products overcomes numerical problems high dimensional database available via ftp ftp mpik tueb mpg de pub chair dataset roth steinhage feature spaces studied classification performance kda classifier simulated waveform data mpi chair database widely used benchmarking literature medium size problems especially number classes high kda algorithm showed significantly faster svm leading classification performance classical lda presented algorithm inherits convenient property data visualization since allows low dimensional views data vectors makes intuitive interpretation possible helpful many practical applications presented kda algorithm used maximization step em algorithm feature spaces allows include unlabeled observation learning process improve classification results studying performance kda classiftcation problems well theoretical comparison optimization criteria used kda svm algorithm subject future work acknowledgements work supported deutsche forschungsgemeinschaft dfg heavily profitted discussions armin cremers john held lothat hermes references duda hart pattern classification scene analysis wiley sons hastie tibshirani buja flexible discriminant analysis optimal scoring jasa vol pp vapnik statistical learning theory wiley sons schslkopf smola muller nonlinear component analysis kernel eigenvalue problem neural computation vol pp mika itsch weston schslkopf milllet fisher discriminant analysis kernels neural networks signal processing ix hu larsen wilson douglas eds pp ieee roth steinhage nonlinear discriminant analysis using kernel functions tech rep iai tr department computer science iii bonn university roth pogoda steinhage schrsder pattern recognition combining feature pixel based classification within real world application mustererkennung fsrstner buhmann faber faber eds informatik aktuell pp dagm symposium bonn springer hastie buja tibshirani penalized discriminant analysis armstat vol pp saunders gammermann vovk ridge regression learning algorithm dual variables tech rep royal holloway university london breiman friedman olshen stone classification regression trees monterey ca wadsworth brooks cole hastie tibshirani discriminant analysis gaussian mixtures jrssb vol pp schslkopf support vector learning phd thesis oldenbourg verlag munich joachims making large scale svm learning practical advances kernel methods support vector learning schslkopf burges smola eds mit press flury first course multivariate statistics springer
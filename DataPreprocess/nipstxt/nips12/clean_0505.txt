abstract recent years bayesian networks become highly successful tool diagnosis analysis decision making real world domains present efficient algorithm learning bayes networks data approach constructs bayesian networks first identifying node markov blankets connecting nodes maximally consistent way contrast majority work typically uses hill climbing approaches may produce dense causally incorrect nets approach yields much compact causal networks heeding independencies data compact causal networks facilitate fast inference also easier understand prove mild assumptions approach requires time polynomial size data number nodes randomized variant also presented yields comparable results much higher speeds introduction great number scientific fields today benefit able automatically estimate probability certain quantities interest may difficult expensive observe directly example doctor may interested estimating probability heart disease indications high blood pressure directly measurable quantities computer vision system may benefit probability distribution buildings based indicators horizontal vertical straight lines probability densities proliferate sciences today advances estimation likely wide impact many different fields bayesian networks succinct efficient way represent joint probability distribution among set variables applied fields mentioned herskovits agosta besides ability density estimation semantics lend sometimes loosely referred causal discovery namely directional relationships among quantities involved widely accepted parsimonious representation bayesian net one closely represents causal independence relationships may exist reasons great interest automatically inducing structure bayesian nets automatically data preferably also preserving independence relationships process two research approaches emerged first employs independence properties underlying network produced data order discover parts structure approach mainly exemplified sgs pc algorithms spirtes well margaritis thrun figure left example markov blanket variable shown members blanket shown shaded right example reconstruction rectangular net branching factor algorithm presented paper using samples indicated dotted lines directionality errors restricted classes trees chow polytrees rebane second approach concerned data prediction disregarding independencies data typically identified greedy hill climbing best first beam search space legal structures employing scoring function form data likelihood sometimes penalized network complexity result local maximum score network structure representing data one popular techniques used today paper presents approach belongs first category addresses two main shortcomings prior work believe preventing use becoming widespread two disadvantages exponential execution times proneness errors dependence tests used former problem addressed paper two ways one identifying local neighborhood variable bayesian net preprocessing step order facilitate recovery local structure around variable polynomial time assumption bounded neighborhood size second randomized version goes one step employing user specified number randomized tests constant logarithmic order ascertain result high probability second disadvantage research approach namely proneness errors also addressed randomized version using multiple data sets available bayesian accumulation evidence grow shrink markov blanket algorithm concept markov blanket variable set variables central paper concept new example see pearl surprising however little attention attracted fundamental property bayesian net new paper introduction explicit use idea effectively limit unnecessary computation well simple algorithm compute definition markov blanket follows denoting set variables xs conditional dependence given set markov blanket set variables bl bl words bl completely shields variable variable notion minimal markov blanket called markov boundary also introduced pearl uniqueness shown certain conditions markov boundary unique certain pathological situations equality two variables following discussion assume conditions necessary existence uniqueness satisfied identify markov blanket markov boundary using notation blanket variable also illuminating mention bayesian net framework markov blanket node easily identifiable graph consists parents children parents children example markov blanket shown fig note nodes say dependent given bayesian network induction via local neighborhoods os growing phase shrinking phase figure basic markov blanket algorithm algorithm recovery markov blanket shown fig idea behind step simple long markov blanket property violated ie exists variable dependent add current set variables process however may variables added really outside blanket variables would rendered independent later point intervening nodes underlying bayesian net added observation necessitates step identifies removes variables algorithm efficient requiring conditional tests making running time idi ivi set examples detailed derivation bound well formal proof correctness see margaritis practice one may try minimize number tests step heuristically ordering variables loop step example ascending mutual information probability dependence computed using test see section grow shrink gs algorithm bayesian net induction recovery local structure around node greatly facilitated knowledge nodes markov blankets would normally daunting task employing dependence tests conditioned exponential number subsets large sets variables even though members may irrelevant focused markov blankets nodes involved making structure discovery much faster reliable present plain version gs algorithm utilizes blanket information inducing structure bayesian net later point paper present robust randomized version potential faster reliable well able operate anytime manner following represents direct neighbors compute markov blankets compute markov blanket compute graph structure determine direct neighbor dependent given smaller orient edges orient exists variable dependent given smaller remove cycles following exist cycles graph compute set edges part cycle remove edge part greatest number cycles put margaritis thrun reverse edges insert edge graph reversed propagate directions neither execute following rule longer applies exists directed path orient algorithm description step determines members blanket node actually direct neighbors parents children assuming without loss generality smaller set tests successful separating making independent algorithm determines direct connection would happen conditioning set includes parents common children interesting note motivation behind selecting smaller set condition stems computational efficiency reliability well conditioning set causes data set split partitions smaller conditioning sets cause data set split larger partitions make dependence tests reliable step exploits fact two variables common descendant become dependent conditioning set includes descendant since direct neighbors known step determine whether direct neighbor parent exists another node coincidentally also parent attempt separate conditioning subset blanket includes fails assuming smaller directionality indeed subset since conditioning permanent dependency path created would case child straightforward show algorithm requires rib conditional independence tests maxx ib assumption bounded constant algorithm number conditional independence tests worthwhile note time compute conditional independence test pass data set id vl analysis formal proof correctness algorithm presented margaritis discussion main advantage algorithm comes use markov blankets restrict size conditioning sets markov blankets may usually wrong side including many nodes represented disjunction tests values conditioning set data emphasizes importance direct neighbors step removes nodes incorrectly added markov blanket computation step admitting variables whose dependence shown high confidence large number different tests also possible edge direction wrongly determined step due nonrepresentative noisy data may lead directed cycles resulting graph therefore necessary remove cycles identifying minimum set edges need reversed cycles disappear problem closely related margaritis minimum feedback arc set problem concerned identifying minimum set edges need removed graph possibly contains directed cycles order cycles disappear unfortunately problem np complete generality jtinger introduce reasonable heuristic solution based number cycles edge part cycle involved edge directions determined last two steps example nodes single parent multi parent nodes called colliders whose parents directly connected apply step steps concerned already directed edges step attempts ameliorate orienting edges way introduce bayesian network induction via local neighborhoods cycle reverse direction necessarily obvious example direction produces cycle otherwise acyclic graph opposite direction also however case proof see margaritis algorithm similar sgs algorithm presented spirtes differs number ways main difference lies use markov blankets dramatically improve performance many cases bounded blanket size assumptions hold structure similar sgs stability frequently referred robustness following discussion arguments presented spirtes apply increased reliability stems use smaller conditioning sets leading greater number examples per test pc algorithm also spirtes differs gs algorithm involves linear probing separator set makes unnecessarily inefficient randomized version gs algorithm gs algorithm presented appropriate situations maximum markov blanket set variables small reasonable assume many real life problems high level variables involved may case problems bayesian image retrieval computer vision may employ finer representations cases variables used may depend direct manner many others example may choose use variables characterize local texture different parts image resolution mapping textures variables increasingly fine direct dependencies among variables may plentiful therefore maximum markov blanket size may significant another problem plagued independence test based algorithms bayesian net structure induction general decisions based single tests hard decisions making prone errors due noise data also applies gs algorithm would therefore advantageous employ multiple tests deciding direct neighbor direction edge randomized version gs algorithm addresses two problems tackled randomized testing bayesian evidence accumulation problem exponential running times maximum blanket size steps plain algorithm overcome replacing series tests whose number may specified user members conditioning set chosen randomly smallest blanket two variables test provides evidence direct connection two variables appropriately weighted probability circumstances causing event occur due fact connectedness conjunction elementary events version algorithm shown detail due space restrictions operation follows closely one plain gs version main difference lies usage bayesian updating posterior probability direct link dependence collider pair variables using conditional dependence tests take account independent evidence posterior probability pi link executing dependence tests dj pi di pi pi ldi pi di lti factor takes values interval interpreted un importance truth test di smaller use accumulated evidence guide decisions hypothesis feel confident besides able timely manner due user specified number tests also note approach also addresses robustness problem mentioned use multiple weighted tests leaving end hard decisions involve threshold ie comparing posterior probability threshold case margaritis thrun edge errors versus number samples plain bn randomized gsbn hill climbing score data likelihood hill climbing score bic kl divergence versus number samples plain gsi randomized gsbn hill climbing score data likelihood hill climbing score bic number samples direction errors versus number samples plain bn randomized gsbn hill climbing score data likelihood hill climbing score bic number samples number samples figure results rectangular net branching factor directions blanket size function number samples top kl divergence depicted plain gs randomized gs hill climbing algorithms bottom percentage edge direction errors shown note certain edge error rates hill climbing algorithm exceed results throughout algorithms presented paper employ standard chi square conditional dependence tests done also spirtes order compare histograms test gives us probability error assuming two variables dependent fact type ii error dependence test easily derive probability dependent implicit confidence threshold involved dependence test indicating certain wish correctness test without unduly rejecting dependent pairs something always possible reality due presence noise experiments used corresponds confidence test test effectiveness algorithms following procedure generate random rectangular net specified dimensions branching factor number examples drawn net using logic sampling used input algorithm test resulting nets compared original ones along dimensions kl divergence difference edges edge directionality kldivergence estimated using monte carlo procedure example reconstruction shown beginning paper fig fig shows kl divergence original reconstructed net well edge omissions false additions reversals function number samples used demonstrates two facts first typical kl divergence gs hill climbing algorithms low hill climbing slightly lower shows good performance applications prediction prime concern second number incorrect edges errors directionality edges present much higher hill climbing algorithm making unsuitable accurate bayesian net reconstruction fig shows effects increasing markov blanket increasing branching factor expected see dramatic exponential increase execution time plain bayesian network induction via local neighborhoods loo lo edge direction errors versus branching factor edge errors plain gsbn edge errors randomized gsbn direction errors plain gsbn direction errors randomized gsbn branching factor execution time versus branching factor plair gsbn branching factor figure results rectangular net samples generated used reconstruction versus increasing branching factor left errors slowly increasing expected comparable plain randomized versions gs algorithm right corresponding execution times shown gs algorithm though mild increase randomized version latter uses constant conditional tests per decision execution time increase attributed quadratic increase number decisions note error percentages plain randomized version remain relatively close number direction errors gs algorithm actually decreases due larger number parents node structures allows greater number opportunities recover directionality edge using increased number tests discussion paper presented efficient algorithm computing markov blanket node used two versions gs algorithm plain randomized exploiting properties markov blanket facilitate fast reconstruction local neighborhood around node assumptions bounded neighborhood size also presented randomized variant advantages faster execution speeds added reconstruction robustness due multiple tests bayesian accumulation evidence simulation results demonstrate reconstruction accuracy advantages algorithms presented hill climbing methods additional results also show randomized version dramatical execution speed benefit plain one cases assumption bounded neighborhood hold without significantly affecting reconstruction error rate
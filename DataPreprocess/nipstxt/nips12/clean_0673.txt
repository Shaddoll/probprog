abstract local belief propagation rules sort proposed pearl guaranteed converge correct posterior probabilities singly connected graphical models recently number researchers empirically demonstrated good performance oopy belief propagation using rules graphs loops perhaps dramatic instance near shannon limit performance turbo codes whose decoding algorithm equivalent loopy belief propagation except case graphs single loop little theoretical understanding performance oopy propagation analyze belief propagation networks arbitrary topologies nodes graph describe jointly gaussian random variables give analytical formula relating true posterior probabilities calculated using loopy propagation give sufficient conditions convergence show belief propagation converges gives correct posterior means graph topologies networks single loop related max product belief propagation algorithm finds maximum posterior probability estimate singly connected networks show even non gaussian probability distributions convergence points max product algorithm loopy networks maxima particular large local neighborhood posterior probability results help clarify empirical performance results motivate using powerful belief propagation algorithm broader class networks problems involving probabilistic belief propagation arise wide variety applications including error correcting codes speech recognition medical diagnosis graph singly connected exist local message passing schemes calculate posterior probability unobserved variable given observed variables pearl derived scheme singly connected bayesian networks showed belief propagation algorithm guaranteed converge correct posterior probabilities beliefs several groups recently reported excellent experimental results running algorithms weiss freeman equivalent pearl algorithm networks loops perhaps dramatic instance performance turbo code error correcting codes codes described exciting potentially important development coding theory many years recently shown utilize algorithm equivalent belief propagation network loops progress analysis loopy belief propagation made case networks single loop networks shown unless compatabilities deterministic oopy belief propagation converge difference loopy beliefs true beliefs related convergence rate messages faster convergence exact approximation hidden nodes binary loopy beliefs true beliefs maximized assignments although confidence assignment wrong loopy beliefs paper analyze belief propagation graphs arbitrary topology nodes describing jointly gaussian random variables give exact formula relating correct marginal posterior probabilities ones calculated using oopy belief propagation show belief propagation converges give correct posterior means graph topologies networks single loop show covariance estimates generally incorrect present relationship error covariance estimates convergence speed gaussian non gaussian variables show max product algorithm calculates map estimate singly connected networks converges points maxima particular large neighborhood posterior probability loopy networks analysis simplify notation assume graphical model preprocessed undirected graphical model pairwise potentials graphical model converted form running belief propagation pairwise graph equivalent running belief propagation original graph assume node zi local observation iteration belief propagation node zi sends message neighboring zj based messages received neighbors local observation pairwise potentials ij xi ii xi yi assume message passing occurs parallel idea behind analysis build unwrapped tree unwrapped tree graphical model belief propagation solving exactly one applies belief propagation rules loopy network constructed maintaining local neighborhood structure oopy network nodes replicated loops potentials observations replicated loopy graph figure shows unwrapped tree diamond shaped graph construction belief root node identical node zx loopy graph four iterations belief propagation node shaded observed node attached omitted clarity original network represents jointly gaussian variables unwrapped tree since tree belief propagation guaranteed give correct answer unwrapped graph thus use gaussian marginalization formulae calculate true mean variances original unwrapped networks way calculate accuracy belief propagation gaussian networks arbitrary topology assume joint mean zero means added later joint districorrectness belief propagation xl figure left markov network multiple loops right unwrapped network corresponding structure bution given ae vvv straightforward construct inverse covariance matrix joint gaussian describes given gaussian graphical model writing exponent joint completing square shows mean given observations given vvy covariance matrix given denote iy ith row lv marginal posterior variance zi given data iv use unwrapped quantities scan tree breadth first order denote vector values hidden nodes tree scanned simlarly denote observed nodes scanned order inverse covariance matrices since scanning breadth first order last nodes leaf nodes denote number leaf nodes nature unwrapping mean belief node zx iterations belief propagation number unwrappings similarly lv variance belief node iterations data replicated write oy replica ofyj otherwise since potentials zi yi replicated write vo since zi also replicated non leaf connectivity corresponding zi write ovzz zero last rows relationships loopy unwrapped inverse covariance matrices substituted loopy unwrapped versions equation one obtains following expression true iteration lve vector zero everywhere last components corresponding leaf nodes choice node root tree arbitrary applies nodes oopy network formula relates node network loops means calculated iteration belief propagation true posterior means similarly relationship loopy unwrapped inverse covariance matrices substituted loopy unwrapped definitions relate weiss freeman node oo figure conditional correlation root node nodes unwrapped tree fig eight iterations potentials chosen randomly nodes presented breadth first order last elements correlations root node leaf nodes show correlation goes zero belief propagation converges loopy means exact symbols plotted star denote correlations nodes correspond node loopy graph sum correlations gives correct variance node ca loopy propagation uses first correlation marginalized covariances calculated belief propagation true ones lvel lve el vector zero everywhere last components equal nodes unwrapped tree replicas ca except ea components zero figure shows iv diamond network fig generated random potential functions observations calculated conditional correlations unwrapped tree note conditional correlation decreases distance tree scanning breadth first order last components correspond leaf nodes number iterations loopy propagation increased size unwrapped tree increases conditional correlation leaf nodes root node decreases equations clear conditional correlation leaf nodes root nodes zero sufficiently large unwrappings belief propagation converges means exact variances may incorrect practice conditional correlations actually equal zero finite unwrapping give precise statement conditional correlation root node leaf nodes decreases rapidly enough belief propagation converges means exact variances may incorrect also show sufficient conditions potentials xi xj correlation decrease rapidly enough rate correlation decreases determined ratio diagonal diagonal components quadratic form defining potentials wrong variances term ox lye equation simply sum many components lv figure shows components correct variance sum components belief propagation variance approximates sum first dominant term whenever positive correlation root node replicas loopy variance strictly less true variance oopy estimate overconfident correctness belief propagation iterations figure graphical model simulation unobserved nodes untilled connected four nearest neighbors observation node filled error estimates loopy propagation successive relaxation sor function iteration note belief propagation converges much faster sor note conditional correlation decreases rapidly zero two things happen first convergence faster te approaches zero faster second approximation error variances smaller te smaller thus shown single loop case quick convergence correlated good approximation simulations ran belief propagation grid fig joint probability exp wij xi xj wii xi yi wij nodes zi zj neighbors otherwise wii randomly selected probability set observations yi chosen randomly problem corresponds approximation problem sparse data points visible found exact posterior solving equation also ran belief propagation found converged calculated means identical true means machine precision also predicted theory calculated variances small belief propagation estimate overconfident many applications solution equation matrix inversion intractable iterative methods used figure compares error means function iterations oopy propagation successive relaxation sor considered one best relaxation methods note essentially five iterations loopy propagation gives right answer sor requires many expected fast convergence approximation error variances quite small median error comparison true variances ranged mean also nodes approximation error worse indeed nodes converged slower weiss freeman discussion independently two groups recently analyzed special cases gaussian graphical models frey analyzed graphical model corresponding factor analysis gave conditions existence stable fixed point rusmevichientong van roy analyzed graphical model topology turbo decoding gaussian joint density specific graph gave sufficient conditions convergence showed means exact main interest gaussian case understand performance belief propagation general networks multiple loops struck similarity results gaussians arbitrary networks results single loops arbitrary distributions first single loop networks binary nodes loopy belief node true belief node maximized assignment confidence assignment incorrect gaussian networks multiple loops mean node correct confidence around mean may incorrect second singleloop gaussian networks fast belief propagation convergence correlates accurate beliefs third gaussians discrete valued single loop networks statistical dependence root leaf nodes governs convergence rate accuracy two models quite different mean field approximations exact gaussian mrfs work poorly sparsely connected discrete networks single loop results gaussian single loop cases lead us believe similar results may hold larger class networks analysis extended non gaussian distributions basic idea applies arbitrary graphs arbitrary potentials belief propagation performing exact inference tree local neighbor structure oopy graph however linear algebra used calculate exact expressions error belief propagation iteration holds gaussian variables used similar approach analyze related max product belief propagation algorithm arbitrary graphs arbitrary distributions discrete continuous valued nodes show max product algorithm converges max product assignment greater posterior probability assignment particular large region around assignment weaker condition global maximum much stronger simple local maximum posterior probability sum product max product belief propagation algorithms fast parallelizable due well known hardness probabilistic inference graphical models belief propagation obviously work arbitrary networks distributions nevertheless growing body empirical evidence shows success many networks loops results justify applying belief propagation certain networks multiple loops may enable fast approximate probabilistic inference range new applications
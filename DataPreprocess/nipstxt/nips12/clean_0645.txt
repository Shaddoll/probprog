abstract propose novel approach building finite memory predictive models similar spirit variable memory length markov models vlmms models constructed first transforming block structure training sequence spatial structure points unit hypercube longer common suffix shared two blocks closer lie point representations transformation embodies markov assumption blocks long common suffixes likely produce similar continuations finding set prediction contexts formulated resource allocation problem solved vector quantizing spatial block representation compare model classical variable memory length markov models three data sets different memory stochastic components models superior performance yet construction fully automatic shown problematic case vlmms introduction statistical modeling complex sequences prominent theme machine learning due wide variety applications see classical markov models mms finite order simple yet widely used models sequences generated stationary sources however mms become hard estimate due familiar explosive increase number free parameters increasing model order consequently low order mms considered practical applications time ago ron singer tishby introduced conference markovian model could least partially overcome curse dimensionality classical mms basic idea behind model simple instead fixed order mms consider variable memory length markov models vlmms deep memory really needed see also size vlmms usually controlled one two construction parameters unfortunately constructing series increasingly complex vlmms example enter model selection phase validation set varying construction parameters tiao dorffner troublesome task construction often work smoothly varying parameters large intervals parameter values yielding unchanged vlmms interleaved tiny parameter regions corresponding large spectrum vlmm sizes cases difficult fully automize vlmm construction overcome drawback suggest alternative predictive model similar spirit vlmms searching relevant prediction contexts reformulated resource allocation problem euclidean space solved vector quantization potentially prohibitively large set length blocks assigned much smaller set prediction contexts suffix basis end first transform set blocks appearing training sequence set points euclidean space points corresponding blocks sharing long common suffix mapped close vector quantization set partitions set blocks several classes dominated common suffixes quantization centers play role predictive contexts great advantage model vector quantization performed completely self organized basis compare model classical mms vlmms three data sets representing wide range grammatical statistical structure first train models feigenbaum binary sequence strict topological metric organization allowed subsequences highly specialized deep prediction contexts needed model sequence classical markov models cannot succeed full power admitting limited number variable length contexts exploited second data set consists quantized daily volatility changes dow jones industrial average djia predictive models used predict direction volatility move next day financial time series known highly stochastic relatively shallow memory structure case difficult beat low order classical mms one perform better mms developing deeper specialized contexts hand lead overfitting finally test model experiments ron singer tishby language data bible trained classical mms vlmm books bible except book genesis models evaluated bases negative log likelihood unseen text genesis compare likelihood results model mms vlmms predictive models consider sequences finite alphabet generated stationary sources set sequences exactly symbols denoted information source defined family consistent probability measures ws denotes empty string applications useful consider probability functions easy handle achieved example assuming finite source memory length formulating conditional measures ws using function blocks presumably small finite set prediction contexts slw slc markov models mms order blocks length predictive models fractal representations sequences suffix uv variable memory length markov models vlmms suffices blocks different lengths depending particular block strategies selecting representing prediction contexts prediction suffix trees probabilistic suffix automata see example vlmm construction controlled one several parameters regulating selection candidate contexts growing pruning decisions prediction context function markov models order interpreted natural homomorphism le corresponding equivalence relation blocks two blocks class share suffix length factor set consists blocks classical mms define equivalence suffix bases regardless suffix structure present training data idea keep markov motivated suffix strategy constructing time take account data suffix structure vector quantization set points euclidean space positions codebook vectors cvs cv representing subset points closer cv overall error substituting cvs points represent minimal words cvs tend represent points lying close euclidean metric order use vector quantization determining relevant predictive contexts need two things define suitable metric sequence space would correspond markov assumptions two sequences close share common suffix longer common suffix closer sequences define uniformly continuous map sequence metric space euclidean space sequences close sequence space share long common suffix mapped close euclidean space rigorously study class spatial representations symbolic structures specifically family distances two blocks ut ut vt vt expressed ui otherwise correspond markov assumption parameter influences rate forgetting past construct map sequence metric space euclidean space follows associate symbol amap kz ti ti operating unit dimensional hypercube dimension hypercube large enough symbol associated unique vertex log ti tj whenever map blocks vt unit hypercube rr vlv vl vl vl vt ovl ov ov tiao dorffner center hypercube uniformly continuous indeed whenever two sequences share common suffix length euclidean distance point representations less kq strictly speaking mathematically correct treatment uniform continuity would need consider infinite sequences finite blocks symbols would correspond cylinder sets see sake simplicity deal finite sequences classical markov models define prediction context function via equivalence blocks two blocks class images map represented codebook vector case set prediction contexts identified set codebook vectors bl bi refer predictive models context function predictionfractal machines pfms prediction probabilities determined slbi aea number blocks ua training sequence point allocated codebook vector bi experiments see eq experiments constructed pfms using contraction coefficient means vector quantization tool first data set feigenbaum sequence binary alphabet sequence well studied symbolic dynamics number interesting properties first topological structure sequence described using context sensitive tool restricted indexed context free grammar second block length distribution blocks either uniform two probability levels third block distributions organized self similar fashion see sequence specified subsequence composition rule ao al anan lan chose work feigenbaum sequence increasingly accurate modeling sequence finite memory models requires selective mechanism deep prediction contexts created large portion feigenbaum sequence trained series classical mms variable memory length mms vlmms prediction fractal machines pfms first symbols following symbols formed test set maximum memory length vlmms pfms set mentioned introduction constructing series increasingly complex vlmms varying construction parameters appeared troublesome task spent fair amount time finding critical parameter values model size changed contrast fully automatic construction pfms involved sliding window length training set window position mapping block appearing window point eq vector quantizing resulting set points codebook vectors quantization step computed predictive probabilities according eq predictive models fractal representations sequences table normalized negative log likelihoods nnl feigenbaum test set model contexts nnl captured block distribution pfm vlmm mm negative log likelihoods per symbol base logarithm always taken number symbols alphabet test set computed using fitted models exhibited steplike increasing tendency shown table also investigated ability models reproduce block distribution found training test sets done letting models generate sequences length equal length training sequence block length computing distance block distribution training model generated sequences block distributions test training sets virtually table show block lengths distance exceed small threshold set since experiment either distance less exceeded large amount explanation step like behavior log likelihood block modeling behavior vlmms pfms scope paper briefly mention however combining knowledge topological metric structures feigenbaum sequence careful analysis models one show inclusion prediction context leads abrupt improvement modeling performance fact show vlmms pfms constitute increasingly better approximations infinite self similar feigenbaum machine known symbolic dynamics classical mm totally fails experiment since context length far small enable mm mimic complicated subsequence structure feigenbaum sequence pfms vlmms quickly learn explore limited number deep prediction contexts perform comparatively well second experiment time series daily values dow jones industrial average djia feb april transformed time series returns rt log zt log zt divided partially overlapping epochs containing values spanning approximately years consider squared return rt volatility estimate day volatility change forecasts volatility going increase decrease based historical returns interpreted buying selling signal straddle see volatility decreases go short straddle sold increases take long position straddle bought respect quality volatility model measured percentage correctly predicted directions daily volatility differences tiao dorffner table prediction performance djia volatility series percent correct test set model pfm vlmm mm pfm vlmm mm series rt rt differences successive squared returns transformed sequence dt symbols quantizing series rt rt follows extreme normal normal extreme rt rt rt rt rt rt rt parameters correspond percent percent sample quantiles respectively upper lower daily volatility increases decreases sample considered extremal lower upper daily volatility increases decreases viewed normal epoch partitioned training validation test parts containing symbols respectively maximum memory length vlmms pfms set two weeks trained classical mms vlmms pfms various numbers prediction contexts extremal event quantiles model class model size quantile used test set selected according validation set performance performance models quantified percentage correct guesses volatility change direction next day next symbol sum conditional next symbol probabilities given model greater model guess considered correct results shown table paired test reveals pfms significantly outperform vlmms classical mms course fixed order mms special cases vlmms theoretically vlmms cannot perform worse mms present separate results mms vlmms illustrate practical problems fitting vlmms besides familiar problems setting construction parameter values one parameter schemes like presented used operate small subsets potential vlmms data sets rather shallow memory structure negative effect third experiment extends work ron singer tishby tested classical mms vlmms bible alphabet english letters blank character symbols training set consisted bible except book genesis test set portion characters book genesis set maximal memory depth constructed vlmm contexts summarizing results classical mms order achieved negative log likelihoods per predictive models fractal representations sequences character nnl respectively authors point huge difference number states mms order vlmm performed much better achieved nnl experiments set maximal memory length maximal memory length used vlmm construction pfms constructed vector quantizing dimensional alphabet symbols spatial representation blocks appearing training set test set pfms predictive contexts achieved nnl conclusion presented novel approach building finite memory predictive models similar spirit variable memory length markov models vlmms constructing series vlmms often troublesome highly time consuming task requiring lot interactive steps predictive models prediction fractal machines pfms constructed completely automatic intuitive way number codebook vectors vector quantization pfm construction step corresponds number predictive contexts tested model three data sets different memory stochastic components vlmms excel classical mms feigenbaum sequence requiring deep prediction contexts sequence pfms achieved performance rivals vlmms financial time series pfms significantly outperform purely symbolic markov models mms vlmms natural language bible data pfm outperforms vlmm comparable size acknowledgments work supported austrian science fund fwf within research project adaptive information systems modeling economics management science sfb slovak academy sciences grant sav austrian research institute artificial intelligence supported austrian federal ministry science transport
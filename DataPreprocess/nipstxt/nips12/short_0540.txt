abstract local linear regression performs well many low dimensional forecasting problems high dimensional spaces performance typically decays due well known curse dimensionality possible way approach problem varying shape weighting kernel work suggest new data driven method estimating optimal kernel shape experiments using artificially generated data set data uc irvine repository show benefits kernel shaping introduction local linear regression attracted considerable attention statistical machine learning literature flexible tool nonparametric regression analysis cle fg ams like statistical smoothing approaches local modeling suffers called curse dimensionality well known fact proportion training data lie fixed radius neighborhood point decreases zero exponential rate increasing dimension input space due problem bandwidth weighting kernel must chosen big contain reasonable sample fraction result estimates produced typically highly biased one possible way reduce bias local linear estimates vary shape weighting kernel work suggest method estimating optimal kernel shape using training data purpose parameterize kernel terms suitable shape matrix minimize mean squared forecasting error respect approach meaningful size weighting kernel must constrained minimization avoid overfitting propose new entropy based measure kernel size constraint analogy nearest neighbor approach bandwidth selection fg suggested measure adaptive regard local data density addition leads efficient gradient descent algorithm computation optimal kernel shape experiments using artificially generated data set data uc irvine repository show kernel shaping improve performance local linear estimates substantially remainder work organized follows section briefly review optimal kernel shapes local linear regression local linear models introduce notation section formulate objective function kernel shaping section discuss entropic neighborhoods section describes experimental results section presents conclusions local linear models consider nonlinear regression problem continuous response jr predicted based dimensional predictor jra let xt yt denote set training data estimate conditional expectation xo xo consider local linear expansion ao xo neighborhood xo detail minimize weighted least squares criterion xo determine estimates parameters xt xo non negative weighting kernel assigns weight residuals neighborhood xo residuals distant xo multivariate problems standard way defining xt xo applying univariate noranegative mother kernel distance measure xt xol xt xo ft xt xo positive definite matrix determining relative importance assigned different directions input space example standard normal density xt xo normalized multivariate gaussian mean xo covariance matrix note xt xo normalized satisfy tt xt xo even though restriction relevant directly regard estimation needed discussion entropic neighborhoods section using shorthand notation xo solution minimization problem may written conveniently xtwx lxtwr design matrix rows vector response values diagonal matrix entries wt xt xo resulting local linear fit xo using inverse covariance matrix simply xo obviously xo depends definition weighting kernel discussion focus choices lead favorable estimates unknown function value xo kernel shaping local linear estimates resulting different choices vary considerably practice common strategy choose proportional inverse sample covariance matrix remaining problem finding optimal scaling factor equivalent problem bandwidth selection univariate smoothing fg bbb example bandwidth frequently chosen function distance xo kth nearest neighbor practical applications fg paper take different viewpoint argue optimizing shape ormoneit hastie weighting kernel least important optimizing bandwidth specifically fixed volume weighting kernel bias estimate reduced drastically shrinking kernel directions large nonlinear variation stretching directions small nonlinear variation idea illustrated using example shown figure plotted function sigmoidal along index vector constant directions orthogonal therefore shaped weighting kernel shrunk direction stretched orthogonally minimizing exposure kernel nonlinear variation figure left example single index model form tanh right contours straight lines orthogonal distinguish formally metric bandwidth weighting kernel rewrite follows corresponds inverse bandwidth may interpreted metricor shape matrix suggest algorithm designed minimize bias respect kernel metric clearly approach meaningful need restrict volume weighting kernel otherwise bias estimate could minimized trivially choosing zero bandwidth example might define contingent satisfy constant serious disadvantage idea contrast nearest neighbor approach ft independent design appropriate alternative define terms measure number neighboring observations detail fix volume xt xo terms entropy weighting kernel choose satisfy resulting entropy constraint given definition bandwidth determine metric xt xo minimizing mean squared prediction error yt xt respect way obtain approximation optimal kernel shape expectation differs bias variance term independent details entropic neighborhood criterion numerical minimization procedure described next entropic neighborhoods mentioned previously given shape matrix choose bandwidth parameter fulfill volume constraint weighting kernel purpose interpret kernel weights xt xo probabilities particular optimal kernel shapes local linear regression xt xo xt xo definition formulate local entropy xt xt xo log xt xo entropy probability distribution typically thought measure uncertainty context weighting kernel xt xo used smooth measure size neighborhood used averaging see note extreme case equal weights placed observations entropy maximized extreme single nearest neighbor xo assigned entire weight one entropy attains minimum value zero thus fixing entropy constant value similar fixing number nearest neighbor approach besides justifying correspondence also used derive intuitive volume parameter entropy level specify terms hypothetical weighting kernel places equal weight nearest neighbors zero weight remaining observations note entropy hypothetical kernel log thus natural characterize size entropic neighborhood terms determine numerically solving nonlinear equation system details see oh fi logk precisely report number neighbors terms equivalent sample fraction kit intuition idea illustrated figure using one two dimensional example equivalent sample fractions respectively note cases weighting kernel wider regions observations narrower regions many observations consequence number observations within contours equal weighting remains approximately constant across input space figure left univariate weighting kernel xo evaluated xo xo based sample data set observations indicated bars bottom right multivariate weighting kernel xo based sample data set observations two ellipsoids correspond contours weighting kernel evaluated summarize define value fixing equivalent sample fraction parameter subsequently minimize prediction error training set respect shape matrix note allow possibility may reduced rank means controlling number free parameters minimization procedure use variant gradient descent ormoneit hastie accounts entropy constraint particular algorithm relies fact differentiable respect due space limitations interested reader referred oh formal derivation involved gradients detailed description optimization procedure experiments section compare kernel shaping standard local linear regression using fixed spherical kernel two examples first evaluate performance using simple toy problem allows us estimate confidence intervals prediction accuracy using monte carlo simulation second investigate data set machine learning data base uc irvine bkm mexican hat function first example employ monte carlo simulation evaluate performance kernel shaping five dimensional regression problem purpose sets data points generated independently according model cos exp xl predictor variables drawn according five dimensionai standard normal distribution note even though regression carried five dimensional predictor space really function variables xl particular dimensions two five contribute information regard value kernel shaping effectively discard variables note also noise example figure left true mexican hat function middle local linear estimate using spherical kernel right local linear estimate using kernel shaping estimates based training set consisting data points figure shows plot true function spherical estimate estimate using kernel shaping functions true function familiar mexican hat shape recovered estimates different degrees evaluate local linear estimates values equivalent neighborhood fraction parameter range note warrant fair comparison used entropic neighborhood also determine bandwith spherical estimate value models estimated using artificially generated training sets subsequently performance evaluated training set test set grid points shown figure shape matrix maximal rank experiment results local linear regression using spherical kernel kernel shaping summarized table performance measured terms mean value models standard deviations reported parenthesis optima kerne shapes loca linear regression algorithm training test spherical kernel spherical kernel spherical kernel spherical kernel spherical kernel kernel shaping kernel shaping kernel shaping kernel shaping table performances toy problem results kernel shaping obtained using gradient descent steps step size results table indicate optimal performance test set obtained using parameter values kernel shaping spherical kernel given large difference values conclude kernel shaping clearly outperforms spherical kernel data set figure eigenvectors estimate obtained first training sets graphs ordered left right increasing eigenvalues decreasing extension kernel direction finally figure shows eigenvectors optimized first training sets eigenvectors arranged according size corresponding eigenvalues note two rightmost eigenvectors correspond directions minimum kernel extension span exactly space true function lives kernel stretched remaining directions effectively discarding nonlinear contributions abalone database task second example predict age abalone based several measurements specifically response variable obtained counting number rings shell time consuming procedure preferably age abalone could predicted alternative measurements may obtained easily data set eight candidate measurements including sex dimensions various weights reported along number rings abalone predictor variables normalize variables zero mean unit variance prior estimation overall data set consists observations prevent possible artifacts resulting order data records randomly draw observations training set use remaining observations test set results summarized table using various settings rank equivalent fraction parameter gradient descent step size optimal choice kernel shaping spherical kernel note performance improvement due kernel shaping negligible experiment ormoneit hastie kernel training test spherical kernel spherical kernel spherical kernel spherical kernel spherical kernel spherical kernel kernel shaping kernel shaping kernel shaping kernel shaping kernel shaping kernel shaping table results using abalone database gradient descent steps conclusions introduced data driven method improve performance local linear estimates high dimensions optimizing shape weighting kernel experiments found kernel shaping clearly outperformed local linear regression using spherical kernel five dimensional toy example led small performance improvement second real world example explain results second experiment note kernel shaping aims exploiting global structure data thus absence larger performance improvement may suggest simply corresponding structure prevails data set even though optimal kernel shapes exist locally may vary accross predictor space cannot approximated particular global shape preliminary experiments using localized variant kernel shaping lead significant performance improvements experiments acknowledgments work dirk ormoneit supported grant deutsche forschungsgemeinschaft dfg part post doctoral program trevor hastie partially supported nsf grant dms nih grant roi ca carrie grimes pointed us misleading formulations earlier drafts work references jams bbb bkm cle fg oh atkeson moore schaal locally weighted learning artificial intelligence review birattari bontempi bersini lazy learning meets recursive least squares algorithm kearns solla cohn editors advances neural information processing systems mit press blake koegh merz uci repository machine learning databases http www ics uci edu mlearn mlrepository html cleveland robust locally weighted regression smoothing scatterplots journal american statistical association fan gijbels local polynomial modelling applications chapman hall ormoneit hastie optimal kernel shapes local linear regression tech report department statistics stanford university
abstract present simple variation importance sampling explicitly searches important regions target distribution prove technique yields unbiased estimates show empirically reduce variance standard monte carlo estimators achieved concentrating samples significant regions sample space introduction well known general inference learning graphical models computationally hard therefore necessary consider restricted architectures approximate algorithms perform tasks among convenient successful techniques stochastic methods guaranteed converge correct solution limit large samples methods easily applied complex inference problems overwhelm deterministic approaches family stochastic inference methods grouped independent monte carlo methods importance sampling rejection sampling dependent markov chain monte carlo mcmc methods gibbs sampling metropolis sampling hybrid monte carlo goal methods simulate drawing random sample target distribution generally defined bayesian network graphical model difficult sample directly paper investigates simple modification importance sampling demonstrates advantages independent dependent markov chain methods idea explicitly search important regions target distribution sampling simpler proposal distribution mcmc methods metropolis hybrid monte carlo attempt something like biasing local random search towards higher probability regions preserving asymptotic fair sampling properties exploration investigate simple direct approach one draws points proposal distribution explicitly searches find points significant regions main challenge maintain correctness unbiasedness resulting procedure achieve independently sampling search subsequences weighting sample points expected weight proposal distribution matches true probability target greedy importance sampling importance sampling draw independently weight point zi zi random variable estimate ev indirect importance sampling draw independently weight point xi xi sp random variable estimate eio figure regular indirect importance sampling procedures generalized importance sampling many inference problems graphical models cast determining expected value random variable interest given observations drawn according target distribution interested computing expectation usually random variable simple like indicator event distribution generally form sample efficiently importance sampling useful technique estimating cases idea draw independent points zx simpler proposal distribution weight points obtain fair representation assuming efficiently evaluate point weighted sample used estimate desired expectations figure correctness unbiasedness procedure easy establish since expected weighted value ec ex ex xex ep technique implemented using indirect weights alternative estimator figure requires us compute fixed multiple preserves asymptotic cogectness ei zi zi zi converge ep respectively yields ep generally always possible apply extended approach drop importance stapling effective estimation technique approximates dom fails misses high probability regions systematically yields staples sm weights resulting estimator high iance staple almost always cont unrepresentative points sometimes dominated high weight points overcome problem critic obt data points important regions goal avoid generating systematically weight staples explicitly se ching significant regions get distribution nt unbiasedness resulting procedure develop series extensions importance stapling provably cogect first extension consider stapling blocks points instead individu points let bbeap titionofx finite blocksb esb finite note infinite block stapling procedure figure draws independent blocks points construct final staple weights points get probability divided total block probability discrete spaces easy verify procedure yields unbi ed estimates since eq er xj xj xex eb xj xj schuurrnans block importance sampling draw independently zi recover block bi zi zi create large sample blocks bl bn weight zi random variable estimate sliding window importance sampling draw independently zi recover block bi let zi get zi successors xi rn climbing ra steps get predecessors rn xi climbing ra steps weight zi create final sample successor points ci cl rn rn cn rn random variable estimate figure block sliding window importance sampling procedures crucially argument depend partition chosen fact could fix partition even one depended target distribution still obtain unbiased procedure long partition remains fixed intuitively works blocks drawn independently weighting scheme still produces fair representation note results presented paper extended continuous spaces mild technical restrictions however purposes clarity restrict technical presentation paper discrete case second extension allow countably infinite blocks discrete total order zi zi zi defined elements order could reflect relative probability zi zj consider arbitrary discrete order cope blocks unbounded length employ sliding window sampling procedure selects contiguous sub block size within larger selected block figure procedure builds independent subsample choosing random point zx proposal distribution determining containing block climbing steps obtain successors zx climbing steps obtain predecessors zo successor points including zx appear final sample predecessors used determine weights sample points weights determined target probability divided probability point appears random reconstruction yields unbiased estimator since ej zj zj ex zt middle line brews sum diqoint blocks reorders sum thin instead first choosing st point zt zt successors zt zt first choose successor point zj st points zj could led zj note derivation depend ticul block tition ticular discrete orderings long remen fixed means thin use titions orderings thin explicitly depend still obtain cogect procedure greedy importance sampling greedy importance sampling draw zl independently let xi xi compute successors xi xi xi taking ra size steps direction increase compute predecessors xi xi xi taking ra size steps direction decrease improper ascent descent occurs truncate paths shown upper right weight xi xi create final sample successor points igl xl rn ig ii rn ii rn random variable estimate ep collision merge figure greedy importance sampling procedure colliding merging paths greedy importance sampling dimensional case finally apply sliding window procedure conduct explicit search important regions well known optimal proposal distribution importance sampling xx minimizes variance apply sliding window procedure using order structure determined objective hope obtain reduced variance sampling independent blocks points block virtue constructed via explicit search likely contain least one two high weight points capturing moderate size sample independent high weight points intuitively expect outperform standard methods unlikely observe points chance experiments verify intuition figure main technical issue maintaining unbiasedness easy establish dimensional case simple setting greedy importance sampling procedure figure first draws initial point zx follows direction increasing taking fixed size steps either rn steps taken encounter critical point single block final sample comprised complete sequence captured one ascending search weight sample points account possible ways point could appear subsample entails climbing rn steps descent direction calculate denominators unbiasedness procedure follows directly previous section since greedy importance sampling equivalent sliding window importance sampling setting nontrivial issue maintain disjoint search paths note search path must terminate whenever steps point point lower value indicates collision occurred path must reach side critical point figure collision largest ascent point must allocated single path reasonable policy allocate path lowest weight penultimate point critical issue ensuring gets assigned single block ensuring critical point included one two distinct search paths practical estimator obtained exhibits bias figure test effectiveness greedy approach conducted several dimensional experiments varied relationship random variable figure schuurmans experiments greedy importance sampling strongly outperformed standard methods including regular importance sampling directly sampling target distribution rejection sampling metropolis sampling competitive results verify unbiasedness greedy procedure also show obtains significantly smaller variances across wide range conditions note greedy procedure actually uses rn rn points sampled block therefore effectively uses double sample however figure shows greedy approach often obtains variance reductions far greater corresponds standard deviation reduction multi dimensional case course technique worthwhile applied multi dimensional problems principle straightforward apply greedy procedure section multi dimensional sample spaces new issue discrete search paths possibly merge well collide see figure recall paths could merge previous case therefore instead decomposing domain collection disjoint search paths objective decomposes domain forest disjoint search trees however principle could used devise unbiased estimator case one could assign weight sample point target probability divided total probability subtree points lead fewer rn steps weighting scheme shown yield unbiased estimator however resulting procedure impractical dimensional sample space search tree typically branching factor yielding exponentially large trees avoiding need exhaustively examine trees critical issue applying greedy approach multi dimensional spaces simplest conceivable strategy ignore merge events surprisingly turns work reasonably well many circumstances note merges measure zero event many continuous domains cases one could hope ignore merges trust probability double counting points would remain near zero conducted simple experiments version greedy importance sampling procedure ignored merges procedure searched gradient ascent direction objective heuristically inverted search steps climbing gradient descent direction figures show despite heuristic nature procedure nevertheless demonstrates credible performance simple tasks first experiment simple demonstration task sample bivariate gaussian distribution two highly correlated random variables using weak proposal distribution standard normal depicted elliptical circular one standard deviation contours figure respectively greedy importance sampling performs well figure achieving unbiased estimates lower variance standard monte carlo estimators including common mcmc methods conduct significant study applied heuristic greedy method inference problem graphical models recovering hidden state sequence dynamic probabilistic model given sequence observations considered simple kalman filter model one state variable one observation variable per time step used conditional distributions xtixt zt zt xt zt initial distribution xx problem infer value final state variable zt given observations zx zt figure demonstrates greedy approach greedy importance sampling strong advantage standard importance sampling fact greedy approach applied condensation obtain improvements task space bounds preclude detailed discussion overall preliminary results show despite heuristic choices made section greedy strategy still performs well relative common monte carlo estimators terms bias variance least low moderate dimension problems however heuristic nature procedure makes extremely unsatisfying fact merge points easily make significant fraction finite domains turns rigorously unbiased feasible procedure obtained follows first take greedy fixed size steps axis parallel directions ensures steps inverted rather exhaustively explore entire predecessor tree calculate weights sample point use well known technique knuth sample single path root obtain unbiased estimate total probability tree procedure allows one formulate asymptotically unbiased estimator nevertheless feasible implement remains important future work investigate approach compare monte carlo estimation methods large dimensional problems particular hybrid monte carlo current results already suggest method could benefits
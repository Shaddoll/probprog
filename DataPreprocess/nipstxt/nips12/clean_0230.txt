abstract new functionals parameter model selection support vector machines introduced based concepts span support vectors rescaling feature space shown using functionals one predict best choice parameters model relative quality performance value parameter introduction support vector machines svms implement following idea map input vectors high dimensional feature space maximal margin hyperplane constructed shown training data separable error rate svms characterized radius smallest sphere containing training data margin distance hyperplane closest training vector feature space functional estimates vc dimension hyperplanes separating data given margin perform mapping calculate svm technique one uses positive definite kernel specifies inner product feature space example kernel radial basis function rbf iix kernel free parameter generally kernels require parameters set treating noisy data svms another parameter penalizing training errors also needs set problem choosing values parameters minimize expectation test error called model selection problem shown parameter kernel minimizes functional provides good choice model minimum functional coincides minimum test error however shapes curves different article introduce refined functionals specify best choice parameters parameter kernel parameter penalizing training error also produce curves better reflect actual error rate model selection support vector machines paper organized follows section describes basics svms section introduces new functional based concept span support vectors section considers idea rescaling data feature space section discusses experiments model selection functionals support vector learning introduce standard notation svms complete description see let xi yi set training examples xi belong class labeled yi decision function given svm coefficients obtained maximizing following functional ai aiajyiyjk xi xj constraints eaiyi ando ai constant controls tradeoff complexity decision function number training examples misclassified svm linear maximal margin classifiers high dimensional feature space data mapped non linear function xi xj xi xj points xi ai called support vectors distinguish ai ai call respectively support vectors first second category prediction using span support vectors results introduced section based leave one cross validation estimate procedure usually used estimate probability test error learning algorithm leave one procedure leave one procedure consists removing training data one element constructing decision rule basis remaining training data testing removed element fashion one tests elements training data using different decision rules let us denote number errors leave one procedure xl known leave one procedure gives almost unbiased estimate probability test error expectation test error machine trained examples equal expectation provide analysis number errors made leave one procedure purpose introduce new concept called span support vectors chapelle vapnik span support vectors since results presented section depend feature space consider without loss generality linear svms xi xi xj suppose solution optimization problem fixed support vector xp define set ap constrained linear combinations support vectors first category xi ap ixi ot yiypotp note hi less also define quantity call span support vector xp minimum distance xp set see figure xp ap min figure three support vectors al set ai semi opened dashed line shown set empty xp ap dsv dsv diameter smallest sphere containing support vectors intuitively smaller xp less likely leave one procedure make error vector xp formally following theorem holds theorem leave one procedure support vector xp corresponding recognized incorrectly following inequality holds ap max theorem implies separable case number errors made leave one procedure bounded follows xe ye maxv pd maxv vd av already ep improvement compared functional since dsv depending geometry support vectors value span much less diameter dsv support vectors even equal zero go assumption set support vectors change leave one procedure leads us following theorem model selection support vector machines theorem sets support vectors first second categories remain leave one procedure support vector xp following equality holds yp xp fp xp fo fp decision function given svm trained respectively whole training set point xp removed proof theorem follows one theorem assumption set support vectors change leave one procedure obviously satisfied cases nevertheless proportion points violate assumption usually small compared number support vectors case theorem provides good approximation result leave one procedure pointed experiments see section figure already noticed larger ap important decision function support vector xp thus surprising removing point xp causes change decision function proportional lagrange multiplier ap kind result theorem also derived svms without threshold following inequality derived yp xp fp xp vk xp xp span takes account geometry support vectors order get precise notion important given point previous theorem enables us compute number errors made leave oneout procedure corollary assumption theorem test error prediction given leave one procedure yl ye card ap ypf xp note points support vectors correctly classified leave one procedure therefore tt defines number errors leave one procedure entire training set assumption theorem box constraints definition ap removed moreover consider hyperplanes passing origin constraint hi also removed therefore assumptions computation span unconstrained minimization quadratic form done analytically support vectors first category leads closed form ksv pp ksv matrix dot products support vectors first category similar result also obtained section use span rule model selection separable nonseparable cases rescaling already mentioned functional bounds vc dimension linear margin classifier bound tight data almost fills surface sphere enclosing training data data lie flat ellipsoid bound poor since radius sphere takes account components largest deviations idea present make rescaling data feature space radius sphere stays constant margin increases apply bound rescaled data hyperplane chapelle vapnik let us first consider linear svms without mapping high dimensional space rescaling achieved computing covariance matrix data rescaling according eigenvalues suppose data centered let qol qo normalized eigenvectors covariance matrix data compute smallest enclosing box containing data centered origin whose edges parallels qol qo box approximation smallest enclosing ellipsoid length edge direction qo max ixi qo rescaling consists following diagonal transformation dx let us consider xi dw decision function changed transformation since xi data fill box side length thus functional replace since rescaled data box actually estimated radius enclosing ball using oo norm instead classical norm theoretical works needs done justify change norm non linear case note even map data high dimensional feature space lie linear subspace spanned data thus number training data large work subspace dimension purpose one use tools kernel pca matrix normalized eigenvectors gram matrix kij xi xj eigenvalues dot product xi qo replaced qo becomes aikyioti thus still achieve diagonal transformation finally functional becomes miax aiky experiments check new methods performed two series experiments one concerns choice width rbf kernel linearly separable database postal database dataset consists handwritten digit size test set examples following split training set subsets training examples task consists separating digit error bars figures standard deviations trials another experiment try choose optimal value noisy database breast cancer database dataset split randomly times training set containing examples test set containing examples section describes experiments model selection using span rule separable case non separable one section shows vc bounds model selection separable case without rescaling model selection using span rule section use prediction test error derived span rule model selection figure shows test error prediction given span different values width cr rbf kernel postal database figure plots functions different values breast cancer database see method predicts correct value minimum moreover prediction accurate curves almost identical available http horn first imd de raetsch data breast cancer model selection support vector machines test err span pred ct log sigma choice cr postal database ttt log st erro span pred ction choice breast cancer database figure test error prediction using span rule computation span rule involves computing span every support vector note however interested inequality vf xv rather exact value span thus minimizing sv xv av find point av xv vf xv ct stop minimization point correctly classified leave one procedure turned experiments time required compute span prohibitive since training time noteworthy extension application span concept denote one hyperparameter kernel derivative ok computable possible compute analytically derivative upper bound number errors made leave one procedure see theorem provides us powerful technique model selection indeed initial approach choose value width cr rbf kernel according minimum span rule case hyperparamter possible try different values several hyperparameters example one cr per component possible exhaustive search possible values hyperparameters nevertheless previous remark enables us find optimal value classical gradient descent approach preliminary results seem show using approach previously mentioned kernel improve test error significantely vc dimension rescaling section perform model selection postal database using functional rescaled version figure shows values classical bound different values bound predicts correct value minimum reflect actual test error easily understandable since large values data input space tend mapped flat ellipsoid feature space fact taken account figure shows performing rescaling data manage much tighter bound curve reflects actual test error given figure chapelle vapnik ooo vc dir ensi log sigma without rescaling dimension scalincj log sigma rescaling figure bound vc dimension different values postal database shape curve rescaling similar test error figure conclusion paper introduced two new techniques model selection svms one based span based rescaling data feature space demonstrated using techniques one predict optimal values parameters model evaluate relative performances different values parameters functionals also lead new learning techniques establish generalization ability due margin acknowledgments authors would like thank jason weston patrick haffner helpfull discussions comments
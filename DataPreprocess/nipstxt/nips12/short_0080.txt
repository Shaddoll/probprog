abstract attractor networks map input space discrete output space useful pattern completion however designing net given set attractors notoriously tricky training procedures cpu intensive often produce spurious attractors ill conditioned attractor basins difficulties occur connection network participates encoding multiple attractors describe alternative formulation attractor networks encoding knowledge local distributed although localist attractor networks similar dynamics distributed counterparts much easier work interpret propose statistical formulation localist attractor net dynamics yields convergence proof mathematical interpretation model parameters attractor networks map input space usually continuous sparse output space composed discrete set alternatives attractor networks long history neural network research attractor networks often used pattern completion involves filling missing noisy incorrect features input pattern initial state attractor net typically determined input pattern time state drawn one predefined set statesrathe attractors attractor net dynamics described state trajectory figure la attractor net generally implemented set visible units whose activity represents instantaneous state optionally set hidden units assist computation attractor dynamics arise interactions among units formulations attractor nets dynamics characterized gradient descent energy landscape allowing one partition output space attractor basins instead homogeneous attractor basins often desirable sculpt basins depend recent history network arrangement attractors space psychological models human cognition example priming fundamental model visits attractor faster fall attractor near future attractor basin broadened another property attractor nets key explaining behavioral data psychological neurobiological models gang effect strength attractor influenced attractors neighborhood figure lb illustrates gang effect proximity two rightmost attractors creates deeper attractor basin input starts origin get pulled right generaave model ttractor dynamics figure two dimensional space carved three regions dashed lines attractor net dynamics net cause input pattern mapped one attractors solid line shows temporal trajectory network state actual energy landscape localist attractor net function input fixed origin three attractors uniform prior shapes attractor basins influenced proximity attractors one another gang effect origin space depicted point equidistant attractor left attractor upper right yet origid dearly lies basin right attractors effect emergent property distribution attractors basis interesting dynamics produces mutuall reinforcing inhibitory influy ence similar items domains semantics memory olfaction training attractor net notoriously tricky training procedures cpu intensive often produce spurious attractors ill conditioned attractor basins indeed aware existing procedure robustly translate arbitrary specification attractor landscape set weights difficulties due fact connection partialpates specification multiple attractors thus knowledge net distributed connections describe alternative attractor network model knowledge localized hence name localist attractor network model many virtues including trivial procedure wiring architechn given attractor landscape eliminating spurious attractors achieving gang effects providing dear mathematical interpretation model parameters clarifies parameters control qualitative behavior model magnitude gang effects proofs convergence stability localist attractor net consists set state units attractor units parameters associated attractor unit encode location attractor denoted wi pull strength denoted rri influence shape attractor basin activity time qi reflects normalized distance attractor center current state weighted attractor strength qi ti wi rd wj exp ly wl thus attractors form layer normalized radial basis function units input net serves initial value state thereafter state pulled toward attractors proportion activity straightforward zemel mozer expression behavior qi wi first update fort generally however one might want gradually reduce time allowing persistent effect external input asymptotic state variables tr free parameters model derived formalism present localist attractor net motivated generative model input based attractor distribution network dynamics corresponds search maximum likelihood interpretation observation following section derive result present simulation studies architecture maximum likelihood formulation starting point statistical formulation localist attractor network mixture gaussians model standard mixture gaussians consists gaussian density functions dimensions gaussian parameterized mean covariance matrix mixture coefficient mixture model generative considered produced set observations observation generated selecting gaussian based mixture coefficients stochastically selecting point corresponding density function model parameters adjusted maximize likelihood set observations expectation maximization em algorithm provides efficient procedure estimating parameters expectation step calculates posterior probability qi gaussian observation maximization step calculates new parameters based previous values set qi mixture gaussians model provide interpretation localist attractor network unorthodox way gaussian corresponds attractor observation corresponds state however instead fixing observation adjusting gaussians fix gaussians adjust observation single observation gaussians uniform spread tr equation corresponds expectation step equation maximization step unusual mixture model unfortunately simple characterization localist attractor network produce desired behavior many situations produce partial solutions observation end attractor example two unidimensional gaussians overlap significantly likely value observation midway rather mean either gaussian therefore extend mixture gaussians formulation better characterize localist attractor network simple model attractors gaussian generator mean location dimensional state space input net considered generated stochastic selection one attractors followed addition zero mean gaussian noise variance specified attractor given particular observation attractor posterior probability normalized gaussian probability weighted mixing proportion posterior distribution attractors corresponds distribution state space weighted sum gaussians consider attractor network encoding distribution states implied attractor posterior probabilities one time however attractor network represent single position state space rather generative model ttractor dynamics entire distribution states restriction appropriate state dimensional point represented pattern activity state units accommodate restriction change standard mixture gaussians generative model interjecting intermediate level attractors observation first generative level consists discrete attractors second state space third observation observation considered generated moving hierarchy select attractor set attractors select state pattern activity across state units based preferred location attractor wi select observation yg observation produced particular state depends generative weight matrix networks consider observation state spaces identical identity matrix formulation allows lie space describe zero mean spherical gaussian noise introduced two levels deviations try try respectively comparison level gaussian mixture model described level model complicated standard observation preserved stable data rather model manipulating data viewed iteratively manipulating internal representation fits observation attractor structure attractor dynamics correspond iterative search state space find likely single state generated mixture gaussian attractors turn generated observation model one could fit observation finding posterior distribution hidden states given observation ylz ly trip yli fy ip dy conditional distributions gaussian yl wi try trz evaluating distribution equation tractable pal ition function sum set gaussian integrals due restriction network cannot represent entire distribution directly evaluate distribution instead adopt mean field approach approximate posterior another distribution based approximation network dynamics seen minimizing objective function describes upper bound negative log probability observation given model mean field parameters approach one choose form estimate posterior distribution better estimate allows network approach maximum likelihood solution select simple posterior qid qi responsibility assigned attractor estimate state accounts observation delta function motivated restriction explanation input consists single state given posterior distribution objective network minimize free energy described particular input example dy ln ln lnp lnp ri zemel mozer rci prior probability mixture coefficient associated attractor priors parameters generative model rv rz srl eqilnq srl qilsr wi nln rv rz given observation good set mean field parameters determined alternating updating generative parameters mean field parameters update procedure guaranteed converge mimum long updates done asynchronously update minimizes respect parameter update equations mean field parameters qi wjp lj simulations hold parameters generative model constant priors weights generative noise observation az aspect changes generative noise state single parameter shared attractors qil wi updates equations order typically initialize state time cyclically update qi generarive model avoids problem spurious attractors described standard gaussian mixture model intuition model avoids spurious attractors gained inspecting update equations equations effectively tie together two processes moving closer wi others increasing corresponding responsibility qi two processes evolve together act descrease noise ry accentuates pull attractor thus stable points correspond attractors rare simulation studies create attractor net specify parameters rri wi associated attractors based desired structure energy landscape figure lb remaining free parameter plays important role determining responsive system external input conducted several simulation studies explore properties localist attractor networks systematic investigations dimensional state space attractors randomly placed comers hypercube demonstrated spurious responses exceedingly rare unless input features distorted figure manipulating parameters noise prior probabilities predicted effects also conducted studies localist attractor networks domain visual images faces simulations shown gang effects arise structure among attractors example attractor set consists single view several different faces multiple views one face input morphed face linear combination one single view faces one view gang face end gang attractor even initial weighting assigned gang face less generative model attractor dynamics missing features missing features figure input must severely corrupted net makes spurious final state attractor adulterous final state neighbor generating attractor responses percentage spurious responses increases increased percentage adulterous responses increases rz decreased test architecture larger structured problem modeled domain three letter english words idea use attractor network content addressable memory might example queried retrieve word third position letter second position word hip attractors consist three letter english words ace zoo state space attractor network one dimension letters english alphabet positions total dimensions refer given dimension letter position encodes pa denotes dimension corresponding letter third position word attractors comers tm hypercube attractor word hip located state value dimensions except hi pa value external input specifies state constrains solution example one might specify third position setting external input dimension pa dimensions aa letters one might specify absence constraint particular letter position setting external input dimensions ap letters network task settle state corresponding one words given soft constraints letters interactive activation model word perception performs similar computation implementation exhibits key qualitative properties model external input specifies word course attractor net select word interesting queries external input underconstrains overconstrains solution illustrate one example network behavior external input specifies ga deg nonword attractor exists state closest attractors share two letters deg peg beg den dog figure shows effect gangs selection response beg conclusion localist attractor networks offer attractive altemative standard attractor networks dynamics easy control adapt described statistical formulation type localist attractor showed provides lyapunov function system well mathematical interpretation network parameters dynamics system derived intuitive arguments formal mathematical model simulation studies show architecture achieves gang effects spurious attractors rare approach ineffident attractors compositional structure many applications pattern recognition associative memory number items zemel mozer iteration figure simulation letter word attractor network queried deg frame shows relative activity attractor units various points processing activity frame normalized active unit printed black ink lighter ink color less active unit attractor units sharing least one letter deg shown selection beg product gang effect gangs example formed words sharing two letters common word beginnings pe instances di common word endings ag et common first last pairings one gangs supports two support three support ga hence beg selected stored small approach especially useful cases attractor locations known key focus network mutual influence attractors many cognitive modelling studies references becker moscovitch ivl behrmann joordens long term semantic primin computational account empirical evidence journal experimental psychology learning memory cognition golden il probabilistic characterization neural model computations anderson ed neural infvnnation processing systems pp american institute physics hopfield neural networks physical systems emergent collective computational abilities proc ings national academy sciences kay ivl lancaslea freeman eafference attractors olfactory system odor recognition int neural systems mathis computational theory consciousness cognition unpublished doctoral dissertation boulder co department computer science univemity colorado mathis mozer conscious unconscious perception computational theory cottrell ed procer dings eighteenth annual conference cognitive sc ence soc ety pp erlbaum mcclelland rumelhart interactive activation model context effects letter perception part account basic findings psychologicalreview neal il ivl hinton view em algorithm justifies incremental sparse variants ivl jordan ed lmrning graphicaimodels kluwer academic press mcrae de sa seldenberg ivl nature scope featural repreooentations word meaning journal experimental psychology general redish touretzky role hippocampus solving morris water maze neural computation rodrigues fontanari multivalley structure attractor neural networks journal physics mathematicaland general samsonovich mcnaughton pathintegration cognitive mapping continuous attractor neural network model journal neuroscience saul jaakkola jordan ivli mean field theory sigmoid belief networks journal ai research part ii neuroscience
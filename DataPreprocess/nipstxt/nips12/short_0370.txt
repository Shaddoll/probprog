abstract recently sample complexity bounds derived problems involving linear functions neural networks support vector machines paper extend theoretical results area deriving dimensional independent covering number bounds regularized linear functions certain regularization conditions show bounds lead class new methods training linear classifiers similar theoretical advantages support vector machine furthermore also present theoretical analysis new methods asymptotic statistical point view technique provides better description large sample behaviors algorithms
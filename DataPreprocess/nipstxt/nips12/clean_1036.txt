abstract problem developing good policies partially observable markov decision problems pomdps remains one challenging areas research stochastic planning one line research area involves use reinforcement learning belief states probability distributions underlying model states promising method small problems application limited intractability computing representing full belief state large problems recent work shows many settings maintain approximate belief state fairly close true belief state particular great success shown approximate belief states marginalize correlations state variables paper investigate two methods full belief state reinforcement learning one novel method reinforcement learning using factored approximate belief states compare performance algorithms several well known problem literature results demonstrate importance approximate belief state representations large problems introduction markov decision processes mdp framework good way mathematically formalizing large class sequential decision problems involving agent interacting environment generally mdp defined way agent complete knowledge underlying state environment formulation poses challenging research problems still optimistic modeling assumption rarely realized real world time agent must face uncertainty incompleteness information available extension formalism generalizes mdps deal uncertainty given partially observable markov decision processes pomdps focus paper solving pomdp means finding optimal behavior policy maps agent available knowledge environment belief state actions usually done function assigns values belief states fully observable mdp work presented paper done first author stanford university reinforcement learning using approximate belief states case value function computed efficiently reasonably sized domains situation somewhat different pomdps finding optimal policy pspacehard number underlying states date best known exact algorithms solve pomdps taxed problems dozen states several general approaches approximating pomdp value functions using reinforcement learning methods space permit full review approach upon focus use belief state probability distribution underlying model states contrast methods manipulate augmented state descriptions finite memory methods work directly observations main advantage probability distribution summarizes information necessary make optimal decisions main disadvantages model required compute belief state task representing updating belief states large problems difficult paper address problem obtaining model focus effective way using model even known model reinforcement learning techniques quite competitive exact methods solving pomdps hence focus extending model based reinforcement learning approach larger problems use approximate belief states risks approach inaccuracies introduced belief state approximation could give agent hopelessly inaccurate perception relationship environment recent work however presents approximate tracking approach provides theoretical guarantees result process cannot stray far exact belief state approach rather maintaining exact belief state infeasible realistically large problems maintain approximate belief state usually restricted class distributions approximate belief state updated due actions observations continuously projected back restricted class specifically use decomposed belief states certain correlations state variables ignored paper present empirical results comparing three approaches belief state reinforcement learning direct approach use neural network one input element full belief state second spova method uses function approximator designed pomdps third use neural network approximate belief state input present results several well known problems pomdp literature demonstrating belief state approximation ill suited problems effective means attacking large problems basic framework algorithms pomdp defined tuple three sets three functions set states set actions set observations transition function ii specifies actions affect state world viewed si sj si probability agent reaches state currently state si takes action reward function determines immediate reward received agent observation model ii determines agent perceives depending environment state action taken ola probability agent observes state taken action rodriguez parr andd koller pomdp belief states belief state defined probability distribution states represents probability environment state taking action observing belief state updated using bayes rule es zs sj size exact belief state equal number states model large problems maintaining manipulating exact belief state problematic even transition model compact representation example suppose state space described via set random variables xi takes values finite domain val xi particular defines value val xi variable xi full belief state representation exponential use approximation method analyzed boyen koller variables partitioned set disjoint clusters ce belief functions maintained variables cluster time step compute exact belief state compute individual belief functions marginalizing inter cluster correlations assignment ci variables ci obtain bi ci ey cl approximation original full belief state reconstructed bi ci representing belief state product marginal probabilities projecting belief state reduced space full belief state representation state variables would exponential size decomposed belief state representation exponential size largest cluster additive number clusters processes mix rapidly enough errors introduced approximation stay bounded time discussed boyen koller type decomposed belief state particularly suitable processes factored represented dynamic bayesian network cases avoid ever representing exponentially sized belief state however approach fully general applied setting state defined assignment values set state variables value functions policies pomdps one thinks pomdp mdp defined belief states well known fixed point equations mdps still hold specifically discount factor defined next belief state optimal policy determined maximizing action belief state principle could use learning value iteration directly solve pomdps main difficulty lies fact uncountably many belief states making tabular representation value function impossible exact methods pomdps use fact finite horizon value functions piecewiselinear convex ensuring finite representation finite representation grow exponentially horizon making exact approaches impractical settings function approximation attractive alternative exact methods implement function approximation using set parameterized functions reward go taking action belief state value function reconstructed functions max update rule transition reinforcement learning using approximate belief states state action reward xw vwa function approximation architectures consider two types function approximators first two layer feedforward neural network sigmoidal internal units linear outermost layer used one network function full belief state reinforcement learning used networks inputs one component belief state hidden nodes approximate belief state reinforcement learning used networks one input assignment variables cluster two clusters example binary variables networks would inputs kept number hidden nodes network square root number inputs second function approximator spova soft max function designed exploit piecewise linear structure pomdp value functions spova function maintains set weight vectors wai evaluated practice small value usually adopted start learning making function smooth increased learning spova closely approximates pwlc function usually maintained one spova function action assigned vectors function gave aii parameters spova full belief state neural network empirical results present results several problems pomdp literature present extension known machine repair problem designed highlight effects approximate belief states results presented form performance graphs value current policy obtained taking snapshot value function measuring discounted sum reward obtained resulting policy simulation use nn refer neural network trained reinforcement learner trained full belief state term decomposed nn refer neural network trained approximate belief decomposed product marginals used simple exploration strategy starting probability acting randomly decreased linearly due space limitations able describe model detail however used publicly available model description files table shows running times different methods generally much lower would required solve problems using exact methods grid worlds begin considering two grid worlds world state world world contains states natural decomposition state variables compared spova full belief state neural network see http www cs brown edu research ai pomdp index html note file format specifies starting distribution problem results reported respect starting distribution rodriguez parr koller figure grid world state maze experimental results averaged training runs simulations per policy snapshot presented figure show spova learns faster neural network network eventually catch state robot navigation problem amenable decomposed belief state approximation since underlying state space comes product robot positions robot orientations decomposed belief state two clusters one containing position state variable containing orientation state variable figure lb shows results spova dominates decomposed nn trouble problem effects position orientation value function easily decoupled effect orientation value highly state dependent meant decomposed nn forced learn much complicated function inputs function learned network using full belief state aircraft identification aircraft identification another problem studied cassandra thesis includes sensing actions identifying incoming aircraft actions attacking threatening aircraft attacks friendly aircraft penalized failures intercept hostile aircraft challenging problem tension deciding various sensors better sensors tend make base visible hostile aircraft stealthy sensors less accurate sensors give information aircraft type distance base state space problem comprised three main components aircraft type either aircraft friend foe dis tance far aircraft currently base discretized adjustable number distinct distances visibility measure visible base approaching aircraft discretized levels chose gaving problem states problem natural decomposition state variables aircraft type distance base visibility results three algorithms shown figure first problem start see advantage decomposing belief state decomposed nn used three separate clusters one variable meant network inputs simpler network learn faster learned better policy overall believe illustrates important point even though spova full belief state neural network may expressive decomposed nn decomposed nn able search space functions represent much efficiently due reduced number parameters reinforcement learning using approximate belief states nn figure aircraft identification machine maintenance machine maintenance last problem machine maintenance problem cassandra database problem assumes machine certain number components quality parts produced machine determined condition components component one four conditions good component good condition fair component amount wear would benefit maintenance bad part worn could use repairs broken part broken must replaced status components observable machine completely disassembled figure shows performance results problem component version problem states maximum size full belief state approach manageable however belief state problem decomposes naturally clusters describing status machine creating decomposed belief state four components graph shows dominance simple decomposition approach believe problem clearly demonstrates advantage belief state decomposition decomposed nn learns function inputs fraction time takes full net spova learn lower quality function inputs running times table shows running times different problems presented generally much less would required solve problems exactly full nn spova roughly comparable decomposed neural network considerably faster exploit problem structure approximate belief state computation time spent computing belief states actually larger decomposed nn savings comes reduction number parameters used reduced number partial derivatives computed expect savings significantly substantial processes represented factored way approximate belief state propagation algorithm also take advantage additional structure concluding remarks proposed new approach belief state reinforcement learning use approximate belief states using well known examples pomdp literature compared approximate belief state reinforcement learning two methods rodriguez parr andd koller problem spova nn decomposed nn hallway min min min aircraft id min min min machine min table run times seconds minutes hours different algorithms use exact belief states results demonstrate approximate belief states may ideal tightly coupled problem features position orientation robot natural effective means addressing large problems even medium sized problems showed approximate bejief state reinforcement learning outperform full belief state reinforcement learning using fewer trials much less cpu time many problems exact belief state methods simply impractical approximate belief states provide tractable alternative acknowledgements work supported aro muri program integrated approach intelligent systems onr contract darpa hpkb program generosity powell foundation sloan foundation
abstract previous biophysical modeling work showed nonlinear interactions among nearby synapses located active dendritic trees provide large boost memory capacity cell mel aim present work quantify boost estimating capacity neuron model passive dendritic integration inputs combined linearly across entire cell followed single global threshold active dendrite model threshold applied separately output branch branch subtotals combined linearly focus limiting case binary valued synaptic weights derive expressions measure model capacity estimating number distinct input output functions available neuron types show application fixed nonlinearity dendritic compartment substantially increases model flexibility neuron realistic size capacity nonlinear cell exceed sized linear cell order magnitude largest capacity boost occurs cells relatively large number dendritic subunits relatively small size validated analysis empirically measuring memory capacity randomized two class classification problems stochastic delta rule used train linear nonlinear models found large capacity boosts predicted nonlinear dendritic model readily achieved practice http lnc usc edu poirazi mel introduction physiological evidence connectionist theory support notion brain memories stored pattern learned synaptic weight values experiments variety neuronal preparations however ind icate efficacy synaptic transmission undergo substantial fluctuations brief trains synaptic stimuli large fluctuations synaptic efficacy short time scales seem inconsistent conventional connectionist assumption stable high resolution synaptic weight values furthermore recent experimental study suggests excitatory synapses hippocampus region implicated certain forms explicit memory may exist long term stable states continuous grading synaptic strength seen standard measures long term potentiation ltp may exist average large population two state synapses randomly staggered thresholds learning petersen malenka nicoll hopfield according conventional connectionist notions possibility individual synapses hold one two bits long term state information would seem serious implications storage capacity neural tissue exploration question one main themes paper related vein found previous biophysical modeling studies nonlinear interactions synapses co activated branch active dendritic tree could provide alternative form long term storage capacity capacity largely orthogonal tied conventional synaptic weights contained instead spatial permutation synaptic connections onto dendritic tree could principle modified course learning development mel abstract setting recently showed large repository model flexibility lies choice large number possible interaction terms available high dimension actually included learning machine discriminant function excess capacity contained choice flexibility quantified using straightforward counting arguments poirazi mel two alternative models dendritic integration paper use similar function counting approach address biologically relevant case neuron multiple quasi independent dendritic compartments fig primary objective compare memory capacity cell assuming two different modes dendritic integration according linear model neuron activation level prior thresholding given weighted sum inputs cell whole according nonlinear model synaptic inputs branch first combined linearly static sigmoidal nonlinearity applied rn branch subtotals resulting branch outputs summed produce cell overall activity ei ej expressions written similar form emphasize models identical number synaptic weights differing presence absence fixed nonlinear function applied branch subtotals though individual synaptic weights models constrained value input lines may form multiple connections different memory capacity linear vs nonlinear models dendritic integration figure cell modeled set identical branches connected soma branch contains synaptic contacts driven one distinct input lines branches means representing graded synaptic strengths similarly input line forms connection implicit weight light restriction positive zero weight values linear nonlinear models split two opponent channels dedicated positive vs negative coefficients respectively leads final output model yl sgn yn sgn sgn operator maps total activation level class label following derive expressions number distinct parameter states available linear rs nonlinear models measure found reliable predictor storage capacity certain restrictions poirazi mel based expressions compute capacity boost provided branch nonlinearity function number branches synaptic sites per branch input space dimensionality finally test predictions analytical model training linear nonlinear models randomized classification problems using stochastic delta rule empirically measure compare storage capacities two models results counting parameter states linear vs nonlinear model derived expressions bn estimate total number parameter bits available linear vs nonlinear models respectively bn log log expressions estimate number non redundant states neuron type assignments input lines dendritic sites yield distinct poirazi mel input output functions yl yn formulae plotted figure curve represents cell fixed number branches indicated case capacity increases steadily number synapses per branch increased logarithmic growth capacity linear model evident asymptotic analysis expression shown bottom graph circles may seen boost capacity provided dendritic branch nonlinearity increases steadily number synaptic sites cell branches containing synaptic sites capacity boost relative linear model exceeds factor figure shows given total number synaptic sites case capacity nonlinear cell maximized specific choice peak three curves computed different values occurs cell containing branches synapses however capacity moderately sensitive branch count capacity cell branches synapses example lies within factor two optimal configuration linear cell capacities found far right edge plot since nonlinear model one synapse per branch number trainable states identical linear model validating analytical model test predictions analytical model trained linear nonlinear cells randomized two class classification problems training samples drawn dimensional spherical gaussian distribution randomly assigned positive negative labels runs training patterns evenly divided positive negative labels similar results original input dimensions recoded using set dimensional binary nonoverlapping receptive fields centers spaced along dimension receptive fields would activated equally often manipulation mapped original dimensional learning problem dimensions thereby increasing discriminability training samples relative memory capacity linear vs nonlinear cells determined empirically comparing number training patterns learnable fixed error rate learning rule used cell types similar clusteron learning rule described mel involved two mechanisms known contribute neural development random activity independent synapse formation activity dependent synapse stabilization iteration set synapses chosen random worst synapse identified based correlation training set input pre synaptic activity ii post synaptic activity local nonlinear branch response nonlinear energy model constant linear model iii global delta signal value cell responded correctly input pattern cell responded incorrectly poorest performing synapse branch targeted replacement new synapse drawn random input lines probability replacement actually occurred given boltzmann equation based difference training set error rates replacement temperature variable gradually lowered course simulation terminated improvement error rates seen results learning runs shown fig analytical capacity measured bits scaled numerical capacity measured training patterns memory capacity linear vs nonlinear models dendritic integration capacity linear vs nonlinear model various geometries nonlinear model linear mod el total synaptic sites capacity linear vs nonlinear model different input space dimensions ta oo nonlinear model number branches figure comparison linear vs nonlinear model capacity function branch geometry capacity bits linear several nonlinear cells different branch counts curve indexed branch count sites per branch increases left right indicated iconically beneath axis cells capacity increases increasing number sites though capacity linear model grows logarithmically leading increasingly large capacity boost size matched nonlinear cells capacity nonlinear model sites different values input space dimension branch count grows along axis cells right edge plot contain one synapse per branch thus number modifiable parameters hence capacity equivalent linear model three curves show exist optimal geometry maximizes capacity nonlinear model case branches synapses learned error two key features theoretical curves dashed lines echoed empirical performance curves solid lines including much larger storage capacity nonlinear cell model specific cell geometry maximizes capacity boost discussion found using analytical numerical methods limit lowresolution synaptic weights application fixed output nonlinearity compartment dendritic tree leads significant boost capacity relative cell whose post synaptic integration linear example given cell synaptic contacts originating distinct input lines analysis predicts fold increase capacity nonlinear cell numerical simulations using stochastic delta rule actually achieve fold boost given linear nonlinear model identical number synaptic contacts uniform synaptic weight values accounts capacity boost principal insight gained work attachment fixed nonlinearity branch neuron substantially increases underlying model poirazi ke mel analytical bits numerical training patterns nonlinear model figure comparison capacity boost predicted analysis vs observed empirically linear nonlinear models trained using stochastic delta rule dashed lines analytical curves linear vs nonlinear model cell sites show capacity varying cell geometries solid lines empirical performance two cells error cri terion using subunit nonlin earity similar sults seen using sigmoidal nonlinearity though param eters optimal sigmoid depended cell geometry linear model analytical numeri cal curves peak capacity seen cel branches synapses per branch capacity number branches exceeds sized linear model factor rn peak factor cells ranging synapses per branch horizontal dotted line flexibility confers upon cell much larger choice distinct input output relations select learning may illustrated follows linear model branching structure irrelevant depends number input connections formed input lines spatial permutations set input connections thus interchangeable produce identical cell responses massive redundancy confines capacity linear model grow logarithmically increasing number synaptic sites fig unfortunate limitation brain formation large numbers synaptic contacts neurons routine contrast model nonlinear subunits contains many fewer redundancies spatial permutations set input connections lead non identical values yn since input swapped branch bl branch leads elimination interaction terms involving branch bl creation new interaction terms branch interestingly particular form branch nonlinearity virtually effect capacity cell far counting arguments concerned though profound effect cell representational bias see since principal effect nonlinearity capacity calculations break symmetry among different branches issue representational bias critical one however must considered attempting predict absolute relative performance rates particular classifiers confronted specific learning problems thus intrinsic differences geometry linear vs nonlinear discriminant functions mean parammemory capacity linear vs nonlinear models dendritic integration eters available two models may better worse suited solve given learning problem even two models equated total parameter flexibility biases taken account analysis could nonetheless substantial effect measured error rates could thus throw performance advantage one machine one danger performance differences measured empirically could misinterpreted arising differences underlying model capacity fact arise differential suitability two classifiers learning problem hand avoid difficulty random classification problems used empirically assess memory capacity chosen level playing field linear vs nonlinear cells since previous study found coefficients linear vs nonlinear quadratic terms equally efficient features task way differences measured performance tasks primarily attributable underlying capacity differences rather differences representational bias experimental control permitted meaningful comparisons analytical empirical tests fig problem representational bias crops second guise wherein analytical expressions capacity eq significantly overestimate actual performance cell occurs particular ensemble learning problems fails utilize entropy available cell parameter space example requiring cell visit small subset parameter states relatively often invalidates maximum parameter entropy assumption made derivation eq measured performance tend fall predicted values actual performance either model confronted ensemble learning problems thus determined number trainable parameters available neuron measured eq suitability neuron parameters solving assigned learning problems utilization parameters relates entropy joint probability parameter values averaged ensemble learning problems comparisons linear nonlinear cells calculated attempted control conclusion results build upon results earlier biophysical simulations indicate limit large number low resolution synaptic weights nonlinear dendritic processing could nonetheless major impact storage capacity neural tissue references mel clusteron toward simple abstraction complex neuron moody hanson lippmann eds advances neural information processing systems vol pp morgan kaufmann san mateo ca mel nmda based pattern discrimination modeled cortical neuron neural comp petersen malenka nicoll hopfield ornone potentiation ca ca synapses proc natl acad sci usa poirazi mel choice value flexibility jointly contribute capacity subsampled quadratic classifier neural comp press
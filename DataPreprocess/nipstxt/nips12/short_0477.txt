abstract invariance topographic transformations translation shearing image successfully incorporated feedforward mechanisms convolutional neural networks tangent propagation describe way add transformation invariance generafive density model approximating nonlinear transformation manifold discrete set transformations em algorithm original model extended new model computing expectations set transformations show add discrete transformation variable gaussian mixture modeling factor analysis mixtures factor analysis give results filtering microscopy images face facial pose clustering handwritten digit modeling recognition introduction imagine happens point dimensional space corresponding pixel image object object deformed shearing small amount shearing move point slightly deforming object shearing trace continuous curve space pixel intensities illustrated fig la extensive levels shearing produce highly nonlinear curve consider shearing thin vertical line although curve approximated straight line locally linear approximations transformation manifold used significantly improve performance feedforward discriminative classifiers nearest neighbors simard et multilayer perceptrons simard et linear generarive models factor analysis mixtures factor analysis also modified using linear approximations transformation manifold build degree transformation invariance hinton et al general linear approximation accurate transformations couple neighboring pixels inaccurate transformations couple nonneighboring pixels applications handwritten digit recognition input blurred linear approximation becomes robust significant levels transformation nonlinear manifold better modeled using discrete approximation example curve fig la jojic fret figure pixel greyscale image represented point untilled disc ndimensional space object imaged deformed shearing point moves along continuous curve locally curve linear high levels shearing produce highly nonlinear curve approximate discrete points filled discs indexed graphical model showing discrete transformation variable added density model latent image model observed image gaussian pdf captures th transformation plus small amount pixel noise use box represent variables gaussian conditional pdfs explored transformed mixtures gaussians discrete cluster index transformed component analysis tca vector gaussian factors may model locally linear transformation perturbations mixtures transformed component analyzers transformed mixtures factor analyzers represented set points filled discs approach discrete set possible transformations specified beforehand parameters learned model invariant set transformations approach used design convolutional neural networks invariant translation le cun et al develop general purpose learning algorithm generarive topographic maps bishop et al describe invariance discrete set known transformations like translation built generafive density model show em algorithm original density model extended new model computing expectations set transformations give results different types experiment involving translation shearing transformation discrete latent variable represent transformation sparse transformation generating matrix gt operates vector pixel intensities example integer pixel translations image represented permutation matrices although types transformation matrix may accurately represented permutation matrices many useful types transformation represented sparse transformation matrices example rotation blurring represented matrices small number nonzero elements per row rotations observed image linked nontransformed latent image transformation index follows xlt af gez diagonal matrix pixel noise variances since probability transformation may depend latent image joint distribution latent image transformation index observed image af gez lz corresponding graphical model shown fig lb example model noisy transformed images one shape choose gaussian distribution topographic transformation discrete latent variable transformed mixtures gaussians tmg fig lc shows graphical model tmg different clusters may different transformation probabilities cluster mixing proportion mean diagonal covariance matrix joint distribution gtz ptc rc probability transformation cluster ptc marginalizing latent image gives cluster transformation conditional likelihood xl jv gt gt cgt used compute cluster transformation responsibility clx likelihood looks like likelihood mixture factor analyzers ghahramani hinton however whereas likelihood computation latent pixels takes order time mixture factor analyzers takes linear time order tmg gt sparse transformed component analysis tca fig ld shows graphical model tca transformed factor analysis latent image modeled using linearly combined gaussian factors joint distribution gtz ar ay ar pt mean latent image matrix latent image components factor loading matrix diagonal noise covariance matrix latent image marginalizing factors latent image gives transformation conditional likelihood xl af ge ge aa used compute transformation responsibility lx aa sparse computing likelihood exactly takes time however likelihood computed linear time assume igi aa igt aat ti corresponds assuming observed noise smaller variation due latent image observed noise accounted latent noise model experiments approximation lead degenerate behavior produced useful models setting columns equal derivatives respect continuous transformation parameters tca accommodate local linear approximation discrete approximation transformation manifold mixtures transformed component analyzers mtca combination tmg tca used jointly model clusters linear components transformations alternatively mixture ganssians invariant discrete set transformations locally linear transformations obtained combining tmg tca whose components set equal transformation derivatives joint distribution combined model fig le gtz acy ptc rc cluster transformation likelihood gi gi aca approximated linear time tca jojic frey mixed transformed component analysis mtca present em algorithm mtca em algorithms tmg tca emerge setting number factors setting number clusters let represent parameter generarive model data derivative log likelihood training set xl xt respect written ologp xl xt ologp xt lxt expectation taken ylxt em algorithm iteratively solves new set parameters using old parameters compute expectations procedure consistently increases likelihood training data setting solving new parameter values obtain update equations based expectations given appendix notation sufficient statistic computed averaging training set aiag gives vector containing diagonal elements matrix diag gives diagonal matrix whose diagonal contains elements vector gives element wise product vectors denoting updated parameters clx tlx clx acylx clx diag clxt acy acy ixt ctx diag xt gtz xt gtz ixt clxt ytlxt clxt yyttxt reduce number parameters sometimes assume ptc depend even ptc held constant uniform distribution experiments filtering images scanning electron microscope sem sem images fig low signal noise ratio due high variance electron emission rate modulation variance imaged material golem cohen reduce noise multiple images usually averaged pixel variances used estimate certainty rendered structures fig shows estimated means variances pixels sem images like ones fig fact averaging images take account spatial uncertainties filtering imaging process introduced electron detectors high speed electrical circuits trained single cluster tmg horizontal shifts vertical shifts sem images using iterations em keep number parameters almost equal number parameters estimated using simple averaging transformation probabilities learned pixel variances observed image set equal step tmg parameter fig shows mean variance learned tmg compared simple averaging tmg finds sharper detailed structure variances significantly lower indicating tmg produces confident estimate image topographic transformation discrete latent variable figure pixel sem images mean variance image pixels mean variance found tmg reveal structure less uncertainty figure frontal face images two people cluster means learned tmg mixture gaussians images one person different poses cluster means learned tmg less detailed cluster means learned mixture gaussians mean first principal components data mostly model lighting translation clustering faces poses fig shows examples training set jerky images two people walking across cluttered background trained tmg clusters horizontal shifts vertical shifts using iterations em initializing weights small random values loop rich matlab script executed minutes mhz pentium processor fig shows cluster means include two sharp representations person face background clutter suppressed fig shows much blurrier means mixture gaussians trained using iterations em fig shows examples training set jerky images one person different poses trained tmg clusters horizontal shifts vertical shifts using iterations em fig shows cluster means capture poses mostly suppress background clutter mean cluster includes part background cluster also low mixing proportion traditional mixture gaussians trained using iterations em finds blurrier means shown fig first principal components mostly try account lighting translation shown fig jojic frey figure modeling handwritten digits means components sheared translated means dimmed transformations low probability tea models trained examples digit means components fa models trained data digits generated tca models fa models means mixture gaussians mixture factor analyzers cluster tmg trained digits case best experiments selected modeling handwritten digits performed supervised unsupervised learning experiments greyscale versions digits cedar cdrom hull although preprocessed images fit snugly window wide variation writing angle vertical stroke different angles produced set shearing translation transformations see top row fig use transformed density models supervised learning experiments trained one component tca class digit using iterations em fig shows mean components models lower rows images fig show sheared translated means cases transformation probability image dimmed also trained one component factor analyzer class digit using iterations em means components shown fig means found tca sharper whereas components found factor analysis often account writing angle see components components found tca tend account line thickness arc size fig show digits randomly generated tcas factor analyzers since different components factor analyzers account different stroke angles simulated digits often extra stroke whereas digits simulated tcas contain fewer spurious strokes test recognition performance trained component factor analyzers tcas examples digit using iterations em set models used bayes rule classify test patterns factor analysis gave error rate tca gave error rate unsupervised learning experiments fit cluster mixture models entire set digits see models could identify digits tried mixture gaussians mixture factor analyzers cluster tmg case models trained using iterations em model topographic transformation discrete latent variable highest likelihood selected shown fig compared tmg first two methods found blurred repeated classes identifying cluster prevalent class digit found first two methods error rates tmg much lower error rate summary many learning applications know beforehand data includes transformations easily specified nature shearing digit images generafive density model learned data model must extract model transformations interesting potentially useful structure described way add transformation invariance generatire density model approximating transformation manifold discrete set points releases generatire model needing model transformations different types experiment show method effective quite efficient although time needed method scales exponentially dimensionality transformation manifold believe useful many practical applications illustrates possible generafive model incorporates latent transformation variable exploring performance faster variational learning method extending model time series acknowledgements used cito nserc nsf beckman foundation grants references bishop svensen williams gtm generative topographic mapping neural computation hinton dayan revow modeling manifolds images handwritten digits ieee trans neural networks ghahramani hinton algorithm mixtures factor analyzers university toronto technical report crg tr available www gatsby ucl ac uk zoubin golem cohen scanning electron microscope image enhancement school computer electrical engineering project report ben gurion university hull database handwritten text recognition research ieee trans pattern analysis machine intelligence le cun bottou bengio haffner gradient based learning applied document recognition proceedings ieee november simard victorri le cun denker tangent prop formalism specifying selected invariances adaptive network advances neural information processing systems morgan kaufmann san mateo ca simard le cun denker efficient pattern recognition using new transformation distance hanson cowan giles advances neural information processing systems morgan kaufmann san mateo ca appendix sufficient statistics found step sufficient statistics step computed step using sparse linear algebra single pass training set making pass following matrices computed cov gt gt dt cov ytx iac ae case training set xt first computed combination computing xt cac lx aci cac cgtq xt gtlac uc uc lxt xt zlxt diag flt diag flt acdt cac lxt zlx ylx acdt expectations needed computed xt ylxt et lx ix ylx clx acy acy ixt xt lxt diag acdt ca diag ace zt ace ylxt ac ylxt xt gtz xt gtz ixt ec xt xtgte zlxt xt gee zlxt diag gefle eg diag gtflt aedt ea flt eg clxt lxt llxt lxt clxt yy lxt et llxt dt et tlxt ylxt ylxt
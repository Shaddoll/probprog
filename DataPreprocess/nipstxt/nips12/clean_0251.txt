abstract present three simple approximations calculation posterior mean gaussian process classification first two methods related mean field ideas known statistical physics third approach based bayesian online approach motivated recent results statistical mechanics neural networks present simulation results showing mean field bayesian evidence may used hyperparameter tuning online approach may achieve low training error fast introduction gaussian processes provide promising non parametric bayesian approaches regression classification statistical models assumed likelihood output target variable given input written yla functions gaussian prior distribution priori assumed gaussian random field means finite set field variables xi jointly gaussian distributed given covariance xi xj xi xj also assume zero mean throughout paper predictions novel inputs set training examples xi yi given computed posterior distribution variables xl major technical problem gaussian process models difficulty computing posterior averages high dimensional integrals likelihood gaussian happens example classification problems far variety approximation techniques discussed monte carlo sampling map approach bounds likelihood tap mean field approach paper introduce three different novel methods approximating posterior mean random field think simple enough used practical applications two techniques csat fokoud opper schottky lt nther based mean field ideas statistical mechanics contrast previously developed tap approach easier implement also yield simple approximations total likelihood data evidence used tune hyperparameters covariance kernel bayesian evidence mlii framework aims maximizing likelihood data specialize case binary classification problem simplicity class label assumed noise free likelihood chosen ya unit step function equals zero else interested computing efficient approximations posterior mean use prediction labels via sign denotes posterior expectation posterior distribution symmetric around mean give bayes optimal prediction starting let us add two comments likelihood first map approach predicting fields maximize posterior would applicable gives trivial result second noise easily introduced within probit model subsequent calculations slightly altered moreover gaussian average involved definition probit likelihood always shifted likelihood gaussian process prior redefinition fields change prediction leaving us simple likelihood modified process covariance exact results first glance may seem order calculate deal joint posterior fields ai xi together field test point would imply test point different new dimensional average performed actually show case let denote expectation gaussian prior posterior expectation point say ij yjlay integration parts likelihood written np yjlaj xj jyj yj oaj showing dependent test point therefore necessary compute dimensional average every prediction chosen specific definition order stress similarity predictions support vector machines likelihood aj come nonnegative next sections develop three approaches approximate computation aj mean field method ensemble learning first goal approximate true posterior distribution ald vr detk efficient approaches gaussian process classification simpler tractable distribution denotes covariance matrix elements kij xi xj variational mean field approach known ensemble learning neural computation community relative entropy distance kl da minimized family product distributions hj qj aj contrast variational bound likelihood computed get kl daiqi ai ln qi ai yilai ai denotes expectation setting functional derivative kl respect qi equal zero find best product distribution gaussian prior times original likelihood qi cr yila rni ai jv ij aj ai using specific form approximated posterior replacing average true posterior approximation get using likelihood set rn nonlinear equations unknowns np yjlaj oand rnj kjiyi ti jyjotj zoodt useful byproduct variational approximation upper bound bayesian evidence da dla derived denotes gaussian process prior dla ij yjlaj bound written terms mean field free energy lnp sqlnq eqln vr dla eln rnj der hi used yardstick selecting appropriate hyperparameters covariance kernel ensemble learning approach little drawback requires inversion covariance matrix free energy one must compute determinant second simpler approximation avoids computations mean field theory ii naive approach second mean field theory aims working directly variables otj starting point consider partition function evidence dze yjlzj csat fokoud opper schottky winther follows standard gaussian integration introducing fourier eaeiazp imaginary transform likelihood ylz unit tempting view normalizing partition function gaussian process zi covariance matrix likelihood unfortunately real number precludes proper probabilistic interpretation nevertheless dealing formally complex measure defined integration parts shows one yjaj zj brackets denote average complex measure suggests simple approximation calculating aj one may think trying saddle point steepest descent approximation replace zj value zj complex plane makes integrand stationary thereby neglecting fluctuations zj hence approximation would treat expectations products zizj zi zj may reasonable definitely self correlation according general formalism mean field theories outlined one separately improve idea treating self interactions done replacing zi except form new variable inserting dirac function representation integrate variables exactly integral factorizes finally perform saddle point integration variables details calculation given elsewhere within saddle point approximation get system nonlinear equations mj kji kjiyioq iyj yj form aj replaced simpler kjj equations also derived us using callen identity present derivation allows also approximation evidence plugging saddlepoint values back partition function get lnp ln yi yic ki sijkii yjc also simpler compute give bound true evidence sequential approach previous algorithms give explicit expression posterior mean require solution set nonlinear equations must obtained iterative procedure present different approach approximate computation posterior mean based single sequential sweep whole dataset giving explicit update posterior algorithm based recently proposed bayesian approach online learning see articles opper winther solla basic idea applied gaussian process scenario follows suppose qt gaussian approximation posterior seen examples means approximate posterior process gaussian process mean covariance kt starting new data point yt observed posterior updated according bayes rule new non gaussian posterior projected back family gaussians choosing closest gaussian qt minimizing relative entropy kl qt efficient approaches gaussian process classifican order keep loss information small projection equivalent matching first two moments qt first moment get yt lla xt kt xt second line follows integration parts zt kt xt xt recursion corresponding one kt solved ansatz vector matrix also nonzero elements updated kt kt vector elements kts denotes element wise product vectors sequential algorithm defined advantage requiring matrix inversions also need solve numerical optimization problem time approach different update gaussian posterior approximation proposed since require linearization likelihood method equivalent extended kalman filter approach since possible compute evidence new datapoint yt based old posterior compute approximation log evidence data via simulations present two sets simulations mean field approaches first test bayesian evidence framework tuning hyperparameters covariance function kernel second test ability sequential approach achieve low training error stable test error fixed hyperparameters evidence framework give simulation results mean field free energies single data set pima indian diabetes training test examples input dimensionality results therefore taken conclusive evidence merits approaches simply indication may give reasonable results use eldwl radial basis function covariance function exp diagonal term added covariance matrix corresponding gaussian noise added fields variance free energy lnp minimized gradient descent respect lengthscale parameters wx wd mean field equations solved iteration update hyperparameters details given elsewhere figure shows evolution naive mean free energy test error starting uniform csat fokou opper schottky winther ws typically requires order iteration steps equations hyperparameter update also used hybrid approaches free energy minimized one mean field algorithm hyperparameters used may seen table naive mean field theory overestimate free energy since ensemble free energy upper bound free energy overestimation nearly severe minimum naive mean field free energy another interesting observation long hyperparameters used actual performance measured test error sensitive algorithm used also seems case tap mean field approach support vector machines iterations iterations figure hyperparameter optimization pima indians data set using naive mean field free energy left figure free energy function number hyperparameter updates right figure test error count function number hyperparameter updates table pima indians dataset hyperparameters found free energy minimization left column gives free energy used hyperparameter optimization test error counts range previously reported ensemble mf naive mf free energy minimization error error ensemble mean field eq naive mean field eq sequential algorithm studied sonar crab datasets since computed approximation evidence far simple fixed polynomial kernel used although probabilistic justification algorithm valid single sweep data used independence data assumed tempting reuse data iterate procedure heuristic two plots show way small improvement obtained seems method rather efficient extracting information data single presentation sonar dataset single sweep enough achieve zero training error acknowledgements bs would like thank leverhulme trust support work also supported epsrc grant gr efficient approaches gaussian process classification training error test error iteration iteration figure training test errors learning sonar left crab dataset right vertical dash dotted line marks end training set starting point reusing kernel function used order dimension inputs
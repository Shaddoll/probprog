abstract ever since pearl probability propagation algorithm graphs cycles shown produce excellent results error correcting decoding years ago curious whether local probability propagation could used successfully machine learning one simplest adaptive models factor analyzer two layer network models bottom layer sensory inputs linear combination top layer factors plus independent gaussian sensor noise show local probability propagation factor analyzer network usually takes iterations perform accurate inference even networks sensors factors derive expression algorithm fixed point show fixed point matches exact solution variety networks even fixed point unstable also show method used successfully perform inference approximate em give results online face recognition task factor analysis simple way encode input patterns suppose input wellapproximated linear combination component vectors amplitudes vectors modulated match input given training set appropriate set component vectors depend expect modulation levels behave emd measure distance input approximation effects captured generative probability model specifies distribution modulation levels zk distribution xlz sensors given modulation levels principal component analysis independent component analysis factor analysis viewed maximum likelihood learning model type assume training set appropriate modulation levels independent overall distortion given sum individual sensor distortions factor analysis modulation levels called factors distributions following form xnlz xn ek nkzk lp zk xlz az parameters model factor loading matrix elements emd diagonal sensor noise covariance matrix diagonal elements pn belief network factor analyzer shown fig la likelihood hf hf az dz hf aa local probability propagation factor analysis figure belief network factor analysis high dimensional data online factor analysis consists adapting increase likelihood current input vector pixels image fig lb probabilistic inference computing estimating zlx needed dimensionality reduction fill unobserved factors online em type learning paper focus methods infer independent factors zlx gaussian turns posterior means variances factors zlx ia iat ix diag cov zlx diag la given computing values exactly takes computations mainly time needed compute ia since kn connections network exact inference takes least bottom top iterations course network going applied times inference batch em matrices computed reused however directly applicable online learning biological models one way circumvent computing matrices keep separate recognition network approximates zlx rx dayan et al optimal recognition network approximated jointly estimating generative network recognition network using online wakesleep learning hinton et al probability propagation factor analyzer network recent results error correcting coding show cases pearl probability propagation algorithm exact probabilistic inference graphs trees gives excellent performance even network contains many cycles minimal cut set exponential prey mackay prey mackay fact probability propagation algorithm decoding lowdensity parity check codes mackay turbocodes berrou glavieux widely considered major breakthrough information theory community network contains cycles local computations give rise iterative algorithm hopefully converges good answer little known convergence properties algorithm networks containing single cycle successfully analyzed weiss smyth et al results networks containing many cycles much less revealing probability messages produced probability propagation factor analyzer network fig la gaussians iteration propagation consists passing mean variance along edge bottom pass followed passing mean variance along edge top pass instant fre bottom means variances combined form estimates means variances modulation levels given input initially variance mean sent kth top layer unit nth sensor set bottom pass begins computing noise lkn level error signal sensor using top variances means previous iteration nk kn xn nk kn used compute bottom variances means follows ni knk nk bottom variances means combined form current estimates modulation riances means zn bnk nk pnk top pass proceeds computing top variances means follows bn lv lkn kn notice variance updates independent mean updates whereas mean updates depend variance updates performance local probability propagation created total factor analysis networks different sizes ranging size network measured inference error function number iterations propagation networks given size produced drawing standard normal distributions drawing sensor variance exponential distribution similar procedure used neal dayan mean random network pattern simulated network probability propagation applied using simulated pattern input measured error estimate correct value zlx computing difference coding costs exact posterior distribution normalizing get average number nats per top layer unit fig shows inference error logarithmic scale versus number iterations maximum different network sizes cases median error reduced hats within iterations rate convergence error improves larger indicated general trend error curves drop increased contrast rate convergence error appears worsen larger shown general slight trend error curves rise increased networks actually diverge better understand divergent cases studied means variances divergent networks cases variances converge within iterations whereas means oscillate diverge networks diverged shown fig observation suggests general dynamics determined dynamics mean updates fixed points condition global convergence variance updates converge dynamics probability propagation factor analysis networks become linear allows us derive fixed point propagation closed form write eigenvalue condition global convergence local probability propagation factor analysis lo ii lo lo figure performance probability propagation median inference error bold curve logarithmic scale function number iterations different sizes network parameterized two curves adjacent bold curve show range within errors lie errors fourth topmost curve error bottom variances top means function number iterations maximum divergent networks size analyze system mean updates define following length kn vectors means input ik xl xl xl xn xn xn repeated times last vector network parameters represented using kn kn diagonal matrices diagonal aik diagonal identity matrix converged bottom variances represented using diagonal matrix diagonal elk nk summation operations propagation formul represented kn kn matrix sums means sent top layer kn kn matrix sums means sent sensory input matrices blocks block ones identity matrix using representations bottom pass given top pass given diag substituting get linear update diag lg fre figure error log scale versus number iterations log scale max vergent networks means initialized fixed point solutions machine round errofs cause divergence fixed points whose effofs shown horizontam lines fixed point dynamic system exists di fixed point exists determinant expression large braces nonzero found simplified expression determinant terms determinants smaller matrices reinterpreting dynmics dynamics stability fixed point determined gest eigenvalue update matrix diag modulus largest eigenvalue less fixed point stable since system linear stable fixed point exists system globally convergent point networks explored networks converged divergent networks used iterations probability propagation compute steady state variances computed modulus largest eigenvalue system computed fixed point initializing bottom means fixed point values performed iterations see numerical errors due machine precision would cause divergence fixed point fig shows error versus number iterations logarithmic scales network error fixed point modulus largest eigenvalue cases network diverges fixed point reaches dynamic equilibrium lower average error fixed point online factor analysis perform imum likelihood factor analysis online fashion parameter modified slightly increase log probability current sensory input logp however since factors hidden must probabilistically filled using inference incremental leaning step performed estimated mean variance kth factor turns neal dayan parameters updated follows lv learning rate online learning consists performing number iterations probability propagation current input iterations modifying ameters processing next input results simulated data produced training sets cases input sizes ranging sensors sensors sizes factor analyzer randomly selected sets parameters described generated tr ning set factor analyzer sizes local probability propagation factor analysis figure achievable errors number epochs learning using iterations versus iteration horizontal axis gives log probability error log scale learning iteration vertical axis gives error number epochs learning iterations achievable errors learning using iterations propagation versus wake sleep learning using iterations factor analyzer simulated data set estimated optimal log probability data using iterations em learning size model trained set equal size model used generate data avoid issue schedule learning rates searched achievable learning curves regardless whether simple schedule learning rate exists given method randomly initialized parameters performed one separate epoch learning using learning rates picked learning rate improved log probability successive learning rate determined comparing performance using old learning rate one times smaller mainly interested comparing achievable curves different methods differences scale two methods trained data plot log probability error optimal logprobability minus log probability learned model one method log probability error method fig shows achievable errors using iterations versus using iteration usually using iterations produces networks lower errors learned using iteration difference significant networks large sec found convergence inference error slower fig shows achievable errors learning using iterations probability propagation versus wake sleep learning using iterations generally probability propagation achieves much smaller errors wake sleep learning although small wake sleep performs better close optimum log probability significant difference methods occurs large aside local optima probability propagation achieves nearly optimal log probabilities log probabilities wake sleep learning still close values start learning online face recognition fig lb shows examples set greyscale face images different people contrast data sets used test face recognition methods faces include wide variation expression pose make classification difficult normalized images person pixel frey mean variance used probability propagation recognition network factor analyzer reduce dimensionality data online dimensions dimensions probability propagation rather arbitrarily chose learning rate wake sleep learning tried learning rates ranging multilayer perceptron one hidden layer tanh units one output layer softmax units simultaneously trained using gradient descent predict face identity mean factors learning rate multilayer perceptron set value used methods image prediction made parameters modified fig shows online error curves obtained filtering losses curve probability propagation generally curves wake sleep learning figure also shows error curves two forms online nearest neighbors recent cases used make prediction form nearest neighbors performs worst set storage requirements factor analysis multilayer perceptron method better form nearest neighbors set number computations factor analysis multilayer perceptron method number pattern presentations figure online error curves probability propagation solid wake sleep learning dashed nearest neighbors dot dashed summary guessing dotted turns iterative probability propagation fruitful used learning graphical model cycles even model densely connected although interested extending work complex models exact inference takes exponential time studying iterative probability propagation factor analyzer allowed us compare results exact inference allowed us derive fixed point algorithm currently applying iterative propagation multiple cause networks vision problems references berrou glavieux near optimum error correcting coding decoding turbo codes ieee trans communications dayan hinton neal zemel helmholtz machine neural computation frey mackay revolution belief propagation graphs cycles jordan kearns solla eds advances neural information processing systems denver frey graphical models machine learning digital communication mit press cambridge see http vvv cs utoronto ca frey hinton dayan frey neal wake sleep algorithm unsupervised neural networks science mackay information theory inference learning algorithms book preparation currently available http wol ra phy cam ac uk mackay neal dayan factor analysis using delta rule wake sleep learning neural computation myth mceliece xu aji horn probability propagation graphs cycles presented workshop inference learning graphical models vail colorado weiss correctness local probability propagation graphical models appear neural computation
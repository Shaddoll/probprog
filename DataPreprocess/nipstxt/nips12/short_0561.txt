abstract adaboost ensemble methods successfully applied number classification tasks seemingly defying problems overfitting adaboost performs gradient descent error function respect margin asymptotically concentrating patterns hardest learn noisy problems however disadvantageous indeed theoretical analysis shown margin distribution opposed minimal margin plays crucial role understanding phenomenon loosely speaking outliers tolerated benefit substantially increasing margin remaining points propose new boosting algorithm allows possibility pre specified fraction points lie margin area even wrong side decision boundary
abstract paper treat input selection radial basis function rbf like classifier within bayesian framework approximate posteriori distribution model coefficients input subsets samples drawn gibbs updates reversible jump moves using public datasets compare classification accuracy method conventional ard scheme datasets also used infer posteriori probabilities different input subsets introduction methods aim determine relevance inputs always interested researchers various communities classical feature subset selection techniques reviewed use search algorithms evaluation criteria determine one optimal subset although approaches improve classification accuracy explore different equally probable subsets automatic relevance determination ard another approach determines relevance inputs ard due uses bayesian techniques hierarchical priors penalize irrelevant inputs approach also bayesian relevance inputs measured probability distribution possible feature subsets probability measure determined bayesian evidence corresponding models general idea already used variable selection linear regression mo dels though interest different select inputs nonlinear classification model want approximation true distribution different subsets number subsets grows exponentially total number inputs calculate bayesian model evidence directly need method samples efficiently across different dimensional parameter spaces general method reversible jump markov chain monte carlo sampler reversible jump mc recently proposed approach successfully applied determine probability distribution mixture density model variable number kernels sample posterior rbf regression networks variable number kernels markov chain switches different input subsets useful two tasks counting often particular subset visited gives us relevance measure corresponding inputs classification approximate input selection reversible jump mcmc integral input sets coefficients summation samples markov chain next sections show implement reversible jump mc apply proposed algorithm classification input evaluation using public datasets though approach could improve mlp ard scheme terms classification accuracy still think interesting assess importance different feature subsets different importance single features estimated ard methods classifier used paper rbf like model inference performed within bayesian framework conditioning one set inputs posterior model parameters already multimodal therefore resort markov chain monte carlo mcmc sampling chniques approximate desired posterior model coefficients feature subsets next subsections propose appropriate architecture classifier hybrid sampler model inference hybrid sampler consists two parts use gibbs updates sample conditioning particular set inputs reversible jump moves carry dimension switching updates classifier order allow input relevance determination bayesian model selection classifier needs least one coefficient associated input roughly speaking probability model proportional likelihood probable coefficients weighted posterior width divided prior width first factor always increases using coefficients input features second decrease inputs use together gives peak probable model classifier satisfies constraints called classification sampling paradigm model class conditional densities together class priors express posterior probabilities classes neural network literature approach first proposed use model allows overlapping class conditional densities lk wkap pkp lk using class priors xlk class conditional densities expresses posterior probabilities classes klx xlk choose component densities xl gaussian restricted parametrisation kernel multivariate normal distribution mean diagonal covariance matrix gaussian kernels together get parameters denoting current input dimension denoting number kernels apart kernel coefficients coefficients per class indicating prior kernel allocation probabilities class priors model allows treat labels patterns missing data use labeled well unlabeled data model inference case training carried using likelihood observing inputs targets xlo rl pk lo ii denotes labeled unlabeled training data coefficients th class conditional density depends use model sykacek coefficients together nk number samples belonging class index unlabeled samples make gibbs updates possible introduce two latent allocation variables first one indicates kernel number sample generated second one unobserved class label introduced unlabeled data typical approaches training models like use em algorithm closely related gibbs sampler introduce next subsection fixed dimension sampling subsection formulate gibbs updates sampling posterior conditioning fixed set inputs order allow sampling full conditional choose priors coefficients conjugate family component mean given gaussian prior rn afd inverse variance input kernel gets gamma prior variances input common hyperparameter gamma hyperprior hi mixing coefficients get dirichlet prior class priors also get dirichlet prior sp sp quantitative settings similar used values usually hi typically loire ri denoting th input range mean gets gaussian prior centered midpoint diagonal inverse covariance matrix ii prior counts set give corresponding probabilities non informative proper dirichlet priors gibbs sampler uses updates full conditional distributions notational convenience use parameters determine class conditional densities use index unlabeled data latent class label index data latent kernel allocations number samples allocated th component one distribution occur prior specification aan multinomial one distribution finally need counters counts per class rn count kernel allocations class patterns full conditional th kernel variances hyper parameter contain index input dimension express separately expression th kernel mean input selection reversible jump mcmc use denote entire covariance matrix gq da hiq talk mdk rn rn ndv nd nd moving different input subsets core part sampler reversible jump updates move different feature subsets probability feature subset determined corresponding bayesian model evidence additional prior number inputs accordance use truncated poisson prior im constant im total nr inputs reversible jump updates generalizations conventional metropolis hastings updates moves bijections thorough treatment refer order switch subsets efficiently use two different types moves first consist step add one input chosen random matching step removes one randomly chosen input second move exchanges two inputs allows tunneling low likelihood areas adding input increase dimension kernel means diagonal covariances coefficients drawn priors addition move proposes new allocation probabilities semi deterministic way assuming ordering wk wk vd beta matching step proposes removing randomly chosen input removing corresponding kernel coefficients combined semi deterministic proposal new allocation probabilities exactly symmetric proposal sykacek table summary experiments data avg max rbf ha mlp ionosphere pima wine accept births probability ao min rt exp jr exp rd first line likelihood prior ratio prior ratio results difference input dimension affects kernel means prior number inputs first term proposal ratio proposing add remove one input second term proposal density additional kernel components cancels corresponding term prior ratio due symmetry proposal reverse death move contribution changing allocation probabilities death moves accepted probability ad ao second type move exchange move select new input one model inputs propose new mean coefficients gives following acceptance probability rain ratio exp lid exp iv first line likelihood prior ratio exchange moves prior ratio ratio different values kernel means first term proposal ratio proposing exchange input second term proposal density new kernel mean components last part proposing new allocation probabilities experiments although method used labeled unlabeled data following experiments performed using labeled data experiments set first two data sets uci repository use available http www ics uci edu mlearn mlrepository html input selection reversible jump mcmc ionosphere data inputs training test samples experiment use kernels set second data wine recognition data provides inputs training test samples data use kernels set third experiment performed pima data provided ripley one use kernels set experiments draw samples posterior coefficients input subsets discard first samples burn use rest predictions classification accuracy compared mlp classifier using neals hybrid monte carlo sampling ard priors inputs experiments use hidden units table contains details avg average max maximal number inputs used hybrid sampler rbf ha classification accuracy hybrid sampler number errors made made ard mlp mlp ard mlp compare classifiers testing ha null hypothesis observation binomial bn na distribution reveals neither difference significant although could improve classification accuracy data really matter ard methods usually lead high generalization accuracy compete real benefit using hybrid sampler infer probabilities telling us much different subsets contribute explanation target variables figure shows occurrence probabilities feature subsets features note table also details many features used problems especially results ionosphere data interesting average use input features ionosphere wine data markov chain visits different input subsets within samples pima data number order magnitude smaller discussion paper discussed hybrid sampler uses gibbs updates reversible jump moves approximate posteriori distribution parameters input subsets nonlinear classification problems classification accuracy method could compete neals mlp ard implementation however real advantage method provides us relevance measure feature subsets allows infer optimal number inputs many different explanations data provides acknowledgement want thank several people used resources provide used neals hybrid markov chain sampler mlp experiments data used experiments obtained form university irvine repository ripley furthermore want express gratitude anonymous reviewers comments de freitas useful discussions conference work done framework research project gz verbesserung der biosignalverarbeitung durch beruecksichtigung von unsicherheit und konfidenz funded austrian federal ministry science transport bmwv available http www stats ox ac uk sykacek probabilities input subsets ionosphere ii probabilities input subsets pima oo probabilities inputs ionosphere probabilities inputs pima probabilities inputs wine probabilities input subsets wine lll jib figure probabilities inputs input subsets measuring relevance
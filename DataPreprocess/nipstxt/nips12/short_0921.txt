abstract committee approach proposed reducing model uncertainty improving generalization performance advantage committees depends performance individual members correlational structure errors members paper presents input grouping technique designing heterogeneous committee technique input variables first grouped based mutual information statistically similar variables assigned group member input set formed input variables extracted different groups designed committees less error correlation members since member observes different input variable combinations individual member feature sets contain less redundant information highly correlated variables combined together member feature sets contain almost complete information since set contains feature information group empirical study noisy nonstationary economic forecasting problem shows committees constructed proposed technique outperform committees formed using several existing techniques introduction committee approach widely used reduce model uncertainty improve generalization performance developing methods generating candidate committee members important direction committee research good candidate members committee good necessarily excellent individual performance small residual error correlations members many techniques proposed reduce residual correlations members include resampling training validation data adding randomness data decorrelation training approaches effective certain models problems genetic algorithms also used generate good diverse members input feature selection one important stages model learning process crucial impact learning complexity general liao andj moody ization performance essential feature vector gives sufficient information estimation however many redundant input features burden whole learning process also degrade achievable generalization performance input feature selection individual estimators received lot attention importance however much research feature selection estimators context committees previous research found giving committee members different input features useful improving committee performance difficult implement feature selection problem committee members conceptually different single estimators using committees estimation stated previously committee members need reasonable performance also make decisions independently committee members trained model underlying function difficult committee members optimize criteria time order generate members provide good balance two criteria propose feature selection approach called input feature grouping committee members idea give member estimator committee rich distinct feature sets hope member generalize independently reduced error correlations proposed method first groups input features using hierarchical clustering algorithm based mutual information features different groups less related features within group statistically similar feature set committee member formed selecting feature group empirical results demonstrate forming heterogeneous committee using input feature grouping promising approach committee performance analysis many ways construct committee paper mainly interested heterogeneous committees whose members different input feature sets committee members given different subsets available feature set trained independently committee output either weighted unweighted combination individual members outputs following analyze relationship committee errors average member errors regression point view discuss residual correlations members affect committee error define training data xz yz test data assumed generated model rr data independent inputs drawn unknown distribution assume committee members denote available input features xl xm feature sets th jth members xi xil xi xm xj xjl xj xmj respectively xi xj xi xj mapping function th jta member models trained data fi xi fj xj define model fi andi eftof constructing heterogeneous committees economic forecasting mse committee average mse made committee members acting individually eave denotes expectation test data using jensen inequality get ec eave indicates performance committee always equal better average performance members define average model error correlation ej ei ec eave eave consider following four cases eave case case model errors members anti correlated might achieved decorrelation training case case model errors members eave say committee uncorrelated ec much better average performance members case eave bounded committee size ec qeave gives asymptotic limit committee performance size committee goes infinity committee error equal average model error correlation difference ec eave determined ratio case case ec equal eave happens obvious advantage combining set models act identically clear analyses committee shows advantage ratio less one smaller ratio better committee performs compared average performance members committee achieve substantial improvement single model committee members small errors individually also small residual correlations input feature grouping one way construct feature subset committee member randomly picking certain number features original feature set advantage method simple however control member performance residual correlation members randomly selecting subsets liao andj moody instead randomly picking subset features member propose input feature grouping method forming committee member feature sets input grouping method first groups features based relevance measure way features different groups less related one another features within group related one another grouping two ways form member feature sets one method construct feature set member selecting feature group forming member feature set way member enough information make decision feature set less redundancy method use paper another way use group feature set committee member method member partial information likely hurt individual member performance however input features different members less dependent members tend make decisions independently always trade increasing members independence hurting individual members performance redundancy among input feature representations removing several features may hurt individual members performance badly overall committee performance hurt even though members make decisions independently method currently investigation mutual information xi xj two input variables xi xj used relevance measure group inputs mutual information xj defined equation measures dependence two random variables xi xj xi xilxj xi yi log xi xj xi xj features xi xj highly dependent xi xj large mutual information measures arbitrary dependencies random variables effectively used feature selections complex prediction tasks methods bases linear relations like correlation likely make mistakes fact mutual information independent coordinates chosen permits robust estimation empirical studies apply input grouping method predict one month rate change index industrial production ip one key measures economic activity computed published monthly figure plots monthly ip data nine macroeconomic time series whose names given table used forecasting ip macroeconomic forecasting difficult task data usually limited series intrinsically noise nonstationary series preprocessed applied forecasting models representation used input series first difference one month time scales logged series example notation ip represents ip ip ip target series ip fd defined ip fd ip ip data set one benchmarks various studies constructing heterogeneous committees economic forecasting index industrial production year figure index industrial production ip period shaded regions denote official recessions unshaded regions denote official expansions boundaries recessions expansions determined national bureau economic research based several macroeconomic series evident ip business cycles irregular magnitude duration structure making prediction ip interesting challenge series description ip index industrial production sp standard poor dl index leading indicators money supply cp consumer price index cb moody aaa bond yield hs housing starts tb month treasury bill yield tr yield curve slope year bond composite month treasury bill table input data series data taken citibase database grouping procedure measures mutual information pairs input variables computed first simple histogram method used calculate estimates hierarchical clustering algorithm applied values group inputs hierarchical clustering proceeds series successive fusions nine input variables groups particular stage process fuses variables groups variables closest base mutual information estimates distance two groups defined average distances pairs individuals two groups result presented tree illustrates fusions made successive level see figure clustering tree clear break input variables four groups ip dl measure recent economic changes sp reflects recent stock market momentum cb tb tr give interest rate information cp hs provide inflation information grouping algorithm meaningfully clusters nine input series ao andj moody figure variable grouping based mutual information label distance eighteen different subsets features generated four groups selecting feature group subset given committee member example subsets ip sp cb dl sp tb used feature sets different committee members committee totally eighteen members data jan dec used training validation jan dec used testing member linear model trained using neural net techniques compare input grouping method three committee member generating methods baseline random selection bootstrapping baseline method train committee member using input variables members different initial weights bootstrapping method also trains member using input features member different bootstrap replicates original training data training validation sets random selection method constructs feature set member randomly picking subset available features comparison grouping method committee generated three methods members twenty runs performed four methods order get reliable performance measures figure shows boxplots normalized mse four methods grouping method gives best result performance improvement significant compared methods grouping method outperforms random selection method meaningfully grouping input features interesting note heterogeneous committee methods grouping random selection perform better homogeneous methods data set one reasons giving different members different input sets increases model independence another reason could problem becomes easier model smaller feature sets conclusions performance committee depends performance individual members correlational structure errors members empirical study noisy nonstationary economic forecasting problem demonstrated committees constructed input variable grouping outperform committees formed randomly selecting member input variables also outperform committees without input variable manipulation constructing heterogeneous committees economic forecasting committee psdon anc comparison runs grouping random selection bas bootstrapping figure comparison four different committee member generating methods proposed grouping method gives best result performance improvement significant compared three methods references battiti using mutual information selecting features supervised neural net learning ieee trans neural networks july everitt cluster analysis heinemann educational books breimam bagging predictors machine learning cherkauer human expert level performance scientific image analysis task system using combined artifical neural networks chan editor working notes aaai workshop integrating multiple learned models pages moody levin rehfuss predicting index industrial production proceedings parallel applications statistics economics conference zeist netherlands special issue neural network world opitz shavlik generating accurate nd diverse members neuralnetwork ensemble touretzky mozer hasselmo editors advances neural information processing systems mit press cambridge raviv intrator bootstrapping noise effective regularization technique connection science rosen ensemble learning using decorrelated neural networks connection science tumer ghosh error correlation error reduction ensemble classifiers connection science december wu moody smoothing regularizer feedforward recurrent neural networks neural computation
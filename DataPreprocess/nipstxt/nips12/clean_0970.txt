abstract describe bayesian approach model selection unsupervised learning determines feature set number clusters evaluate scheme based marginal likelihood one based cross validated likelihood bayesian scheme derive closed form solution marginal likelihood assuming appropriate forms likelihood function prior extensive experiments compare approaches results verified comparison ground truth experiments bayesian scheme using objective function gave better results cross validation introduction recent efforts define model selection problem one estimating number clusters easy see particularly applications large number features various choices feature subsets reveal different structures underlying data contention interplay feature subset number clusters essential provide appropriate views data thus define problem model selection clustering selecting number clusters feature subset towards end propose unified objective function whose arguments include feature space number clusters describe two approaches model selection using objective function first approach based bayesian scheme using marginal likelihood model selection second approach based scheme using cross validated likelihood section apply approaches document clustering making assumptions document generation model bayesian approach derive closed form solution marginal likelihood using document generation model also describe heuristic initial feature selection based distributional clustering terms section describes experiments approach validate proposed models algorithms section reports discusses results experiments finally section provides directions future work model selection unsupervised learning high dimensions model selection clustering model selection approaches clustering primarily concentrated determining number components clusters attempts include bayesian approaches mdl approaches cross validation techniques noticed however optimal number clusters dependent feature space clustering performed related work described generalized model clustering let data set consisting patterns dv assume represented feature space dimension particular problem address clustering groups likelihood described probability model drlf maximized indicates representation feature space structure model consists number clusters partitioning feature set explained assignment patterns clusters model weighted sum models drlf set parameters associated define model begin assuming feature space consists two sets useful features noise features feature selection problem thus consist partitioning given number clusters assumption feature sets represented conditionally independent dri indicates data represented noise feature space indicates data represented useful feature space using assumption assuming data independently drawn rewrite equation drlf lsi ii ii dt vis number patterns dy probability given parameter vector probability given parameter vector note explicit dependence removed notation implicit number clusters partition bayesian approach model selection objective function represented equation regularized attempts optimize directly may result set becoming empty resulting overfitting overcome problem use marginal likelihood assumption parameter vectors independent denotes bayesian prior distribution marginal likelihood using assumption written orlf dfl dyl vaithyanathan dom integral limits appropriate particular parameter spaces omitted simplify notation document clustering document clustering algorithms typically start representing document bag words features number ad hoc dimensionality reduction techniques stop word removal frequency based truncations techniques lsi available dimensionality reduced documents usually clustered arbitrary number clusters multinomial models several models text generation studied choice multinomial models using term counts features choice introduces another parameter indicating probability split equivalent assuming generation model document number noise useful terms determined probability terms document drawn probability marginal likelihood stochastic complexity apply bayesian objective function begin substituting multinomial models simplifying obtain ifs tv os tn os tu os ti ulu multinomial coefficient number occu ences feature term document total number useful features terms document ti ti interpreted similar noise features total number noise features patterns tsis total number useful features patterns solve still need form priors beta family conjugate binomial family choose dirichlet distribution multiple beta form beta distribution os substituting equation simplifying yields di tv fi aa ld ts idel model selection unsupervised learning high dimensions au hyper parameters dirichlet prior noise useful features respectively sn au akand fo gamma function hyper parameters beta prior split probability id number documents cluster tt computed results dk reported evaluation negative log equation following rissanen refer stochastic complexity sc experiments values hyper parameters ta set equal yielding uniform priors cross validated likelihood compute cross validated likelihood using multinomial models first substitute multinomial functional forms using mle found using training set results following equation te cv tgl cv io cv ok mle appropriate parameter vectors implementation mccv following suggestion used split training test set vcv criterion although value suggested therein computational reasons used value feature subset selection algorithm document clustering noted section feature set size total partitions large would computationally intractable search possible partitions find optimal subset section propose heuristic method obtain subset tokens topical indicative underlying topics used features bag words model cluster documents distributional clustering feature subset selection identifying content bearing topical terms active research area less concerned modeling exact distributions individual terms simply identifying groups terms topical distributional clustering dc apparently first proposed pereira et al used feature selection supervised text classification clustering images video sequences hypothesize function content bearing topical terms different distributions documents dc helps reduce size search space feature selection number clusters produced dc algorithm following suggestions compute following histogram token first bin consists number documents zero occurrences token second bin number documents consisting single occurrence token third bin number documents contain two occurrences term histograms clustered using relative entropy ii vaithyanathan dom distance measure two terms probability distributions given ii pl log use means style algorithm histograms normalized sum one sum equation taken three bins corresponding counts assignment clusters step means compute pw ii pc pw normalized histogram term pc centroid cluster term assigned cluster minimum experimental setup evaluation experiments compared clustering results human labeled ground truth corpus used ap reuters newswire articles trec collection total documents routing track existing classes analyzed experiments simplify matters disregarded multiple assignments retained document member single class mutual information evaluation measure clustering verify models comparing clustering results pre classified text force clustering algorithms produce exactly many clusters classes pre classified text report mutual information mi cluster labels pre classified class labels results discussions tokenizing documents discarding terms appeared less documents left unique terms experimented several numbers clusters dc report best lowest sc lack space clusters chose best runs corresponding different random starting clusters sets includes one cluster consists high frequency words upon examination found contain primarily function words eliminated consideration remaining non function word clusters used feature sets clustering algorithm combinations feature sets produced good results used document clustering runs initialized em algorithm using means algorithm initialization schemes discussed feature vectors used means initialization generated using pivoted normal weighting suggested parameter vectors estimated using laplace rule succession table shows best results sc criterion vcv mccv using feature subsets selected different combinations distributional clusters feature subsets coded fsxp indicates number clusters distributional clustering indicates cluster number used sc mi results reported averages runs means em combination different initialization fo means clarity mi numbers reported normalized theoretical maximum also show comparisons feature selection nf lsi model selection unsupervised learning high dimensions lsi principal eigenvectors retained means clustering performed reduced dimensional space determining number clusters computational reasons limited evaluation feature subset provided us highest mi fs feature useful sc vcv mccv mi set features fs fs nf lsi na na na table comparison results figuro discussion consistency mi sc figure striking monotonic trend apparent higher sc indicating bad clusterings easily detected sc solution improves differences subtle note best value sc mi coincide given assumptions made deriving equation consistency encouraging interested reader referred details figures indicate certainly reasonable consistency cross validated likelihood mi although striking sc note mi feature sets picked mccv vcv significantly lower best feature set figures show plots sc mccv vcv number clusters increased using sc see fs reveals optimal structure around clusters feature selection mccv vcv obtain models lower complexity sc show optimum clusters experiments required draw final conclusions however full bayesian approach seems practical useful approach model selection document clustering choice likelihood function priors provide closed form solution computationally tractable provides meaningful results conclusions paper tackled problem model structure determination clustering main contribution paper bayesian objective function treats optimal model selection choosing number clusters feature subset important aspect work formal notion forms basis feature selection unsupervised learning evaluated two approaches model selection one using objective function based cross validation vaithyanathan dom approaches performed reasonably well bayesian scheme outperforming cross validation approaches feature selection experiments using different parameter settings cross validation schemes different priors bayesian scheme result better understanding therefore powerful applications approaches figure figure gure figure
abstract paper propose full bayesian model neural networks model treats model dimension number neurons model parameters regularisation parameters noise parameters random variables need estimated propose reversible jump markov chain monte carlo mcmc method perform necessary computations find results better previously reported ones also appear robust respect prior specification moreover present geometric convergence theorem algorithm introduction early nineties buntine weigend macl showed principled bayesian learning approach neural networks lead many improvements particular macl showed approximating distributions weights gaussians adopting smoothing priors possible obtain estimates weights output variances automatically set regularisation coefficients neal cast net much introducing advanced bayesian simulation methods specifically hybrid monte carlo method analysis neural networks bayesian sequential monte carlo methods also shown provide good training results especially time varying scenarios recently rios insua milllet holmes mallick addressed issue selecting number hidden neurons growing pruning algorithms bayesian perspective particular apply reversible jump markov chain monte carlo mcmc algorithm green feed forward sigmoidal networks radial basis function rbf networks obtain joint estimates number neurons weights also apply reversible jump mcmc simulation algorithm rbf networks compute joint posterior distribution radial basis parameters number basis functions however advance area research two important directions firstly propose full hierarchical prior rbf networks authorship based alphabetical order andrieu freitas doucet adopt full bayesian model accounts model order uncertainty regularisation show results appear robust respect prior specification secondly present geometric convergence theorem algorithm complexity problem allow comprehensive discussion short paper therefore focused describing objectives bayesian model convergence theorem results readers encouraged consult technical report results implementation details problem statement many physical processes may described following nonlinear multivariate input output mapping yt xt nt xt ir corresponds group input variables yt target variables unknown noise process index variable data context learning problem involves computing approximation function estimating characteristics noise process given set input output observations xl yl typical examples include regression yl continuous classification corresponds group classes nonlinear dynamical system identification inputs targets correspond several delayed versions signals consideration adopt approximation scheme holmes mallick consisting mixture rbfs linear regression term yet work easily extended regression models precisely model yt aj llx tjll ii ii denotes distance metric usually euclidean mahalanobis ird denotes th rbf centre model rbfs aj ir th rbf amplitude ir ir ir linear regression parameters noise sequence nt irc assumed zero mean white gaussian important mention although explicitly indicated dependency nt parameters indeed affected value convenience express approximation model vector matrix form yl yl yn yn xl xl xn xn xl xl ak ak nl software available http www cs berkeley edu jfgf yl matrix number data number outputs adopt notation yl yl denote observations corresponding th output th column simplify notation equivalent one index appear implied referring possible values similarly equivalent yl favour shorter notation adopt longer notation avoid ambiguities emphasise certain dependencies robust full bayesian methods neural networks noise process assumed normally distributed nt jv cr shorter notation nt assume number rbfs parameters th er unknown given data set objective estimate ok bayesian model aims follow bayesian approach unknowns regarded drawn appropriate prior distributions priors reflect degree belief relevant values quantities furthermore adopt hierarchical prior structure enables us treat priors parameters hyper parameters random variables drawn suitable distributions hyper priors instead fixing hyper parameters arbitrarily acknowledge inherent uncertainty think values devising probabilistic models deal uncertainty able implement estimation techniques robust specification hyper priors overall parameter space written finite union subkmax ok ira spaces uk ok kmax hyper parmeter space elements discussed end section space radial basis cemres defined compact set including input data min xl tei xl tei ei xl min xl denotes euclidean distance th dimension input user specified parameter need consider wish place basis functions outside region input data lie allow include space input data extend factor proportional spread input data hyper volume space ei imum number basis functions defined kma also define sumption independent outputs given likelihood ylk approximation model described previous section exp yl al yl al assume following structure prior distribution al lk scale parameters assumed independent hyperparameters la independent distributed according conjugate inverse gamma prior distri wo obtain je eys unbutions informative prior given prior distribution ln tr xrisilml exp andrieu freitas doucet im denotes identity matrix size ul indicator function set otherwise prior model order distribution kla truncated poisson distribution conditional upon rbf centres uniformly distributed finally conditional upon coefficients ot assumed zero mean gaussian variance ier hyper parameters ir ir respectively interpreted expected signal noise ratios expected number radial basis assume independent moreover iic scale parameter ascribe vague conjugate prior density aa fia aa fia variance hyper prior aa infinite apply method setting uninformative conjugate prior estimation inference aims bayesian inference based joint posterior distribution obtained bayes theorem aim estimate joint distribution standard probability marginalisation transformation techniques one theoretically obtain posterior features interest propose use reversible jump mcmc method perform necessary computations see details mcmc techniques introduced mid statistical physics started appearing fields applied statistics signal processing neural networks key idea build ergodic markov chain whose equilibrium distribution desired posterior distribution weak additional assumptions samples generated markov chain asymptotically distributed according posterior distribution thus allow easy evaluation posterior features interest example jlx addition obtain predictions yn llxi yl integration nuisance parameters according bayes theorem obtain posterior distribution follows blx ylk case integrate respect gaussian distribution inverse gamma distribution obtain following expression respect posterior yl ipi kyl im robust full bayesian methods neural networks worth noticing posterior distribution highly non linear rbf centres expression klx cannot obtained closed form geometric convergence theorem easy prove reversible jump mcmc algorithm applied model converges markov chain ergodic present stronger result namely converges required posterior distribution geometric rate theorem let kernel described section bility distribution markov chain whose transition markov chain converges proba furthermore convergence occurs geometric rate almost every initial point lt exists function initial states co constant tv xj distribution ii litv total variation norm proof see corollary iteration one samples nuisance parameters distribution series ier converges ge ometrically towards er rate demonstration robot arm data data often used benchmark compare learning algorithms involves implementing model map joint angle robot arm xl position end arm data generated following model cos cos sin xx sin ei af use first observations data set train models last observations test simulations chose use cubic basis functions figure shows plots training data contours training test data contour plots also include typical approximations obtained using algorithm chose uninformative priors parameters hyper parameters table demonstrate robustness algorithm chose different values fi critical hyper parameter quantifies mean spread obtained mean square errors probabilities cr cr shown figure clearly indicate algorithm robust respect prior specification mean square errors magnitude ones reported researchers slightly better moreover algorithm leads parsimonious models ones previously reported athe robot arm data set found david mackay home page http wol ra phy cam ac uk mackay andrieu freitas doucet xl xl figure top plots show training data surfaces corresponding coordinate robot arm position middle bottom plots show training validation data respective rbf network mappings table simulation parameters mean square test errors fi ms error conclusions presented general methodology estimating jointly noise variance parameters number parameters rbf model adopting bayesian model reversible jump mcmc algorithm perform necessary integrations demonstrated method accurate contrary previous reported results experiments indicate model robust respect specification prior addition obtained parsimonious rbf networks better approximation errors ones previously reported literature many avenues research include estimating type basis functions performing input variable selection considering noise models extending framework sequential scenarios possible solution first problem formulated using reversible jump mcmc flamework variable selection schemes also implemented via reversible jump mcmc algorithm presently working sequential version algorithm allows us perform model selection non stationary environments
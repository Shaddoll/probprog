abstract suppose given dataset drawn underlying probability distribution want estimate simple subset input space probability test point drawn lies outside equals priori specified propose method approach problem trying estimate function positive negative complement functional form given kernel expansion terms potentially small subset training data regularized controlling length weight vector associated feature space provide theoretical analysis statistical performance algorithm algorithm natural extension support vector algorithm case unlabelled data introduction recent years new set kernel techniques supervised learning developed specifically support vector sv algorithms pattern recognition regression estimation solution inverse problems received considerable attention attempts transfer idea using kernels compute inner products feature spaces domain unsupervised learning problems domain however less precisely specified generally characterized estimating functions data tell something interesting underlying distributions instance kernel pca characterized computing functions training data produce unit variance outputs minimum norm feature space another kernel based unsupervised learning technique regularized principal manifolds computes functions give mapping onto lower dimensional manifold minimizing regularized quantization error clustering algorithms examples unsupervised learning techniques kernelized extreme point view unsupervised learning estimating densities clearly knowledge density would allow us solve whatever problem solved basis data present work addresses easier problem support vector method novelty detection proposes algorithm computes binary function supposed capture regions input space probability density lives support function data live region function nonzero line vapnik principle never solve problem general one actually need solve moreover applicable also cases density data distribution even well defined singular components part motivation present work paper turns considerable amount prior work statistical literature discussion cf full version present paper algorithms first introduce terminology notation conventions consider training data number observations set simplicity think compact subset let feature map map dot product space dot product image computed evaluating simple kernel ri ri gaussian kernel tlx yl indices understood range compact notation bold face greek letters denote dimensional vectors whose components labelled using normal face typeset remainder section shall develop algorithm returns function takes value small region capturing data points elsewhere strategy map data feature space corresponding kernel separate origin maximum margin new point value determined evaluating side hyperplane falls feature space via freedom utilize different types kernel functions simple geometric picture corresponds variety nonlinear estimators input space separate data set origin solve following quadratic program llwll min lt subject xi parameter whose meaning become clear later since nonzero slack variables penalized objective function expect solve problem decision function sgn ri positive examples xi contained training set sv type regularization term still small actual trade two goals controlled deriving dual problem using solution shown sv expansion sgn ik xi patterns xi nonzero ai called svs coefficients found solution dual problem min ot oziozjk xi xj subjectto ai eozi ij scholkopf lliamson smola shawe taylor platt problem solved standard qp routines however possess features sets apart generic qps notably simplicity constraints exploited applying variant smo developed purpose offset recovered exploiting cti upper lower bound corresponding pattern xi satisfies xi ctjk xi note approaches upper boundaries lagrange multipliers tend infinity second inequality constraint becomes void problem resembles corresponding hard margin algorithm since penalization errors becomes infinite seen primal objective function shown data set separable origin algorithm find unique supporting hyperplane properties separates data origin distance origin maximal among hyperplanes hand approaches constraints alone allow one solution cti upper bound case kernels integral normalized versions decision function corresponds thresholded parzen windows estimator conclude section note one also use balls describe data feature space close spirit algorithms hard boundaries soft margins certain classes kernels gaussian rbf ones corresponding algorithm shown equivalent one theory section show parameter characterizes fractions svs outliers proposition following state robustness result soft margin proposition error bounds theorem results proofs reported full version present paper use italic letters denote feature space images corresponding patterns input space xi proposition assume solution satisfies following statements hold upper bound fraction outliers ii lower bound fraction svs iii suppose data generated independently distribution contain discrete components suppose moreover kernel analytic nonconstant probability asymptotically equals fraction svs fraction outliers proof based constraints dual problem using fact outliers must lagrange multipliers upper bound proposition local movements outliers parallel change hyperplane move subject generalization goal bound probability novel point drawn underlying distribution lies outside estimated region certain margin start introducing common tool measuring capacity class functions map definition let pseudo metric space let subset set cover every exists covering number fa minimal cardinality cover finite cover defined distance function differs metric semidefinite support vector method novelty detection idea finite approximate respect pseudometric use distance finite sample pseudometric space functions dx maxi xi xi let supx xt fax logarithms base theorem consider distribution suppose xl generated probability sample find xi log consider possibility small number points xi fails exceed corresponds non zero slack variable algorithm take llwll use class linear functions feature space application theorem well known bounds log covering numbers class let real valued function space fix tl define max similarly training sequence define zex theorem fix consider fixed unknown probability distribution input space class real valued functions range probability randomly drawn training sequences size log ae log xf log log theorem bounds probability new point falling region value less complement estimate support distribution choice gives trade size region bound holds increasing increases size region size probability holds increasing decreases size log covering numbers result shows bound probability points falling outside region estimated support quantity involving ratio log covering numbers bounded fat shattering dimension scale proportional number training examples plus factor involving norm slack variables stronger related results given since bound involves square root ratio pollard dimension fat shattering dimension tends number training examples output algorithm described sec function aik zi greater equal example xi though non linear input space function fact linear feature space defined kernel time norm weight vector given arx apply theorem function class linear functions feature space norm bounded assume fixed hence support distribution set bound gives probability randomly generated point falling outside set terms log covering numbers function class sum slack variables since log covering numbers scholkopf qlliamson smola shawe taylor platt scale class ff bounded log gives bound terms norm weight vector ideally one would like allow chosen value determined perhaps fixed fraction value could obtained another level structural risk minimisation possible values least mesh possible values result beyond scope current preliminary paper form result would similar theorem larger constants log factors whilst premature give specific theoretical recommendations practical use yet one thing clear bound generalize novel data decision function used employ threshold corresponds nonzero experiments apply method artificial real world data figure displays toy examples shows parameter settings influence solution next describe experiment usps dataset handwritten digits database contains digit images size last constitute test set trained algorithm using gaussian kernel width common value svm classifiers data set cf test set used identify outliers folklore community usps test set contains number patterns hard impossible classify due segmentation errors mislabelling experiment augmented input patterns ten extra dimensions corresponding class labels digits rationale disregarded labels would hope identify mislabelled patterns outliers fig shows worst outliers usps test set note algorithm indeed extracts patterns hard assign respective classes experiment took seconds pentium ii running mhz used value width frac svs ols margin llwll figure first two pictures single class svm applied two toy problems domain note cases least fraction examples estimated region cf table large value causes additional data points upper left corner almost influence decision function smaller values third picture points cannot ignored anymore alternatively one force algorithm take outliers account changing kernel width fourth picture using data effectively analyzed different length scale leads algorithm consider outhers meaningful points support vector method novelty detection figure outliers identified proposed algorithm ranked negative output svm argument sgn decision function outputs convenience units written underneath image italics alleged class labels given bold face note examples difficult either atypical even mislabelled discussion one could view present work attempt provide algorithm line vapnik principle never solve problem general one one actually interested situations one interested detecting novelty always necessary estimate full density model data indeed density estimation difficult several respects mathematically speaking density exist underlying probability measure possesses absolutely continuous distribution function general problem estimating measure large class sets say sets measureable borel sense solvable discussion see therefore need restrict making statement measure sets given small class sets simplest estimator accomplishing task empirical measure simply looks many training points fall region interest algorithm opposite starts number training points supposed fall region estimates region desired property often many regions solution becomes unique applying regularizer case enforces region small feature space associated kernel course implies measure smallness sense depends kernel used way different method regularizes feature space similar problem however appears density estimation already done input space let denote density perform nonlinear coordinate transformation input domain density values change loosely speaking remains constant da transformed directly estimating probability measure regions faced problem regions automatically change accordingly attractive property measure smallness chose use also placed context regularization theory leading interpretation solution maximally smooth sense depends specific kernel used main inspiration approach stems earliest work vapnik collaborators proposed algorithm characterizing set unlabelled data points separating origin using hyperplane however quickly moved two class classification problems terms algorithms theoretical development statistical learning theory originated days algorithmic point view identify two shortcomings original approach may caused research direction stop three decades firstly original scholkopf vfi lliamson smola shawe taylor platt algorithm limited linear decision rules input space secondly way dealing outliers conjunction restrictions indeed severe generic dataset need separable origin hyperplane input space two modifications incorporated dispose shortcomings firstly kernel trick allows much larger class functions nonlinearly mapping high dimensional feature space thereby increases chances separability origin particular using gaussian kemel separation exists data set xt see note xi xj thus dot products positive implying mapped patterns lie inside orthant moreover since xi xi unit length hence separable origin second modification allows possibility outliers incorporated softness decision rule using trick thus obtained direct handle fraction outliers believe approach proposing concrete algorithm well behaved computational complexity convex quadratic programming problem far mainly studied theoretical point view abundant practical applications turn algorithm easy use black box method practicioners questions like selection kernel parameters width gaussian kernel tackled expectation theoretical results briefly outlined paper provide foundation formidable task acknowledgement part work supported arc dfg ja done bs australian national university gmd first supported grant deutsche forschungsgemeinschaft sm thanks ben david bishop schn rr tipping helpful discussions references ben david lindenbaum learning distributions density levels paradigm learning without teacher journal computer system sciences sch kopf burges vapnik extracting support data given task fayyad uthurusamy editors proceedings first international conference knowledge discovery data mining aaai press menlo park ca sch kopf platt shawe taylor smola williamson estimating support high dimensional distribution tr msr microsoft research redmond wa sch kopf smola mtiller kernel principal component analysis sch kopf burges smola editors advances kernel methods support vector learning mit press cambridge sch kopf williamson smola shawe taylor single class support vector machines buhmann maass ritter tishby editors unsupervised learning dagstuhl seminar report pages smola williamson mika sch kopf regularized principal manifolds computational learning theory th european conference volume lecture notes artificial intelligence pages springer tax duin data domain description support vectors verleysen editor proceedings esann pages brussels facto vapnik statistical learning theory wiley new york vapnik lerner pattern recognition using generalized portraits avtomatika telemekhanika
abstract consider problem learning grid based map using robot noisy sensors actuators compare two approaches online em map treated fixed parameter bayesian inference map matrix valued random variable show even simple example online em get stuck local minima causes robot get lost resulting map useless contrast bayesian approach maintaining multiple hypotheses much robust introduce method approximating bayesian solution called rao blackwellised particle filtering show approximation coupled active learning strategy fast accurate introduction problem getting mobile robots autonomously learn maps environment widely studied see collection recent papers basic difficulty robot must know exactly problem called localization update right part map however know robot must already map relying dead reckoning alone integrating motor commands unreliable noise actuators slippage drift one obvious solution use em alternate estimating location given map step estimating map given location step indeed approach successfully used several groups however works trajectory robot specified hand map learned line fully autonomous operation cope dynamic environments map must learned online consider two approaches online learning online em bayesian inference murphy figure pomdp represented graphical model lt location label th grid cell action zt observation dotted circles denote variables em treats parameters one dimensional grid binary labels white black two dimensional grid four labels closed doors open doors walls free space treat map random variable section show bayesian approach lead much better results online em unfortunately computationally intractable section discuss approximation based rao blackwellised particle filtering model precisely define model use paper similar much simpler occupancy grid model map defined grid cell label represents robot would see point formally map time vector discrete random variables mr nl course map observed directly robot location lt nl observed zt label cell robot current location na action chosen robot time conditional independence assumptions making illustrated figure start considering simple one dimensional grid shown figure two actions move right move left two labels sufficiently small perform exact bayesian inference later generalize two dimensions prior location delta function mass first left cell independent transition model location follows pr lt lt pa ifj ifj iotherwise pa probability successful action pa probability robot wheels slip analogous equation case note possible pass rightmost cell robot use information help localize prior map product priors cell uniform could model correlation neighboring cells using markov random field although computationally expensive transition mot el map product transition models cell defined follows bayesian map learning dynamic environments probability becomes vice versa pc probability change hence probability cell label remains finally observation model pr zt mt ml mnl lt po mi po otherwise po probability succesful observation po probability classification error another way writing useful later introduce dummy deterministic variable following distribution pr klmt ml mnl lt mi otherwise thus acts like multiplexer selecting component mt determined gate lt output multiplexer passed noisy channel flips bits probability po produce zt bayesian learning compared em simplicity assume parameters po pa pc known section use po pa pc world somewhat slippery static appearance state estimation problem compute belief state pr lt mtlyl yt zt evidence time equivalent performing online inference graphical model shown figure unfortunately even though assumed components mt priori independent become correlated virtue sharing common child zt since true location robot unknown cells possible causes observation compete explain data hence hidden variables become coupled belief state size nl world static pc treat fixed unknown parameter combined noisy sensor model define hmm following observation matrix deaf pr zt kilt pr zt learn using em assume hmm transition matrix independent map encodes known topology grid robot move neighboring cell matter label lift restriction example formulate online version em follows use fixed lag smoothing sliding window length compute expected sufficient statistics ess observation matrix within window follows lrlt lt pr lr ilyx compute using forwards backwards algorithm using lt xlt prior initial condition known prior thus cost per time step wn step normalize row ot decay constant get new estimate need downweight previous ess since computed using date parameters addition exponential forgetting allows us handle dynamic environments discuss variations algorithm murphy figure full joint posterior mt yl axis page represent maps every cell every cell respectively mode map corresponds correct pattern estimated map light cells likely contains os correct pattern light bars odd rows marginals exact joint online em offline em window length increases past locations allowed look future data hence estimates become accurate however space time requirements increase nevertheless occasions even maximum window size looking way back perform poorly greedy hill climbing nature em simple example consider environment shown figure suppose robot starts cell keeps going right comes end corridor heads back home suppose single slippage error actual path observation sequence robot follows lt zt study effect sequence computed pr mt ltlyl applying junction tree algorithm graphical model figure marginalized lt compute posterior mt see figure modes corresponding possible bit patterns unobserved cells time step robot thinks moving one step right hence robot thinks cell observes tries move right knows remain cell since robot knows boundaries hence almost confident cell observes contradicts previous observation two possible explanations sensor error motor error likely depends relative values sensor noise po system noise pa experiments found motor error hypothesis much likely hence mode posterior jumps wrong map right map furthermore robot returns familiar territory able better localize see figure continues learn map even far away cells correlated figure entry cell becomes sharper even robot returns cell compare bayesian solution em online em smoothing able learn correct map adding smoothing maximum window size wt improve matters still unable escape local bayesian map learning dynamic environments figure estimated location light cells likely contain robot optimal bayes solution marginalizes map dead reckoning solution ignores map notice blurry online em solution using fixed lag smoothing maximal window length minimum shown figure tried various values decay rate found made little difference wrong map robot gets lost return journey see figure offline em hand well shown figure although initial estimate location see figure rather diffuse updates map use benefit hindsight figure must rao blackwellised particle filtering although bayesian solution exhibits desirable properties running time exponential size environment section discuss sequential monte carlo algorithm called particle filtering also known sir filtering bootstrap filter condensation algorithm survival fittest etc see recent reviews particle filtering pf already successfully applied problem global robot localization however case state space dimension unknowns position robot orientation case state space discrete dimension nl since need keep track map well robot location ignore orientation paper particle filtering inefficient high dimensional spaces key observation makes tractable context known posterior mt would factored hence mt marginalized analytically need sample lt idea known statistics literature raoblackwellisation detail approximate posterior time using set weighted particles particle specifies trajectory corresponding conditionally factored representation mt rii mt denote th particle time note need actually store complete trajectories need recent value approach take essentially one used conditional linear gaussian models except replace kalman filter update one exploits conditionally factored representation mt particular algorithm follows particle ns following sample proposal distribution discuss update component map separately using pr pr xlmt pr lm murphy figure results using particles results using bk update weights defined ua lw resample ns particles normalised weights using liu residual ns consider two proposal resampling algorithm set oat distributions first simple one uses transition model predict new location pr lt llb case incremental weight oc zt optimal proposal distribution one minimizes variance importance weights takes recent evidence account shown form pr incremental weight ut oc lb computing requires marginalizing done nl time details omitted figure show results applying algorithm problem section seen approximates exact solution closely using particles results shown particular random number seed seeds produce qualitatively similar results indicating particles fact sufficient case obviously increase number particles error variance decrease running time increases linearly question many particles use difficult one depends noise parameters structure environment every cell unique label localization easy since sampling trajectories number hypotheses hence number particles needed grows exponentially time example robot able localize quite accurately reached end corridor hypotheses died general number particles depend length longest cycle environment need use active learning ensure tractability dynamic two dimensional grid world figure chose actions maximize expected discounted reward using policy iteration reward visiting cell lt mt lt mt normalized entropy hence robot lost lt robot try visit cell certain see better approach otherwise try explore uncertain cells learning map robot spends time visiting doors keep knowledge state open closed date briefly consider alternative approximate inference algorithms examining graphical structure model see figure see identical bayesian map learning dynamic environments factorial hmm ignoring inputs unfortunately cannot use variational approximation assume conditional gaussian observation model whereas almost deterministic another popular ap proximate inference algorithm dynamic bayes nets dbns bk algorithm entails projecting joint posterior time onto product marginals representation lt mt mt nl iyl ltly ii mt lyl using factored prior bayesian updating time given factored prior compute factored posterior time conditioning lt averaging found bk method poorly problem see figure ignores correlation cells course possible use pairwise higher order marginals tightly coupled sets variables unfortunately running time exponential size largest marginal case mr variables coupled acknowledgments would like thank nando de freitas helping get particle filtering work sebastian thrun interesting discussion conference stuart russell encouraging compare em work supported grant number onr references boyen koller approximate learning dynamic models nips boyen koller tractable inference complex stochastic processes uai chen liu mixture kalman filters submitted doucet godsill andrieu sequential monte carlo sampling methods bayesian filtering statistics computing fox burgard dellaeft thrun monte carlo localization efficient position estimation mobile robots aaai fox burgard thrun active markov localization mobile robots robotics autonomous systems ghahramani jordan factorial hidden markov models machine learning koenig simmons unsupervised learning probabilistic models robot navigation icra kortenkamp bonasso murphy editors artificial intelligence mobile robots case studies successful robot systems mit press liu chen sequential monte carlo methods dynamic systems jasa shatkay kaelbling learning topological maps weak local odometric information ijcai thrun burgard fox probabilistic approach concurrent mapping localization mobile robots machine learning
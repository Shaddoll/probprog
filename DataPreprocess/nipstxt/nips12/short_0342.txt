abstract effective methods capacity control via uniform convergence bounds function expansions largely limited support vector machines good bounds obtainable entropy number approach extend methods systems expansions terms arbitrary parametrized basis functions wide range regularization methods covering whole range general linear additive models achieved data dependent analysis eigenvalues corresponding design matrix introduction model selection criteria based vapnik chervonenkis vc dimension known difficult obtain worst case often tight yet theoretical appeal providing bounds assumptions made recently new methods developed able provide better characterization complexity function classes vc dimension moreover easily obtainable take advantage data hand employ concept luckiness techniques however limited linear functions expansions functions terms kernels happens case support vector sv machines paper show previously mentioned techniques extended expansions terms arbitrary basis functions covering large range practical algorithms general linear models weight decay sparsity regularization regularization networks entropy regularization information criterion support vector machines support vector machines carry effective means capacity control minimizing weighted sum training error remp xi yi xi regularization term wl minimize regularized risk functional rreg reinpill aq xi yi xi wll xm denotes training set ym corresponding labels target values corresponding domains regularization constant cost function given nonlinear case map feature space finally dot products feature space written called mercer kernel denotes dimensional space vectors define spaces follows vector spaces identical addition endowed norms ilxlle ilxllp ixjl oo ilxlle maxj ixjl forp oo write ep furthermore let ut ilxllt unit ball model selection purposes one wants obtain bounds richness map sx xm xm restricted unit ball radius equivalent choosing appropriate value increase decreases vice versa richness sx specifically mean covering numbers sx aug set sx au standard colt notation mean au moo min exists set aug min iiz zillt see details carrying model selection case advanced methods exploit distribution mapped feature space thus spectral properties operator sx analyzing spectrum gram matrix gij ij gij xi possible since xi xj seen dot product xi xj mapped feature space xi xj xi xj property whilst true sv machines mercer kernels hold general case expanded terms less arbitrary basis functions smola shawe taylor sch lkopf lliamson basic problems one basic problem expanding aifi cti ir fi arbitrary functions immediately obvious regard dot product feature space one show vc dimension set linearly independent functions hence one would intuitively try restrict class admissible models controlling number basis functions terms expanded consider extreme case addition basis functions fi defined previously given basis functions linearly independent previous ones differ fi small domain filx fjlx since new set functions linearly independent vc dimension joint set given hand hardly data occurs domain one would notice difference fi ft words joint system functions would behave initial system basis functions analogous situation occurs fi small constant bounded say within case additional effect set functions fj would hardly noticable still joint set functions would count one vc dimension already indicates simply counting number basis functions may good idea figure left right initial set functions dots axis indicate sampling points additional set functions differ globally small amount additional set functions differ locally however large amount spectrum corresponding design matrices bars denote cases corresponding order note difference quite small hand spectra corresponding design matrices see figure similar suggests use latter model selection criterion finally practical problem capacity control sv machines carried minimizing length weight vector feature space cannot done analogous way either several ways consider three appeared literature exist effective algorithms example weight decay define cti coefficients cti function expansion constrained ball case consider following operator moo ct xi xm xx ct xm ct fa fi fn fij fi xj ct cti ctn auto entropy regularization information criterion example sparsity regularization case coefficients function expansion constrained ball enforce sparseness thus mapping except auto similar expansions encountered boosting linear programming machines example regularization networks finally one could set positive definite matrix instance ij could obtained regularization operator penalizing non smooth functions case lives inside dimensional ellipsoid substituting one reduce setting case example different set basis functions consider evaluation operator given xm aut fij fi xj example example support vector machines important special case example support vector machines qij xi xj fi xi hence hence possible values generated support vector machine written xm xm aut entropy numbers covering numbers characterize difficulty learning elements function class entropy numbers operators used compute covering numbers easily tightly traditional techniques based vc like dimensions fat shattering dimension knowing el sx see definition tells one log effective class functions used regularised learning machines consideration section summarize basic definitions results presented ith entropy number el set corresponding metric precision approximated elements exists fi fi el hence el functional inverse covering number entropy number bounded linear operator normed linear spaces defined el ei ua metric induced ii lib dyadic entropy numbers el defined el latter quantity often convenient deal since corresponds log covering number make use following three results entropy numbers identity mapping diagonal operators products operators let pl ti ti ldpx pl dp following result due schtitt constants obtained proposition entropy numbers identity operators el ida log el id log smola shawe taylor sch kopf iamson proposition carl stephani let banach spaces note latter two inequalities follow directly fact ilrii definition operator norm iirii proposition let al aj dx ajxj xj diagonal operator generated sequence aj sup main result state main theorem gives bounds entropy numbers first three examples model selection described since support vector machines special case example deal separately proposition let expanded linear combination basis functions aifi coefficients restricted one convex sets described examples moreover denote fij fj xi design matrix particular sample regularization matrix case example following bound holds case weight decay ex li et log rnll et case weight sparsity regularization ex et log mllx eta og finally case regularization networks ex ff log diagonal scaling operator matrix entries eigenvalues sorted decreasing order matrix ff case examples fq case example entropy number readily bounded terms al using one see first setting weight decay special case third one namely identity matrix proof proof relies factorization following way first consider equivalent operator mapping perform singular value decomposition latter vew operators norm contains singular values singular values fq entropy regularization information criterion respectively ff fq consequently factorize diagram latter however identical square root eigenvalues finally order compute entropy number overall operator one use factorization sx id vew id vewid example apply proposition several times also exploit fact singular value decompositions vii ilwll present theorem allows us compute entropy numbers thus complexity class functions current sample going back examples section led large bounds vc dimension one see new result much less susceptible modifications addition change eigenspectrum design matrix significantly possibly doubling nominal value singular values functions differ fi slightly consequently also bounds change significantly even though number basis functions doubled also note current error bounds reduce results sv case oij fij xi xj design matrix regularization matrix determined kernels therefore fq thus analysis singular values fq leads analysis eigenvalues kernel matrix exactly done dealing sv machines error bounds use result need bound expected error hypothesis terms empirical error training error observed entropy numbers use theorem small modification theorem let set linear functions described previous examples en sx corresponding bound observed entropy numbers dataset moreover suppose fixed threshold sgn correctly classifies set margin min ei finally let min sx confidence drawn randomly probability distribution expected error sgn bounded log log log proof essentially identical theorem omitted also shows compute sx efficiently including explicit formula evaluating et discussion showed improved bounds could obtained entropy numbers wide class popular statistical estimators ranging weight decay sparsity regularization smola shawe taylor sch lkopf lliamson sv machines special case thereof results given way directly useable practicioners without tedious calculations vc dimension similar combinatorial quantities particular method ignores nearly linear dependent basis functions automatically finally takes advantage favourable distributions data using observed entropy numbers base stating bounds true entropy numbers respect function class consideration whilst leads significantly improved bounds achieved improvement approximately two orders magnitude previous vc type bounds involving radius data weight vector iiwll experiments expected risk bounds still good enough become predictive indicates possibly rather using standard uniform convergence bounds used previous section one might want use techniques pac bayesian treatment recently suggested herbrich graepel combination bounds eigenvalues design matrix acknowledgements work supported australian research council grant deutsche forschungsgemeinschaft sm references alon ben david cesa bianchi haussler scale sensitive dimensions uniform convergence learnability acm carl stephani entropy compactness approximation operators cambridge university press cambridge uk chen donoho saunders atomic decomposition basis pursuit technical report department statistics stanford university girosi jones poggio regularization theory neural networks architectures neural computation horn johnson matrix analysis cambridge university press cambridge sch kopf shawe taylor smola williamson generalization bounds via eigenvalues gram matrix technical report nc tr neurocolt university london uk shawe taylor williamson generalization performance classifiers terms observed covering numbers proc eurocolt williamson smola sch kopf generalization performance regularization networks support vector machines via entropy numbers compact operators neurocolt nc tr royal holloway college williamson smola sch kopf maximum margin miscellany typescript
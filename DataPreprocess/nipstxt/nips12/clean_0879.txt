abstract fundamental problem modeling chaotic time series data minimizing short term prediction errors guarantee match reconstructed attractors model experiments introduce modeling paradigm simultaneously learns short term predict locate outlines attractor new way nonlinear principal component analysis closed loop predictions constrained stay within outlines prevent divergence attractor learning exceptionally fast parameter estimation sample laser data santa fe time series competition took less minute mhz pentium pc introduction focus following objective given set experimental data assumption produced deterministic chaotic system find set model equations produce time series identical chaotic characteristics chaotic attractor common approach consists two steps identify model makes accurate shortterm predictions generate long time series model compare nonlinear dynamic characteristics time series original measured time series principe et al found many cases model make good short term predictions team chaotic attractor method would greatly improved could minimize directly difference reconstructed attractors model generated measured data instead minimizing prediction errors however cannot reconstruct attractor without first prediction model research focused optimize step step example important optimize prediction horizon model reduce complexity much possible way possible team attractor benchmark laser time series data santa fe delftchemtech chemical reactor engineering lab julianalaan bl delft netherlands http www cpt stm tudelft nl cpt cre research bakker bakker schouten coppens takens giles bleek time series competition training neural network problem noticed attractor model fluctuated good match complete mismatch one iteration another able circumvent problem selecting exactly model matches attractor however carrying simulations found neglected unfortunate phenomenon really fundamental limitation current approaches important development work principe et al use kohonen self organizing maps soms create discrete representation state space system creates partitioning input space becomes infrastructure local linear model construction partitioning enables verify model input near original data detect model extrapolating without keeping training data set model propose different partitioning input space used learn outlines chaotic attractor means new way nonlinear principal component analysis pca ii enforce model never predict outside outlines nonlinear pca algorithm inspired work kambhatla leen local pca partition input space perform local pca region unfortunately introduces discontinuities neighboring regions resolve introducing hierarchical partitioning algorithm uses fuzzy boundaries regions partitioning closely resembles hierarchical mixtures experts jordan jacobs sec put forward fundamental problem arises trying learn chaotic attractor creating short term prediction model sec describe proposed partitioning algorithm sec outlined partitioning used learn outline attractor defining potential measures distance attractor sec show modeling results toy example logistic map serious problem laser data santa fe time series competition section concludes attractor learning dilemma imagine experimental system chaotic attractor time series noise free measurements taken system data used fit parameters model fit fft nonlinear function contains adjustable parameters positive constant happens fit parameters nonlinear least squares regression model stable closed loop long term prediction converge attractor one represented measurements figure shows result test diks et al compares difference model measured attractor figure shows neural network trained predict chaotic data model quickly converges measured attractor one iteration another match attractors lost understand causes instability imagine try fit parameters model real system point attractor state system attracting value clearly measurements taken system contain information oo training progress co rerati ns figure diks test monitoring curve neural network model trained data experimental chaotic pendulum robust learning chaotic attractors estimate fit model parameters non robust linear least squares may assigned value largest eigenvalue happens greater zero model unstable linear model problem solved long time ago introduction singular value decomposition still need nonlinear counterpart technique particular since work flexible models designed fit wide variety nonlinear shapes see example early work lapedes farber akeady common practice control complexity nonlinear models pruning regularization unfortunately methods always solve attractor learning problem since good chance nonlinear term explains lot variance one part state space causes instability attractor without affecting one stepahead prediction accuracy elsewhere secs introduce new method nonlinear principal component analysis detect prevent unstable behavior split fit algorithm nonlinear regression procedure section form basis nonlinear principal component algorithm sec consists partitioning input space ii local linear model region iii fuzzy boundaries regions ensure global smoothness partitioning scheme outlined procedure procedure partitioning input space start entire set input data determine direction largest variance perform singular value decomposition product uz take eigenvector column largest singular value diagonal split data two subsets called clusters creating plane perpendicular direction largest variance center gravity next select cluster largest sum squared error split next recursively apply stopping criteria met figures show examples partitioning disadvantage dividing regression problems localized subproblems pointed jordan jacobs spread data region much smaller spread data whole increase variance model parameters since always split perpendicular direction maximum variance problem minimized partitioning written binary tree non terminal node split terminal node cluster procedure creates fuzzy boundaries clusters procedure creating fuzzy boundaries input enters tree top partfioning tree euclidean distance splitting hyperplane divided bandwidth fl split passed sigmoidal function range results share erin subset side splitting plane share subset previous step carded non terminal nodes tree bakker schouten coppens takens giles bleek membership ff subset terminal node computed taking product previously computed shares along path terminal node top tree would make parameters adjustable orientation splitting hyperplanes ii bandwidths iii local linear model parameters model structure would identical hierarchical mixtures experts jordan jacobs however akeady fixed hyperplanes use procedure compute bandwidths procedure computing bandwidths bandwidths terminal nodes taken constant use confidence limit normal distribution times variance subset last split direction eigenvector last split bandwidths depend input computed climbing upward tree bandwidth node computed weighted sum rs right left child implicit formula fin fit fir depend fin starting initial guess fin fit else fin fir formula solved iterations procedure designed create large overlap neighboring regions almost overlap non neighboring regions remains fitted set local linear models thej th output split fit model given input computed tp ajzp bj cand contain linear model parameters subset number clusters determine parameters local linear models one global fit linear parameters however prefer locally optimize parameters two reasons makes possible locally control stability attractor principal component analysis sec ii computing time linear regression problem regressors scales would adopt global fitting would scale linearly growing model regression problem would quickly become intractable use following iterative local fitting procedure instead procedure iterative local fitting initialize matrix residuals zero number outputs number data cluster estimate linear model parameters akeady exists input vector add matrix residuals otherwise add yj thej th element desired output vector sample least squares fit linear model parameters cluster predict current residuals subtract new estimate cluster repeat fitting several times default simulations found fast optimization method converges global minimum repeated many times neural network training often better use early stopping prediction error independent test set starts increase robust learning chaotic attractors nonlinear principal component analysis learn chaotic attractor single experimental time series use method delays state consists delays taken time series embedding dimension must chosen large enough ensure contains sufficient information faithful reconstruction chaotic attractor see takens typically results mdimensional state space measurents covering much lower dimensional non linearly shaped subspace creates danger pointed sec stability model directions perpendicular low dimensional subspace cannot guaranteed split fit algorithm sec learn non linear shape low dimensional subspace state system escapes subspace use algorithm redirect state nearest point subspace see malthouse limitations existing nonlinear pca approaches obtain low dimensional subspace proceed according procedure procedure learning low dimensional subspace augment output model dimensional state model learn predict input cluster perform singular value decomposition create set principal directions sorted order decreasing explained variance result decomposition also used step procedure allow local linear model cluster use mre principal directions define potential squared euclidian distance state prediction model potential implicitly defines lower dimensional subspace state subspace zero increase distance subspace model learned predict input small error meaning tried reduce much possible exactly points state space training data sampled words low input close one original points training data set split fit algorithm analytically compute gradient dp since evaluation split fit model involves backward computing bandwidths forward pass computing memberships gradient algorithm involves forward backward pass tree gradient used project states nonlinear subspace onto subspace figure projecting two dimensional data onedimensional self intersecting subspace colorscale represents potential white indicates bakker schouten coppens takens giles bleek one newton rhapson iterations figure illustrates algorithm problem creating one dimensional representation number training set consists clean samples fig shows set noisy inputs projected subset split fit model onto one dimensional subspace note center cannot well represented one imensional space leave development algorithm automatically detects optimum local subspace dimension future research application examples figure learning attractor twoinput logistic map order creation splits indicated colorscale represents potential white indicates first show nonlinear principal component analysis result toy example logistic map zt zt zt use model zt zt prediction depends one previous output lower dimensional space attractor confined however allow output depend single delay create possibility unstable behavior figure shows well split fit algorithm learns one dimensional shape attractor creating five regions parabola slightly deformed seen white lines perpendicular attractor may solved increasing number splits next look laser data complex behavior chaotic systems caused interplay destabilizing stabilizing forces destabilizing forces make nearby points state space diverge stabilizing forces keep state system bounded process known stretching folding results attractor system set points state system visit transients died case laser data behavior clear cut destabilizing forces make signal grow exponentially increasing amplitude triggers collapse reinitiates sequence seen neural network based models study hard models cope sudden collapses without nonlinear subspace correction sec time figure laser data santa fe time series competition sample train data set followed iterated prediction model every prediction correction made keep see sec small plot shows correction robust learning chaotic attractors models tested grow without bounds one rise collapse sequences surprising training data set contains three examples collapse figure shows solved subspace correction every time model grow infinity high potential detected depicted fig state system directed nearest point subspace learned nonlinear principal component analysis trial error selected embedding dimension reduced dimension mrea split fit model starts single dataset grown subsets point error sample train set still decreasing rapidly error independent sample test set increased compared reconstructed attractors model measurements using samples closed loop generated samples measured data significant difference two could detected diks test conclusions present algorithm robustly models chaotic attractors simultaneously learns make accurate short term predictions outlines attractor closed loop prediction mode state system corrected every prediction stay within outlines algorithm fast since main computation least squares fit set local linear models implementation largest matrix stored number data number clusters see many applications attractor learning split fit algorithm used fast learning alternative neural networks new form nonlinear pca useful data reduction object recognition envisage apply technique wide range applications control modeling chaos fluid dynamics problems finance biology fluid dynamics acknowledgements work supported netherlands foundation chemical research son financial aid netherlands organization scientific research nwo
abstract analyz compar well known gradient descent algorithm new algorithm call exponenti gradient algorithm train singl neuron arbitrari transfer function algorithm easili gener larger neural network gener gradient descent standard back propag algorithm paper prove worstcas loss bound algorithm singl neuron case sinc local minima make difficult prove worst case bound gradient base algorithm must use loss function prevent format spuriou local minima defin match loss function strictli increas differenti transfer function prove worst case loss bound transfer function correspond match loss exampl match loss ident function squar loss match loss logist sigmoid entrop loss differ structur bound two algorithm indic new algorithm perform gradient descent input contain larg number irrelev compon helmbold kivinen warmuth
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import glob    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_cleaner(doc):\n",
    "    '''\n",
    "    Clean and preprocess a document.\n",
    "    \n",
    "    1. Use regex to remove all special characters (only keep letters)\n",
    "    2. Make strings to lower case and tokenize / word split reviews\n",
    "    3. Remove English stopwords\n",
    "    \n",
    "    Return a list of words\n",
    "    '''\n",
    "    doc = re.sub(\"[^a-zA-Z]\", \" \",doc)\n",
    "    doc = doc.lower().split()\n",
    "    eng_stopwords = stopwords.words(\"english\")\n",
    "    for stopword in ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']:\n",
    "        eng_stopwords.append(stopword)\n",
    "    doc = [w for w in doc if not w in eng_stopwords]\n",
    "    ps = PorterStemmer()\n",
    "    ps_stems = []\n",
    "    for word in doc:\n",
    "        ps_stems.append(ps.stem(word))    \n",
    "    return(ps_stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean words in each document while keep every sentences\n",
    "txt_files = glob.glob(\"nipstxt/nips12/*.txt\")\n",
    "corpus = []\n",
    "for file in sorted(txt_files):\n",
    "    doc = []\n",
    "    with open(file, 'rt',encoding = \"ISO-8859-1\") as f:\n",
    "        body = False\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line == 'Abstract':\n",
    "                body = True\n",
    "            if line == 'References':\n",
    "                body = False\n",
    "            if body:\n",
    "                if line[-1] == '-':\n",
    "                    line = line.strip('-')\n",
    "                    doc.append(line)\n",
    "                else: \n",
    "                    line += ' '\n",
    "                    doc.append(line)\n",
    "    doc = ''.join(doc)\n",
    "    doc += ' '\n",
    "    corpus.append(doc)\n",
    "corpus = ''.join(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = corpus.split('.')\n",
    "for i, line in enumerate(corpus):\n",
    "    corpus[i] = doc_cleaner(line)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training word2vec model... \n"
     ]
    }
   ],
   "source": [
    "# Set values for various parameters\n",
    "num_features = 100    # Word vector dimensionality                      \n",
    "min_word_count = 0   # ignore all words with total frequency lower than this                       \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "\n",
    "print(\"Training word2vec model... \")\n",
    "model = word2vec.Word2Vec(corpus, workers=num_workers, \\\n",
    "           size=num_features, min_count = min_word_count, \\\n",
    "            window = context)\n",
    "\n",
    "\n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"100dim_0minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab length: 9712\n"
     ]
    }
   ],
   "source": [
    "vocab_tmp = list(model.wv.vocab)\n",
    "print('Vocab length:',len(vocab_tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = model.wv\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "fname = get_tmpfile(\"vectors.kv\")\n",
    "word_vectors.save(fname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.save_word2vec_format('word_vectors.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = KeyedVectors.load(fname, mmap='r')\n",
    "txt_files_clean = glob.glob(\"nipstxt/nips12/clean_*.txt\")\n",
    "for file in sorted(txt_files_clean):\n",
    "    doc_word_embed = np.array ([0]*100)\n",
    "    with open(file, 'rt',encoding = \"ISO-8859-1\") as f:\n",
    "        for line in f:\n",
    "            doc_words = line.split()\n",
    "            for word in doc_words:\n",
    "                doc_word_embed = np.vstack([doc_word_embed, word_vectors[word]])\n",
    "    np.savetxt('nipstxt/nips12we/'+'wordembed_'+file[-8:-4]+'.txt', doc_word_embed[1:,:], delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = KeyedVectors.load(fname, mmap='r')\n",
    "txt_files_clean = glob.glob(\"nipstxt/nips12/short_*.txt\")\n",
    "for file in sorted(txt_files_clean):\n",
    "    doc_word_embed = np.array ([0]*100)\n",
    "    with open(file, 'rt',encoding = \"ISO-8859-1\") as f:\n",
    "        for line in f:\n",
    "            doc_words = line.split()\n",
    "            for word in doc_words:\n",
    "                doc_word_embed = np.vstack([doc_word_embed, word_vectors[word]])\n",
    "    np.savetxt('nipstxt/nips12we/'+'short_wordembed_'+file[-8:-4]+'.txt', doc_word_embed[1:,:], delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

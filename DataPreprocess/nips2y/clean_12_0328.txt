abstract calcul lower bound size sigmoid neural network approxim continu function particular show approxim polynomi network size grow log degre polynomi bound valid input dimens independ number variabl result obtain introduc new method employ upper bound vapnik chervonenki dimens prove lower bound size network approxim continu function introduct sigmoid neural network known univers approxim one theoret result frequent cite justifi use sigmoid neural network applic statement one refer fact sigmoid neural network shown abl approxim continu function arbitrarili well numer result literatur establish variant univers approxim properti consid distinct function class approxim network architectur use differ type neural activ function respect variou approxim criteria see instanc see particular scarselli tsoi recent survey refer result mani other referenc construct mere exist proof provid upper bound network size assert good approxim possibl suffici mani network node avail howev partial answer question mainli aris practic applic given function mani network node need approxim much attent focus establish lower bound network size particular approxim function real far comput binari valu complex approxim continu function neural network function sigmoid network concern output valu network threshold yield result direct specif boolean function koiran show network use standard sigmoid activ function must size fl number input measur network size count input node follow maass establish larger lower bound construct binari valu function show standard sigmoid network requir fl mani network node comput function first work complex sigmoid network approxim continu function due dasgupta schnitger show standard sigmoid network node replac type activ function without increas size network polynomi yield indirect lower bound size sigmoid network term network type dasgupta schnitger also claim size bound sigmoid network layer approxim function sin ax paper consid problem use standard sigmoid neural network approxim polynomi show least fl log network node requir approxim polynomi degre small error norm bound valid arbitrari input dimens depend number variabl lower bound also obtain result binari valu function mention interpol correspond function polynomi howev requir grow input dimens yield lower bound term degre bound establish hold network number layer far know first lower bound result approxim polynomi comput point view simpl class function comput use basic oper addit multipl polynomi also play import role approxim theori sinc dens class continu function approxim result neural network reli approxim polynomi sigmoid network see obtain result introduc new method employ upper bound vapnik chervonenki dimens neural network establish lower bound network size first use vapnik chervonenki dimens obtain lower bound due koiran calcul mention bound size sigmoid network boolean function koiran method develop extend maass use similar argument anoth combinatori dimens paper deriv lower bound comput binari valu function koiran input maass input present new techniqu show lower bound obtain network approxim continu function rest two fundament result vapnik chervonenki dimens neural network one hand use construct provid koiran sontag build network larg vapnik chervonenki dimens consist gate comput certain arithmet function hand follow line reason karpinski macintyr deriv upper bound vapnikchervonenki dimens network estim khovanskil result due warren follow section give definit sigmoid network vapnikchervonenki dimens present lower bound result function approxim final conclud discuss open question schmitt sigmoid neural network vc dimens briefli recal definit sigmoid neural network vapnikchervonenki dimens see consid feedforward neural network certain number input node one output node node input node call comput node associ real number threshold edg label real number call weight comput network take place follow input valu assign input node comput node appli standard sigmoid sum wxxx wrxr xx xr valu comput node predecessor wr weight correspond edg threshold output valu network defin valu comput output node common approxim result mean neural network assum output node linear gate output sum wxxx wrxr clearli comput function finit set output rang output node may appli standard sigmoid well sinc sigmoid function consid refer network sigmoid neural network sigmoid function gener need satisfi much weaker assumpt definit natur gener network employ type gate make use linear multipl divis gate vapnik chervonenki dimens combinatori dimens function class defin follow dichotomi set partit two disjoint subset sx sx given set function map dichotomi sx say induc dichotomi sx sx say shatter induc dichotomi vapnikchervonenki vc dimens denot vcdim defin largest number set element shatter refer vc dimens neural network given term feedforward architectur direct acycl graph vc dimens class function obtain assign real number programm paramet gener weight threshold network subset thereof assum output valu network threshold obtain binari valu lower bound network size present lower bound size sigmoid network requir approxim polynomi first give brief outlin proof idea defin sequenc univari polynomi mean show construct neural architectur af consist variou type gate linear multipl divis gate particular gate comput polynomi architectur singl weight programm paramet weight threshold fix demonstr assum gate comput polynomi approxim sigmoid neural network suffici well architectur shatter certain set assign suitabl valu programm weight final step reason along line karpinski macintyr obtain via khovanski estim warren result upper bound vc dimens afn term number comput node note cannot directli appli theorem sinc deal divis gate compar bound cardin shatter set abl complex approxim continu function neural network figur network valu assign input node xx xa respect weight programm paramet network conclud lower bound number comput node thu network approxim polynomi let sequenc polynomi induct defin clearli uniqu defin everi readili seen degre main lower bound result made precis follow statement theorem sigmoid neural network approxim polynomi interv error lo norm must least comput node proof neural architectur af construct follow network four input node xx xa figur show network input valu assign input node order xa xx one weight consid programm paramet afn associ edg outgo input node denot comput node partit six level indic box figur level network let us first assum sake simplic comput real number exact three level label input node one output node comput call project ya level label ps px one input node output node level ps receiv constant input thu valu paramet network defin output valu level denot input valu level pa valu equal xx otherwis observ calcul schmitt therefor comput level implement use gate comput function pn show afn shatter set cardin let shown lemma exist pq andpq impli dichotomi everi pk pj pi pk pj pi sx note pj pi na valu comput given input valu therefor choos suitabl valu paramet afn network induc dichotomi word shatter shown lemma architectur weight chosen function fn comput network satisfi lim fn yx yn ya moreov architectur consist comput node linear multipl divis gate note size depend therefor choos suffici small implement project network comput node result network still shatter comput node implement three level label ii level number comput node comput respect assum comput node pn replac sigmoid network input paramet valu defin result network comput function note comput node pn programm paramet estim size accord theorem karpinski macintyr sigmoid neural network programm paramet comput node vc dimens ml gener result slightli abl appli readili seen proof theorem result also hold network addit contain linear multipl gate divis gate deriv bound take account gate comput divis say introduc defin equal new variabl see proceed thu network programm paramet comput node linear multipl divis sigmoid gate vc dimens ml particular number comput node vc dimens hand shown shatter set cardin sinc sigmoid network comput function pn sinc number linear multipl divis gate bound valu singl network comput pn must size least yield lower bound size sigmoid network comput thu far assum polynomi pn comput exactli sincb polynomi continu function sinc requir calcul finit set input valu result paramet valu chosen shatter approxim polynomi suffici straightforward analysi base fact output valu network toler close show pn approxim error complex approxim continu function neural network im norm result network still shatter set complet proof theorem statement previou theorem restrict approxim polynomi input domain howev result immedi gener arbitrari interv moreov remain valid multivari polynomi arbitrari input dimens corollari approxim polynomi degre sigmoid neural network approxim error loo norm requir network size log hold polynomi number variabl conclus open question establish lower bound size sigmoid network approxim continu function particular concret class polynomi calcul lower bound term degre polynomi main result alreadi hold approxim univari polynomi intuit approxim multivari polynomi seem becom harder dimens increas therefor would interest lower bound term degre input dimens result approxim error degre coupl natur one would expect number node grow fix function error decreas present know lower bound aim calcul constant bound practic applic valu indispens refin method use tighter result straightforward obtain number expect better lower bound obtain consid network restrict depth establish result introduc new method deriv lower bound network size one main argument use function approxim construct network larg vc dimens method seem suitabl obtain bound also approxim type function long comput power enough moreov method could adapt obtain lower bound also network use activ function gener sigmoid function ridg function radial basi function may lead new separ result approxim capabl differ type neural network order accomplish howev essenti requir small upper bound calcul vc dimens network acknowledg thank han simon help discuss work support part esprit work group neural comput learn ii neurocolt refer barron univers approxim bound superposit sigmoid function ieee transact inform theori schmitt chui li approxim ridg function neural network one hidden layer journal approxim theori cybenko approxim superposit sigmoid function mathemat control signal system dasgupta schnitger power approxim comparison activ function gile hanson cowan editor advanc neural inform process system page morgan kaufmann san mateo ca hornik approxim capabl multilay feedforward network neural network hornik stinchcomb white multilay feedforward network univers approxim neural network karpinski macintyr polynomi bound vc dimens sigmoid gener pfattian neural network journal comput system scienc khovanski fewnomi volum translat mathemat monograph american mathemat societi provid ri koiran vc dimens circuit complex proceed th annual ieee confer comput complex ccc page ieee comput societi press lo alamito ca koiran sontag neural network quadrat vc dimens journal comput system scienc kreinovich arbitrari nonlinear suffici repres function neural network theorem neural network leshno lin pinku schocken multilay feedforward network nonpolynomi activ function approxim function neural network maass noisi spike neuron tempor code comput power sigmoid neuron mozer jordan petsch editor advanc neural inform process system page mit press cambridg mhaskar neural network optim approxim smooth analyt function neural comput scarselli tsoi univers approxim use feedforward neural network survey exist method new result neural network warren lower bound approxim nonlinear manifold transact american mathemat societi
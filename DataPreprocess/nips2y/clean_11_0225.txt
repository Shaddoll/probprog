abstract describ unifi method prove rel loss bound onlin linear threshold classif algorithm perceptron winnow algorithm classif problem discret loss use total number predict mistak introduc continu loss function call linear hing loss employ deriv updat algorithm first prove bound linear hing loss convert discret loss introduc notion averag margin set exampl show rel loss bound base linear hing loss convert rel loss bound discret loss use averag margin introduct consid classic perceptron algorithm hypothesi algorithm trial linear threshold function determin weight vector tot instanc linear activ tot pass threshold function ar argument less threshold otherwis thu predict algorithm binari denot two class perceptron algorithm aim learn classif problem exampl form mr see exampl xt yt algorithm predict cr wr xr next instanc xr algorithm predict agre label yt instanc xr loss zero predict label disagre loss one call loss discret loss converg perceptron algorithm establish perceptron converg theorem second classic algorithm learn linear threshold function winnow algorithm nick littleston lit algorithm also maintain weight vector predict linear threshold function defin current weight vector tot howev updat weight vector tot wt support nsf grant ccr gentil arrnuth perform two algorithm radic differ perceptron winnow wt perceptron algorithm perform simpl addit updat paramet posit learn rate equal lt lie predict algorithm correct updat occur perceptron algorithm winnow updat conserv updat predict algorithm wrong algorithm overshot caus perceptron subtract current weight wt similarli algorithm undershot perceptron add rt current weight wt later interpret gradient loss function winnow use gradient updat done componentwis logarithm weight vector one also rewrit winnow updat wt wt exp gradient appear expon factor multipli old weight factor use correct weight right direct algorithm overshot algorithm good differ purpos gener speak incompar see kwa discuss kw framework introduc deriv simpl line learn updat framework appli varieti differ learn algorithm differenti loss function hkw kw updat alway deriv approxim solv follow minim problem wt argminw where wt rlloss yt ar xt loss denot chosen loss function set would discret loss differ predict algorithm rrr wt xt discret loss discontinu weight vector wt return point later discuss part minim problem paramet learn rate mention importantli wt diverg measur far wt diverg function two purpos motiv updat becom potenti function amort analysi use prove loss bound correspond algorithm use amort analysi context learn essenti goe back lit method deriv updat base diverg introduc kw diverg may seen regular term may also serv barrier function optim problem purpos keep weight particular region addit algorithm gradient descent perceptron algorithm use iiw diverg use potenti function proof perceptron converg theorem multipl updat algorithm winnow variou exponenti gradient algorithm use entropi base diverg potenti function hkw kw function minim differenti work well loss function convex differenti exampl linear regress loss function squar loss wt xt gtt minim diverg wt give widrow hoff updat variou exponenti gradient algorithm kw deriv way entrop diverg use instead howev case cannot differenti discret loss sinc discontinu ask loss function motiv perceptron winnow algorithm framework see loss function achiev continu linear hing loss averag margin gradient wt txt call loss linear hing loss hl believ key tool understand linear threshold algorithm perceptron winnow howev process chang discret loss hl also chang learn problem classif regress problem two version algorithm classif version regress version classif version predict binari label use linearli threshold predict loss function discret loss regress version hand predict next instanc xt linear activ wt xt classif problem label yt exampl regress problem label see version algorithm use rule updat weight vector wt anoth strong hint hl relat perceptron winnow come fact loss may seen limit case entrop loss use logist regress logist regress threshold function replac smooth anh function technic way associ match loss function given increas transfer function hkw match loss anh transfer function entrop loss show make transfer function steeper take right viewpoint match loss entrop loss converg hl limit case slope transfer function infinit becom threshold function question whether introduct hl buy us anyth believ prove unifi meta theorem whole class gener addit algorithm gl kw defin hl bound regress version perceptron winnow simpl special case loss bound convert loss bound correspond classif problem discret loss convers carri work averag margin set exampl rel linear threshold classifi convers hl describ paper consid principl way deriv averag margin base mistak bound averag margin reveal inner structur mistak bound result proven thu far conserv line algorithm previous use definit deviat fs attribut error lit easili relat averag margin reinterpret term hl averag margin preliminari linear hing loss defin two subset gr weight domain instanc domain weight maintain algorithm alway lie weight domain instanc exampl alway lie instanc domain requir convex gener addit algorithm diverg defin term link function function vector valu function interior int weight domain onto properti jacobian strictli posit definit everywher int link function uniqu invers gn int assum gradient potenti function pf int vpf int easi extend domain pf includ boundari link function bregman diverg function df int defin bre pf pf thu dr differ pf first order taylor expans around sinc strictli posit definit jacobian everywher int potenti pf strictli convex thu dr equal hold iff perceptron algorithm motiv ident link weight domain correspond diverg df lu winnow gentil warmuth il tyr tyv figur match loss ml figur hl function two case weight domain oe link function componentwis logarithm diverg relat link function un normal rel entropi dr ui wi ui note must lie int follow key properti immedi follow definit diverg dr lemma kw iv wl int df dr wl dr wl wl paper focu singl neuron use hard threshold transfer function see begin introduct view neuron two way standard view neuron use binari classif output tri predict desir label use threshold new view neuron regressor output linear activ tri predict classif use discret loss dl yl regress use linear hing loss hl parameter threshold hlr rrr dl la lnote argument two loss dl hlr switch intent discuss later easili shown hl convex gradient loss vwhl ar ar note take three valu mention introduct strictli speak gradient defin equal threshold show subsequ section even case properti need figur provid graphic represent hl threshold function transfer linear activ predict hard classif remain discuss section assum loss gener threshold smooth transfer function tanh commonli use neural network tanh rel loss bound proven comparison class consist singl neuron increas differenti transfer function rr hkw kw howev work loss function match transfer function use loss defin follow hkw see figur dz ml match loss squar loss linear regress match loss tanh entrop loss logist regress defin hkw notat use match loss ml use subscript instead stress connect match loss diverg discuss end section linear hing loss averag margin ml entrop loss finit tanh ese rang need logist regress want use type loss classif line threshold function slope earth function increas limit becom hind threshold ao obvious slope us match loss infinit slope also known rel loss bound base notion match loss grow slope ansfer function us seem imposs use match loss ansfer function threshold ao howev still make sens match loss view neuron regressor match loss rewritten anoth bregman diverg function increas slope ansfer function tanh keep fix limit case hind threshold ao loss becom twice linem hing loss threshold zero ml fi hlo ao ao final observ two view neuron relat dualiti properti aw bregman diverg da ml mla algorithm paper alway associ two gener addit algorithm given link function classif algorithm regress algorithm algorithm given next tabl correspond two view linear threshold neuron discuss last section breviti call two algorithm classif algorithm regress algorithm respect gen add classif algorithm instanc xt predict label yt updat gen add regress algorithm instanc xt predict tot xt label ytc updat discret loss classif algorithm receiv tot tot xt linear hing loss hl ar fit bel yt regress algorithm receiv infinit label sign yt assur yt rr classif algorithm predict regress algorithm linear activ loss classif algorithm discret loss dl yt regress algorithm use hlr updat two algorithm equival updat regress algorithm motiv minim problem wt argminwu dr tot hl xt set gradient zero get follow equilibrium equat hold minimum wt wt wt xt rrr xt approxim solv equat replac wt xt wt xt wt wt ar xt short hand mean oc yt oc yt gentil warmuth version perceptron winnow obtain use link function wl respect rel loss bound follow lemma relat hing loss regress algorithm hing loss arbitrari linear predictor proof df wt dt wt df wt wt tot wt hl hlr xt hlr xt first equal follow lemma second follow updat rule regress algorithm last equal use hl diverg see lemma sum first equal trial could relat total hlr regress algorithm total hlr regressor howev goal obtain bound number mistak classif algorithm therefor natur interpret linear threshold classifi threshold use classif algorithm use second equal sum trial note sum equal unaffect trial mistak occur trial yt wt wt thu equival follow set trial mistak occur df wl dr te dr wt wt yt xt wt sinc yt yt dr wt get follow theorem theorem let set trial classif algorithm make mistak everi throughout rest section classif algorithm compar perform linear threshold classifi threshold appli theorem perceptron algorithm wl give bound averag margin linear threshold classifi threshold trial sequenc ytu xt sinc yt tt inequ theorem least iml updat rule tem dr wt wt lla tlt imix sinc eorem mbi vector replac therein set nwhen solv result inequ imi depend cancel give us follow bound number mistak linear hing loss averag margin note usual mistak bound perceptron algorithm averag replac mint ytu xt also observ predict perceptron algorithm wl affect henc previou bound hold next appli theorem normal version winnow version winnow keep weight probabl simplex obtain slight modif winnow link function assum choos lx ioo xoo unlik perceptron algorithm winnow like algorithm heavili depend learn rate care tune need one show detail omit due space limit ifr xoo normal version winnow achiev bound imi df wl rl rl xoo ln dr wl rel entropi two probabl vector conclus full paper studi case consist threshold care give involv bound winnow normal winnow algorithm well norm perceptron algorithm gl refer aw azouri warmuth rel loss bound exponenti famili distribut unpublish manuscript bre bregman relax method find common point convex set applic solut problem convex program ussr comput mathemat physic fs freund schapir larg margin classif use perceptton algorithm th colt pp acm gl grove littleston schuurman gener converg result linear discrimin updat loth colt pp acm hkw helmbold kivinen warmuth worst case loss bound sigmoid linear neuron nip pp mit press kw kivinen warmuth addit versu exponenti gradient updat linear predict inform cornput kw kivinen warmuth rel loss bound multidimension regress problem nip pp mit press kwa kivinen warmuth auer perceptton algorithm rs winnow linear vs logarithm mistak bound input variabl relev artifici intellig lit littleston learn irrelev attribut abound new linearthreshold algorithm machin learn lit littleston mistak bound logarithm linear threshold learn algorithm phd thesi umvers california santa cruz lit littleston redund noisi attribut attribut error linear threshold learn use winnow th colt pp morgan kaufmann averag margin may posit even though consist
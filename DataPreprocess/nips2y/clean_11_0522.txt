abstract introduc two new techniqu densiti estim approach pose problem supervis learn task perform use neural network introduc stochast method learn cumul distribut analog determinist techniqu demonstr converg method theoret experiment provid comparison parzen estim theoret result demonstr better converg properti parzen estim introduct major problem scienc engin model probabilist manner even underli phenomena inher determinist complex phenomena often make probabilist formul feasibl approach comput point view although quantiti mean varianc possibl higher order moment random variabl often suffici character particular problem quest higher model accuraci realist assumpt drive us toward model avail random variabl use probabl densiti cours lead us problem densiti estim see common approach densiti estim nonparametr approach densiti determin accord formula involv data point avail common non parametr method kernel densiti estim also known parzen window estim nearest neighbor techniqu non parametr densiti estim belong class ill pose problem sens small chang data lead larg chang correspond address neural network densiti estim estim densiti therefor import method robust slight chang data reason amount regular need regular embed choic smooth paramet kernel width problem non parametr techniqu extrem sensit choic smooth paramet wrong choic lead either undersmooth oversmooth spite import densiti estim problem propos method use neural network sporad propos two new method densiti estim implement use multilay network addit abl approxim function given precis multilay network give us flexibl choos error function suit applic method develop base approxim distribut function contrast previou work focu approxim densiti straightforward differenti give us estim densiti function distribut function often use right one directli evalu quantil probabl random variabl occur particular interv one techniqu stochast algorithm slc second determinist techniqu base learn cumul sic stochast techniqu gener smoother smaller number data point howev determinist techniqu faster appli one dimens present result consist converg rate estim error method univari case unknown densiti bound bound deriv order find estim error log log number data point comparison kernel densiti estim non neg kernel estim error assumpt unknown densiti squar integr second deriv see optim kernel width use possibl practic comput optim kernel width requir knowledg true densiti one see smooth densiti function bound deriv method achiev error rate approach new densiti estim techniqu illustr method use neural network stress suffici gener learn model well network output repres estim distribut function deriv estim densiti proceed descript two method slc stochast learn cumul let xn data point let underli densiti distribut function dt let neural network output repres set weight network ideal train neural network would like easili shown densiti random variabl gener accord uniform thu close possibl network output densiti close uniform goal attempt train network output densiti uniform network map repres distribut function basic idea behind propos algorithm use data point input network everi train cycl gener differ set network target randomli uniform distribut adjust magdon ismail tiya weight map data point sort ascend order gener target also sort ascend order thu train network map data uniform distribut describ step algorithm note result network repres monoton nondecreas map otherwis repres legitim distribut function simul use hint penalti enforc monoton algorithm follow let xn data point set train cycl number initi weight usual randomli gener randomli uniform distribut point sort un point target output adjust network weight accord backpropag scheme vt ow object function includ error term monoton hint penalti term nh suppress depend second term monoton penalti term posit weight constant small posit number familiar unit step function yk set point wish enforc monoton set go step repeat error small enough upon converg densiti estim deriv note present randomli gener target differ everi cycl smooth effect allow converg truli uniform distribut one version implement simul studi gener new target everi fix number cycl rather everi cycl gener improv speed converg continu learn process also note prefer choos activ function output node rang ensur estim distribut function rang slc applic estim univari densiti multivari case nonlinear map necessarili result uniformli distribut output fortun mani major problem encount practic univari multivari problem even modest number dimens need huge amount data obtain statist accur result next method applic multivari case well sic smooth interpol cumul multilay network input point network output estim distribut function let true densiti function let correspond distribut function let xd distribut function given neural network densiti estim straightforward estim could fraction data point fall area integr defin otherwis method propos use estim target output neural network estim given discontinu neural network method develop provid smooth henc realist estim distribut function densiti obtain differenti output network respect input low dimension case uniformli sampl use grid obtain exampl network beyond two three dimens becom comput intens altern one could sampl input space randomli use say uniform distribut approxim rang xn everi point determin network target accord anoth option use data point exampl target point would also use monoton hint guid train train perform approxim densiti estim obtain simul result true densay sic opllrrtal parzen ow figur comparison optim parzen window neural network estim plot true densiti estim slc sic parzen window optim kernel width pg notic even optim parzen window bumpi compar neural network test techniqu densiti estim data drawn mixtur two gaussian magdon ismail atiya data point randomli gener densiti estim use slc sic data point compar parzen techniqu learn perform standard hidden layer neural network hidden unit hidden unit activ function use tanh output unit err function set typic densiti estim shown figur converg densiti estim techniqu lo lo figur converg densiti estim error sic five hidden unit two layer neural network use perform map xi train accord sic variou result densiti estim error comput run plot result log log scale comparison also shown best fit use techniqu stochast approxim theori shown lc converg similar solut sic focu attent converg sic figur show empir studi converg behavior optim linear fit log log slope indic converg rate theoret deriv converg rate log log shortli discuss analyz sic introduc call approxim gener distribut function assum true distribut function bound deriv therefor cumul approxim implement gener distribut bound deriv asymptot limit probabl obtain converg true densiti let space distribut function real line possess continu densiti exist everywher continu class function interest defin metric respect follow dt ii ii expect squar valu respect distribut let us name norm let data set xn correspond xi let target yi assum true distribut function bound deriv order defin set approxim sampl distribut function follow neural network densiti estim definit fix approxim sampl distribut function satisfi follow two condit log log ih xi yil denot set approxim sampl distribut function data set given let ai sup ig use notat deriv defin inf sup denot tn fix note definit supx ih lowest possibl bound tn deriv approxim sampl distribut function given particular data set sens smoothest approxim sampl distribut function respect tn deriv tn deriv bound one expect bi ai least limit next theorem present main theoret result paper name bound estim error densiti estim obtain use approxim sampl distribut function embed larg amount technic machineri essenti content true distribut function bound deriv order pick approxim distribut function obey certain bound obtain converg rate estim error loglog theorem converg true densiti let data point xi drawn distribut let supx ai fixu letb infq sup let approxim distribut function bk sup definit approxim sampl distribut function must exist inequ log hold probabl present proof elsewher note theorem appli uniformli interpol particular larg enough neural network one monoton interpol provid network train small enough error possibl univers approxim result multilay network note theorem hold smooth densiti function bound higher deriv converg rate approach loglog faster converg kernel densiti estim optim rate magdon ismail atiya note smooth paramet need determin note one tri find approxim distribut function smallest possibl deriv specif sampl distribut function pick one minim bound en deriv could done introduc penalti term penal magnitud deriv exampl tikhonov type regular comment develop two techniqu densiti estim base idea learn cumul map data point uniform densiti two techniqu present stochast techniqu slc expect inherit characterist stochast iter algorithm determinist techniqu sic slc tend slow practic howev set target drawn uniform distribut anticip smooth regular effect seen compar slc sic figur present experiment comparison techniqu parzen techniqu present theoret result demonstr consist techniqu well give converg rate loglog better optim parzen techniqu smooth paramet need chosen smooth occur natur pick interpol lowest bound certain deriv method major time spent learn phase learn done evalu densiti fast acknowledg would like acknowledg yaser abu mostafa caltech learn system group use input refer fukunaga hostetl optim nearest neighbor densiti estim ieee transact inform theori hornik stinchcomb white univers approxim unknown map deriv use multilay feedforward network neural network magdon ismail atiya consist densiti estimati sampl distribut function manuscript prepar submiss parzen estim probabl densiti function mode annal mathemat statist sill abu mostafa monoton hint mozer jordan petsch editor advanc neural inform process system nip volum page morgan kaufmann silverman densiti estim statist data analysi chapman hall london uk tikhonov arsenin solut ill pose problem scripta seri mathemat distribut sole halst press winston new york translat editor fritz john
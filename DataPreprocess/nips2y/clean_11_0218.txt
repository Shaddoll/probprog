abstract gaussian process gp predict suffer scale data set size use finit dimension basi approxim gp predictor comput complex reduc deriv optim finit dimension predictor number assumpt show superior predictor project bay regress method asymptot optim also show calcul minim model size given calcul back numer experi introduct last decad grow interest bayesian approach regress problem use neural network gaussian process gp predict regress perform function space use gaussian random process prior comput complex gp predictor scale size finit dimension approxim gaussian process dataset suggest use finit dimension approxim function space assum dimens use finit dimension model motiv need regress algorithm comput cheaper gp one moreov gp regress may use identif dynam system de nicolao ferrari trecat next step model base control design mani case easier accomplish second task model low dimension use finit dimension model lead natur question basi optim zhu et al show asymptot gime one use first eigenfunct covari function describ gaussian process call method project bay regress pbr main result paper although pbr asymptot optim finit data deriv predictor comput complex outperform pbr obtain upper bound gener error practic need know larg make show depend provid mean calcul minim also provid empir result back theoret calcul problem statement consid problem estim unknown function noisi observ ti xi ei zero mean gaussian random variabl varianc sampl xi drawn independ random distribut prior probabl measur function assum gaussian zero mean autocovari function moreov suppos xi el mutual independ given data set xl xn tn well known posterior probabl fiz gaussian gp predict comput via explicit formula whittl fl xl xn xj matrix ij kroneck delta work interest approxim suitabl dimension space go defin consid mercer hilbert expans jf ti jfr ti tj sij eigenvalu hi order decreas way zhu et al shown least asymptot optim model belong span motiv choic space even deal finit amount data introduc finit dimension approxim call project bay regress aris invers matrix ferrar trecat william opper definit ij ai ij pbr approxim xl ra xl tfl xn xn name pbr come fact gp predictor use mi specifi prior witi whose autocovari function project comput point view interest note calcul pbr scale data assum cost comput matrix product throughout paper follow measur perform extens use definit let predictor use inform error gener error respect defin estim belong class said optim simpli optim respect eso es es data set note optim mean optim fix vector data point obvious optim also simpli optim definit motiv fact gaussian process prior function predictor depend linearli comput es carri finit dimension matrix calcul see lemma although obtain esa difficult averag usual analyt intract optim finit dimension model start consid two class linear approxim name mxn lf mxm matric possibl depend xi sampl point pbr predictor goal character optim predictor state main result two preliminari lemma given first one prove pilz second follow straightforward calcul lemma let ii hold inf tr zaz zb tr zer minimum achiev matrix lemma let hold ai tr lhl finit dimension approxim gaussian process proof view hold error definit set note ex ira ex mercerhilbert expans xi take mean result follow theorem given vn optim moreov ego xi tr eho xi tr predictor given proof start consid case view lemma need minim matrix appli lemma one obtain argn nq minq tr prove first result second case appli lemma perform minim matrix done note note differ gp predictor deriv approxim function xk im gi gi xk moreov complex hand scale comput cost intermedi gp predictor pbr intuit pbr method inferior take account locat set prior also show pbr predictor asymptot equival clear explicit evalu ea ego gener hard problem mean xi sampl enter matric remaind section deriv upper bound ego consid class approxim ff mxm disij inclus optim predictor ego due diagon structur matrix upper bound eu may explicitli comput state next theorem theorem approxim given ij ij ij ferrari trecat william opper optim moreov upper bound gener error given ai qkak qk dx proof order find optim approxim start appli lemma drb need minim ra dq ii di di obtain bound first comput gener error gener approxim ai verifi ii ii nci obtain assum di constant oo eu ki nzd ci ny di minim di recal also simpli optim formula follow stationari express ci coeffici becom simpli ci ai ai remark naiv approach estim coeffici estim wi would set lbi approxim integr wi dx effect matrix shrink bi higher frequenc eigenfunct shrinkag would necessari limit stop poorli determin wi domin equat show fact upper bound improv increas fact equat use upper bound gp predict error tightest cx consist idea increas bayesian scheme lead improv predict practic one would keep otherwis approxim algorithm would comput expens gp predictor choos larg show pbr approxim definit aris matrix becom diagon limit oe due orthogon eigenfunct equat factor indic much prior varianc ith eigenfunct reduc observ datapoint note express exactli posterior varianc mean finit dimension approxim gaussian process figur detach point variou model order dash dash dot dot solid eg plot gaussian prior hi given observ corrupt gaussian nois varianc eigenfunct hi posterior consider tighter prior hi prior posterior almost width suggest littl point includ eigenfunct finit dimension model omit first eigenfunct add term hi expect gener error mean finit dimension model use first eigenfunct expect train set size determin call detatch point dimension approxim convers practic regress problem data set size known knowledg autocovari eigenvalu possibl determin via detatch point formula order approxim use order guarante eo experiment result conduct experi use prior covari function covari function correspond gaussian process mean squar differenti lie famili stationari covari function modifi bessel function eigenvalu eigenfunct covari kernel densiti calcul vivarelli first experi use learn curv obtain averag choic train data set estim use differ sampl notic practic coincid latter curv drawn pictur figur plot learn curv gp regress approxim variou model order correspond detach point also plot show effect determin size data set minimum possibl error attain finit dimension model increas hi plateaux clearli seen right hand side figur ferrari trecat william opper second experi demonstr differ perform estim use figur plot averag differ obtain averag eb eho comput pair comparison choic notic superior pbr estim small expect asymptot equival discuss paper shown finit dimension predictor construct lower gener error pbr predictor comput complex lie complex gp predictor complex pbr also shown calcul number basi function requir accord data set size use finit dimension model approxim gp regress interest altern found work gibb mackay approxim matrix invers method scale investig would interest compar rel merit two method acknowledg thank francesco vivarelli help provid learn curv ej eigenfunct valu section refer de nicolao ferrari trecat identif narx model use regular network consist result ieee int joint conf neural network anchorag us pp gibb mackay ej cient implement gaussian process cavendish laboratori cambridg uk draft manuscript avail http wol ra phi cam ac uk mackay homepag html opper regress gaussian process averag case perform kwok yee wong yeung ed theoret aspect neural comput multidisciplinari perspect springer verlag pilz bayesian estim experiment design linear regress model wiley son ripley pattern recognit neural network cup wahba spline model observ data societi industri appli mathemat cbm nsf region conf seri appli mathemat whittl predict regul linear least squar method english univers press william predict gaussian process linear regress linear predict beyond jordan editor learn infer graphic model kluwer academ press vivarelli studi gener gaussian process bayesian neural network forthcom phd thesi aston univers birmingham uk zhu rohwer bayesian regress filter issu prior neural comput applic zhu william rohwer morciniec gaussian regress optim finit dimension linear model tech rep ncrg aston univers birmingham uk
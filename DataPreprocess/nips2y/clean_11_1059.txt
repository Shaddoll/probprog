abstract describ reinforc learn algorithm partial observ environ use short term memori call blht sinc blht learn stochast model base bayesian learn overfit problem reason solv moreov blht effici implement paper show model learn blht converg one provid accur predict percept reward given short term memori introduct research reinforc learn rl problem partial observ environ gain attent recent mainli assumpt perfect complet percept state environ avail learn agent mani previou rl algorithm requir valid mani realist environ model free worl pomdp figur three approach one approach problem model free approach singh et al jaakkola et al arrow fig give state estim use memori less polici expect approach find realli effect polici necessari accumul inform estim state model base approach superior environ popular model base approach via partial observ markov decis process pomdp model repres decis process agent fig approach describ rout world polici pomdp approach two seriou difficulti one learn pomdp arrow fig abe suematsu hayashi warmuth show learn probabilist automata np hard mean learn pomdp also np hard difficulti find optim polici given pomdp model arrow fig psapc hard shown papadimitri tsitsikli accordingli method base approach chrisman mccallum scale well larg problem approach use short term memori comput tractabl cours construct environ long term memori essenti howev mani environ stochast signific past inform decreas exponenti fast time goe environ memori moder length work fine mccallurn propos util suffix memori usm algorithm usm use tree structur repres short term memori variabl length usm model learn base statist test requir time space proport learn step make difficult adapt usm environ requir long learn step usm suffer overfit problem difficult problem face model base learn method usm may overfit underfit signific level use statist test know proper level advanc paper introduc algorithm call blht suematsu et al environ model histori tree model htm stochast model variabl memori length although blht share tree structur represent short term memori usm comput time requir blht constant step blht cope environ requir larg learn step addit blht base bayesian learn overfit problem solv reason similar version htm introduc use learn hidden markov model ron et al learn method tree grown similar way usm tri adapt rl problem face problem usm paper show htm learn blht converg optim one sens provid accur predict percept reward given shortterm memori blht learn htm effici way arrow fig sinc htm compos subset markov decis process mdp effici solv dynam program dp techniqu arrow fig see blht approach follow easi way world polici goe around pomdp pomdp model decis process agent partial observ environ formul pomdp let finit set state environ finit set agent action finit set possibl percept let us denot probabl reward make transit state use action ps isa respect also denot probabl obtain percept transit use action oilsa pomdp model specifi xo ps lsa oilsa xo probabl distribut initi state denot histori action percept agent till time dr pomdp model given one comput belief state dt state estim time slsl denot map histori belief state defin pomdp model xm xm dt belief state precis state estim known suffici statist optim polici pomdp bertseka also known stochast process mdp continu rl algorithm partial observ environ use memori space dot xlsl xx xlsl xj bayesian learn histori tree model blht section summar rl algorithm partial observ environ call blht suematsu et al histori tree model blht bayesian learn hypothesi space compos predict model call histori tree model htm given short term memori htm provid probabl disctribut next percept expect immedi reward action htm repres tree structur call histori tree paramet given leaf tree histori tree associ histori dt leaf follow start root check recent percept follow appropri branch check action follow appropri branch procedur repeat till reach leaf denot reach leaf kh dt set leav leaf paramet oilla wla oilla denot probabl observ time dt last action cola denot expect immedi reward perform dt let oh oilla figur three state environ agent receiv percept state percept state histori tree repres environ fig show three state environ histori tree repres environ construct htm equival environ set appropri paramet leaf histori tree bayesian learn blht design bayesian learn hypothesi space set histori tree first show posterior probabl histori tree given histori dr deriv posterior probabl set prior densiti oh ohlh kla ilia kta normal constant oeilla hyper paramet specifi prior densiti posterior probabl hldt ctp kl hiez ctilla jr ol ct normal constant gamma function nitll number time observ execut dt histori dr nlra iez nitira olla ei ilia next show estim paramet use averag oilla posterior suematsu hayashi densiti estim lla express la ilia illa nlra oq wl estim accumul reward receiv execut ha dr divid number time perform dr ia nlra tt th occurr execut dr learn algorithm principl evalu eq extract map model howev often impract proper hypothesi space larg agent littl prior knowledg concern environ fortun design effici learn algorithm assum hypothesi space set prune tree larg histori tree ratio prior probabl histori tree obtain prune subtre ah given known function ah defin function hldt take logarithm eq without normal constant rewritten hlr log hl et yiiz nlt illa log kla ylta olla extract map model find histori tree maxim eq show hldt evalu sum accordingli implement effici algorithm use tree whose intern leaf node store nilla ctilta cot suppos agent observ last action eq log otherwis set node path root leaf dr thu updat evalu eq ad nilta recalcul node updat extract map model use procedur find mapsubtre shown fig show learn algorithm fig map model extract polici updm given condit satisfi limit theorem section describ limit theorem blht throughout section assum polici use learn stochast process st ergod first show theorem ensur histori tree model learn blht miss relev memori see suematsu et al proof condit satisfi exampl hl ihl denot size rl algorithm partial observ environ use memori find map subtre node ah child node node return ahc ac find map subtre ah ah ahc ac end ag log ah ag return ah els return main loop condit dt dr goto dt rc polici select action random dr exploratori action perform receiv updat condit satisfi find map subtre root dynam program end figur procedur find map subtre main loop theorem lim lt hldt ha iil ha iil condit entropi given dr defin denot probabl expect valu respect let histori tree shown fig histori tree obtain prune subtre environ shown fig ha ill ha ill miss relev memori make condit entropi increas sinc blht learn histori tree maxim hldt minim ha ill learn histori tree miss relev memori next show limit theorem concern estim paramet denot true pomdp model ri defin follow paramet rrilsa st ps saoilsa ws ps ls follow theorem hold theorem leaf lim oil lim cb saysila outlin proof use ergod theorem lira ilxa suematsu hayashi expand equat use chain rule deriv eq eq deriv similar way explain theorem mean clearli show relationship lla belief state xt st slxh dt xo xsp xt xllt xo ei dx dtl dt lib indic function set dtixm dt dx dx dxlsl ergod assumpt take limt equat ys ii xt xl xh dt la see eq yi averag belief state xt condit densiti belief state distribut accord repres yi shortterm memori give domin inform dt concentr la reason approxim belief state extrem case il non zero point yl xt dr pleas note given short term memori repres yl accur state estim consequ theorem ensur learn htm converg model provid accur predict percept reward among fact provid solid basi blht believ blht compar favor method use short term memori cours theorem also say blht find optim polici environ markovian semi markovian whose order small enough equival model contain experi made experi variou environ paper show one demonstr effect blht environ use grid world shown fig agent four action chang locat one four neighbor grid fail probabl failur agent chang locat probabl goe one two grid perpendicular direct agent tri go probabl agent detect mere exist four surround wall agent receiv reward reach goal grid mark tri go grid occupi obstacl goal action reloc agent one start state mark random order achiev high perform environ agent select differ action ident immedi percept mani state alias look ident immedi percept environ state among largest problem shown literatur model base rl techniqu partial observ environ fig show learn curv obtain averag independ run learn agent updat polici everi trial visit goal rl algorithm partial observ environ use memori polici evalu run step action select use polici random probabl select random decreas exponenti time goe use tree homogen depth fig horizont broken line indic averag reward mdp model obtain assum perfect complet percept give upper bound origin problem higher optim one origin problem learn curv shown close upper bound later stage iii tdal figur grid world learn curv summari paper describ rl algorithm partial observ environ use shortterm memori call blht prove model learn blht converg optim model given hypothesi space provid accur predict percept reward given short term memori believ fact provid solid basi blht blht compar favor method use short term memori refer abe warmuth comput compleixi apporxim distribut probabilist automata machin learn bertseka dyanam program prentic hall chrisman reinforcemnt learn perceptu alias perceptu distinct approach proc loth nation confer artifici intellig jaakkola singh jordan reinforc learn algorithm parial observ markov decis problem advanc neural inform process system pp mccallum overcom incomplet percept util distict memori proc loth intern confer machin learn mccallum instanc base util distinct reinforc learn hidden state proc th intern confer machin learn papadimitri tsitsikli complex markov decis process mathemat oper research ron singer tishbi learn probabilist automata variabl memori length proc comput learn theori pp singh jaakkola jordan learn without state estim partial observ markov decis process proc th intern confer machin learn pp suematsu hayashi li bayesian approch model learn nonmarkovian environ proc th intern confer machin learn pp
abstract kernel paramet one tunabl paramet support vector machin control complex result hypothesi choic amount model select valu usual found mean valid set present algorithm automat perform model select littl addit comput cost need valid set procedur model select learn separ kernel dynam adjust learn process find kernel paramet provid best possibl upper bound generalis error theoret result motiv approach experiment result confirm valid present introduct support vector machin svm learn system design automat trade accuraci complex minim upper bound generalis error provid vc theori practic howev svm still tunabl paramet need determin order achiev right balanc valu usual found mean valid set one import kernel paramet implicitli defin structur high dimension featur space maxim margin hyperplan found rich featur space would caus system overfit data dynam adapt kernel support vector machin convers system unabl separ data kernel poor capac control therefor perform tune kernel paramet subject margin maxim noisi dataset yet anoth quantiti need set name soft margin paramet svm therefor display remark dimension reduct model select system neural network need mani differ architectur test decis tree face similar problem prune phase hand svm shift one model complex anoth simpli tune continu paramet gener model select svm still perform standard way learn differ svm test valid set order determin optim valu kernel paramet expens term comput time train data paper propos differ scheme dynam adjust kernel paramet explor space possibl model littl addit comput cost compar fix kernel learn futhermor approach make use train set inform effici sampl complex sens propos model select procedur first prove theoret result name margin structur risk minim srm bound gener error depend smoothli kernel paramet exploit algorithm keep system close maxim margin kernel paramet chang smoothli phase theoret bound given srm theori comput best kernel paramet one give lowest possibl bound section present experiment result show model select effici perform use propos method though consid gaussian kernel simul outlin support vector learn decis function implement sv machin written obtain maximis follow lagrangian number pattern oqo jyiyjk xi xj respect oq subject constraint oqi function call kernel kernel provid express dot product high dimension featur space cristianini campbel shaw taylor also implicitli defin nonlinear map cg train data featur space may separ use maxim margin hyperplan number choic kernel function made gaussian kernel follow upper bound proven vc theori generalis error use hyperplan featur space radiu smallest ball contain train set number train point margin cf complet survey gener properti sv machin lagrang multipli cq usual found mean quadrat program optim routin kernel paramet found use valid set illustr figur minimum generalis error valu kernel paramet best trade overfit abil find effici solut figur gener error axi function axi mirror symmetri problem gaussian kernel zero train error maxim margin averag exampl automat model order select prove theorem show margin optim hyperplan smooth function kernel paramet upper bound generalis error first state implicit function theorem implicit function theorem let continu differenti function uc xvc let solut equat let partial deriv matrix mid full rank near dynam adapt kernel support vector machin exist one one function function continu theorem margin sv machin depend smoothli kernel paramet proof consid function rp rr given data map choic rr optim paramet lagrang paramet sv machin kernel matrix gij yiyjk xi xj let ra iyjk ff xi xj yi function sv machin maxim fix valu let correspond solut let set indic may assum submatrix index non singular sinc otherwis maxim margin hyperplan could express term subset indic choos imal set indic contain correspond submatrix non singular point index margin consid function ej yjsj neighbourhood ji enumer element ow satisfi equat extrem point sv function implicit function continu uniqu iff continu differenti partial deriv matrix full rank partial deriv matrix given sij jz yji yjj xj xj hji ji non degener definit fj oa oj yjj hj consid non zero satisfi cuy th tg ao ty tgo henc matrix non singular satisfi given linear constraint henc implicit function theorem continu function rr follow proven show continu function rr radiu ball contain point also continu function rr gener error bound form constant follow corollari corollari bound gener error smooth mean margin optim small variat kernel paramet produc small variat margin bound generalis error thu updat system cristianini campbel shaw taylor still sub optim posit suggest follow strategi gaussian kernel instanc kernel select procedur initi small valu maxim margin comput srm bound observ valid error increas kernel paramet stop predetermin valu reach els repeat step procedur take advantag fact small rr converg gener rapid overfit data cours system near equilibrium iter alway suffici move back maxim margin situat word system brought maxim margin state begin comput cheap activ kept situat continu adjust ct kernelparamet gradual increas next section experiment investig procedur real life dataset numer simul use kernel adatron ka algorithm recent develop two author use train sv machin chosen algorithm regard gradient ascent procedur maximis kuhn tucker lagrangian thu sub optim state close optimum littl comput effort need bring system back maxim margin posit kernel adatron algorithm fori zi lr xi xj yizi els margin min max posit neg label pattern margin stop els go step experiment result section implement algorithm real life dataset plot upper bound given vc theori gener error function rr order comput bound need estim radiu ball featur space gener done explicitli maximis follow lagrangian ki use convex quadrat program routin subject constraint ki ki radiu found dynam adapt kernel support vector machin jk xi xj jk xi xj xi xi howev also get upper bound quantiti note gaussian kernel alway map train point surfac sphere radiu center origin featur space easili seen note distanc point origin norm figur give bound upper bound oq generalis error test set two standard dataset aspect angl depend sonar classif dataset gorman sejnowski wisconsin breast cancer dataset see plot littl need addit comput cost determin quadrat progam problem least gaussian kernel fig plot bound generalis error figur unit state postal servic dataset handwritten digit instanc investig minimum bound approxim coincid minimum generalis error give good criterion suitabl choic furthermor estim best deriv sole train data without need addit valid set figur generalis error solid curv sonar classif left fig wisconsin breast cancer dataset right fig upper curv dot show upper bound vc theori top curv start small valu observ margin maximis rapidli furthermor margin remain close er increment small amount consequ studi perform system travers rang er valu altern increment er maximis margin use previou optim set valu start point found procedur add signific comput cost gener exampl sonar classif dataset mention start er increment aa took iter reach er reach er iter learn er valu rough doubl learn time possibl determin reason valu er good generalis without use valid set cristianini campbel shaw taylor figur generalis error solid curv upper bound vc theori dash curv digit usp dataset handwritten digit conclus present algorithm automat learn kernel paramet littl addit cost comput sampl complex sens model select take place learn process experiment result provid show strategi provid good estim correct model complex refer aizerman braverman rozono theoret foundat potenti function method pattern recognit learn autom remot control bartlett taylor gener perform support vector machin pattern classifi advanc kernel method support vector learn bernhard sch kopf christoph burg alexand smola ed mit press cambridg usa burg tutori support vector machin pattern recognit data mine knowledg discoveri friess cristianini campbel kernel adatron algorithm fast simpl learn procedur support vector machin shavlik ed machin learn proceed fifteenth intern confer morgan kaufmann publish san francisco ca gorman sejnowski neural network lecun jackel bottou brunot cort denker drucker guyon muller sackingel simard vapnik comparison learn algorithm handwritten digit recognit intern confer artifici neural network fogelman gallinari ed pp taylor bartlett williamson anthoni structur risk minim data depend hierarchi neurocolt technic report nctr ftp ftp dc rhbnc ac uk pub neurocolt tech report ster dobnikar neural network medic diagnosi comparison method bulsari et al ed proceed intern confer eann vapnik natur statist learn theori springer verlag jame robert advanc calculu belmont calif wadsworth
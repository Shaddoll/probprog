abstract present mont carlo algorithm learn act partial observ markov decis process pomdp real valu state action space approach use import sampl repres belief mont carlo approxim belief propag reinforc learn algorithm valu iter employ learn valu function belief state final samplebas version nearest neighbor use gener across state initi empir result suggest approach work well practic applic introduct pomdp address problem act optim partial observ dynam environ pomdp learner interact stochast environ whose state partial observ action chang state environ lead numer penalti reward may observ unknown tempor delay learner goal devis polici action select maxim reward obvious pomdp framework embrac larg rang practic problem past work predomin studi pomdp discret world discret world advantag distribut state call belief state repres exactli use one paramet per state optim valu function finit plan horizon shown convex piecewis linear make possibl deriv exact solut discret pomdp interest pomdp continu state action space pay tribut fact larg number real world problem continu natur gener pomdp solvabl exactli littl known special case solv paper propos approxim approach mc pomdp algorithm accommod real valu space model central idea use mont carlo sampl belief represent propag reinforc learn belief space employ learn valu function use sampl base version nearest neighbor gener empir result illustr approach find close optim solut effici mont carlo pomdp preliminari pomdp address problem select action stationari partial observ control markov chain establish basic vocabulari let us defin state point time world specif state denot mont carlo pomdp action agent execut action denot observ sensor agent observ noisi project world state use denot observ reward addit agent receiv reward penalti denot simplifi notat assum reward part observ specif use denot function extract reward observ throughout paper use subscript refer specif point time st refer state time pomdp character three probabl distribut initi distribut pr zo specifi initi distribut state time next state distribut pt oct vc vct describ likelihood action execut state lead state perceptu distribut pt ct describ likelihood observ world state histori sequenc state observ simplic assum action observ altern use dt denot histori lead time dt ot ot ao oo fundament problem pomdp devis polici action select maxim reward polici denot cr map histori action assum action chosen polici polici induc expect cumul possibl discount discount factor reward defin denot mathemat expect pomdp problem thu find polici maxim argmax belief state avoid difficulti learn function unbound input histori arbitrarili long common practic map histori belief state learn map belief state action instead formal belief state denot probabl distribut state condit past action observ ot pr xt dt pr xt ot oo belief comput increment use knowledg pomdp defin distribut initi obtain ot pr xt ot oo pr ot pr ot pr ot xt zt pr zt oo zt pr zt oo dzt ct pr oet ct ot dzt thrun lllllllllnillllllllllllllllllllll iiiilll ii ii figur sampl likelihood weight sampl import sampl bottom graph sampl shown approxim function shown top height sampl illustr import factor denot constant normal deriv follow directli fact environ stationari markov chain futur state observ condit independ past one given knowledg state equat obtain use theorem total probabl arm notion belief state polici map belief state instead histori action legitimaci condit instead follow directli fact environ markov impli one need know past make optim decis sampl represent thu far intent left open belief state repres prior work state space discret discret world belief repres collect probabl one state henc belief repres exactli interest real valu state space gener probabl distribut realvalu space possess infinit mani dimens henc cannot repres digit comput key idea repres belief state set weight sampl drawn belief distribut figur illustr two popular scheme sampl base approxim likelihood weight sampl sampl shown bottom figur la drawn directli target distribut label figur la import sampl sampl drawn distribut curv label figur lb latter case sampl annot numer import factor account differ sampl distribut target distribut height bar figur illustr import factor import sampl requir case throughout paper obvious sampl method gener approxim mild assumpt converg denot sampl set size target distribut rate context pomdp use sampl base represent give rise follow algorithm approxim belief propag equat algorithm particl filter ot time draw random state zt ot mont carlo pomdp sampl xt accord xt xt set import factor ot add zt zt toot normal zt zt return ot algorithm converg arbitrari model arbitrari belief distribut defin discret continu mix continu discret state action space minor modif propos name like particl filter condens algorithm surviv fittest context robot mont carlo local project convent plan result appli action state zt distribut pr zt rt zt state zt reward rt next time step oper call project pomdp state zt unknown instead one comput result appli action belief state result distribut pt rt belief state reward rt sinc belief state distribut result project pomdp technic distribut distribut project algorithm deriv follow use total probabl obtain pr pr ot rt dr pr ot ot dt pt dr dot term alreadi deriv previou section equat observ reward trivial comput observ second term obtain integr unknown variabl zt zt exploit markov properti pr ot dt pr ot xt pr xt dt dzt pt zt pr zt zt pr zt idt dzt dzt lead follow approxim algorithm project belief state spirit paper approach use mont carlo integr instead exact integr repres distribut distribut distribut sampl drawn distribut algorithm partid projecfion ot time draw random state zt ot sampl next state zt accord zt sampl observ accord ot comput ot particl filter ot add ot ot toot return ot result algorithm sampl set belief state reward drawn desir distribut pv ot rt ot converg probabl true posterior thrun learn valu function follow rich literatur reinforc learn approach solv pomdp problem valu iter belief space specif approach recurs learn valu function belief state action back valu subsequ belief state ot ot maaxq leav open moment repres easi seen algorithm particl project appli comput mont carlo approxim right hand side express given belief state ot action particl project comput sampl ot expect valu right hand side approxim shown side equal greedi polici argmaxq optim crq furthermor shown discret case repetit applic lead optim valu function thu optim polici approach essenti perform model base reinforc learn belief space use approxim sampl base represent make possibl appli rich bag trick found literatur mdp experi use onlin reinforc learn counter base explor experi replay determin order belief state updat nearest neighbor return issu repres sinc oper real valu space sort function approxim method call howev recal accept probabl distribut sampl set input make exist function approxim neural network inapplic current implement nearest neighbor appli repres specif algorithm maintain set sampl set belief state annot action valu new belief state encount valu obtain find nearest neighbor databas linearli averag valu suffici mani neighbor within pre specifi maximum distanc ad databas henc databas grow time approach use kl diverg rel entropi distanc function technic kl diverg two continu distribut well defin appli sampl set howev cannot comput henc evalu distanc two differ sampl set approach map continu valu densiti use gaussian kernel use mont carlo sampl approxim kl diverg algorithm fairli gener extens nearest neighbor function approxim densiti space densiti repres sampl space limit preclud us provid detail see experiment result preliminari result obtain world shown two domain one synthet one use simul rwi robot synthet environ figur agent start lower left corner object reach heaven either upper left corner lower right stdctli speak kl diverg distanc metric ignor mont carlo pomdp figur environ schemat averag perform reward function train episod black graph correspond smaller environ step min grey graph larger environ step min result plot function number backup thousand comer opposit locat hell agent know locat heaven ask priest locat upper right comer thu optim solut requir agent go first priest head heaven state space contain real valu coordin agent discret locat heaven compon unobserv addit know locat heaven agent also cannot sens real valu coordin random motion nois inject move agent hit boundari penal also told boundari hit make possibl infer coordin along one axi howev notic initi coordin agent known optim solut take approxim step thu success pomdp planner must capabl look step ahead use term success polici refer polici alway lead heaven even path suboptim polici success agent must learn first move priest inform gather proceed right target locat figur show perform result averag experi solid black curv diagram plot averag cumul reward function number train episod figur function number backup figur success polici consist found episod backup experi current implement backup requir approxim minut pentium pc experi success polici identifi episod less backup minut success polici found learn gradual optim path investig scale doubl size environ quadrupl size state space make optim solut step long result depict gray curv figur success polici consist found episod backup minut run success polici identifi episod also appli mc pomdp robot locat retriev task robot figur find grasp object somewher vicin floor tabl height robot task grasp object use gripper reward success grasp object penal unsuccess grasp move far away object state space continu coordin discret object height robot use mono camera system object detect henc view object singl locat insuffici local moreov initi object might sight robot camera robot must look around first simul assum gener detect error fals posit fals neg addit gaussian nois object detect correctli robot action includ tum variabl angl translat variabl distanc grasp one two legal height robot control erron varianc space rotat space typic belief state rang uniformli distribut sampl set initi belief sampl narrowli focus specif locat thrun figur find fetch task success iter mobil robot gripper camera hold target object experi card simul three success run trajectori project success rate function number plan step figur show rate success grasp function iter action initi robot fail grasp object approxim iter perform surpass plan time order hour howev robot fail reach part certain initi configur make imposs succeed object close maximum allow distanc part robot occasion miss object centimet figur depict three success exampl trajectori three robot initi search object move toward grasp success discuss present mont carlo approach learn act partial observ markov decis process pomdp approach repres belief distribut use sampl drawn distribut reinforc learn belief space appli learn optim polici use sampl base version nearest neighbor gener backup perform use mont carlo sampl initi experiment result demonstr approach applic real valu domain yield good perform result environ pomdp standard rel larg refer aaai fall symposium pomdp see http www cs duke edu mlittman talk pomdp symposium html bellman dynam program princeton univers press dayan sejnowski td converg probabl fox burgard dellaert thrun mont carlo local effici posit estim mobil robot aaai isard ake ndensati nditi na densiti pr pagati nf visua track nternati nalj urnal rnputer vision kaelbl littman cassandra plan act partial observ stochast domain submit public kaelbl littman moor reinforc learn survey jair kanazawa koller russel stochast simul algorithm dynam probabilist network uai lin self improv reactiv agent base reinforc learn plan teach machin learn littman cassandra kaelbl learn polici partial observ environ scale icml moor atkeson schaal local weight learn control aireview ormoneit sen kernel base reinforcementlearn tr statist stanford univers pitt shephard filter via simul auxiliari particl filter journal american statist associ sondik optim control partial observ markov process phd thesi stanford sutton barto reinforc learn introduct mit press tanner tool statist infer springer verlag watkin learn delay reward phd thesi king colleg cambridg
abstract problem reinforc learn non markov environ explor use dynam bayesian network condit independ assumpt random variabl compactli repres network paramet paramet learn line approxim use perform infer comput optim valu function rel effect infer valu function approxim qualiti final polici investig learn solv moder difficult drive task two valu function approxim linear quadrat found perform similarli quadrat model sensit initi perform level human perform task dynam bayesian network perform compar model use localist hidden state represent requir exponenti fewer paramet introduct reinforc learn rl address problem learn act maxim reward signal provid environ onlin rl algorithm tri find polici maxim expect time discount reward experi perform sampl backup learn valu function state state action pair decis problem markov observ state optim valu function state action pair yield inform requir find optim polici decis problem complet knowledg environ avail state differ may look uncertainti call perceptu alias caus decis problem dynam non markov perceiv state correspond address learn factor represent pomdp partial observ markov decis process mani interest decis problem markov input partial observ markov decis process pomdp formal assum process markov respect unobserv hidden random variabl state variabl time denot depend state previou time step action perform current observ evid assum independ previou state observ given current state state hidden variabl known certainti belief state maintain instead time step belief updat use bay theorem combin belief state previou time step pass model system dynam newli observ evid case discret time finit discret state action pomdp typic repres condit probabl tabl cpt specifi emiss probabl state transit probabl expect reward state action correspond hidden markov model hmm distinct transit matrix action hidden state repres singl random variabl take one valu exact belief updat comput use bay rule valu function discret state real valu belief state shown valu function piecewis linear convex worst case number linear piec grow exponenti problem horizon make exact comput optim valu function intract notic localist represent state encod singl random variabl exponenti ineffici encod bit inform state process requir possibl hidden state bode well abil model use represent scale problem high dimension input complex non markov structur factor represent bayesian network compactli repres state system set random variabl two time slice dynam bayesian network dbn repres system two time step condit depend random variabl time time within time step repres edg direct acycl graph condit probabl store explicitli parameter weight edg graph network dens connect infer intract approxim infer method includ markov chain mont carlo variat method belief state simplif appli dbn larg problem three distinct issu disentangl well parameter dbn captur underli pomdp much dbn hurt approxim infer good must approxim valu function achiev reason perform tri teas issu apart look perform dbn problem moder larg state space non markov structur algorithm use fulli connect dynam sigmoid belief network dsbn unit time slice see figur random variabl si binari condit proba sallan time figur architectur dynam sigmoid belief network circl indic random variabl fill circl observ empti circl unobserv squar action node diamond reward biliti relat variabl adjac time step encod action specif weight wik weight th unit time step th unit time step assum action taken time nonlinear usual sigmoid function exp note bia incorpor weight clamp one binari unit observ variabl assum discret condit distribut output given hidden state multinomi parameter output weight probabl observ output valu given exp rkk exp ukl denot output weight hidden unit output valu approxim infer infer fulli connect bayesian network intract instead use variat method fulli factor approxim distribut stl ps variat paramet optim standard mean field approxim sigmoid belief network paramet optim iter mean field equat converg iter valu variat paramet time held fix comput valu step analog run forward portion hmm forward backward algorithm paramet dsbn optim onlin use stochast gradient ascent exp log likelihood learn factor represent pomdp transit emiss matric respect aw au learn rate vector contain fulli factor approxim belief state vector zero one tth place notat denot th element vector column matrix approxim valu function comput optim valu function also intract factor state space represent appropri natur extrem assum state action valu function decompos way vst qf simplifi assumpt still enough make find optim valu function tractabl even state complet independ qk would still piecewis linear convex number piec scale exponenti horizon test two approxim valu function linear approxim quadrat approxim tx qk ba paramet approxim notat denot column matrix denot matrix transpos denot element wise vector multipl updat term factor approxim modifi learn rule correspond delta rule target input maxa qv lu qk qk eb ba bat eb oz learn rate tempor discount factor eb bellman residu eb maxqr xt txt experiment result new york drive task involv navig slower faster one way traffic multi lane highway speed agent fix must chang lane avoid slower car move way faster car agent remain front faster car driver fast car honk horn result reward instead collid slower car agent squeez past lane result reward time step horn lane squeez constitut clear progress reward see detail descript task sallan tabl sensori input new york drive task dimens size valu hear horn ye gaze object truck shoulder road gaze speed loom reced gaze distanc far near nose gaze refin distanc far half near half gaze colour red blue yellow white gray tan modifi version new york drive task use test algorithm task essenti describ except gaze side gaze direct input remov see tabl list modifi sensori input perform number algorithm approxim measur task random polici learn sensori input model localist represent hidden state consist singl multinomi random variabl linear quadrat approxim valu function dsbn mean field infer linear quadrat approxim human driver localist represent use linear learn approxim correspond quadrat approxim quadrat approxim train random initi initi correspond learn linear model random quadrat portion non human algorithm train iter case constant learn rate tempor decay rate use human driver author train iter use simpl charact base graphic display iter last second stochast polici use rl algorithm action chosen boltzmann distribut temperatur decreas time atl zs exp qv dsbn hidden unit per time slice localist model use multinomi state learner tabl represent entri train non human algorithm test trial time step human test time step result renorm comparison method result shown figur result neg lower number indic better perform graph error bar show one standard deviat across trial littl perform differ localist represent dsbn expect dsbn exponenti effici hidden state represent linear quadrat approxim perform compar well human perform howev dsbn quadrat approxim sensit initi initi random paramet set fail find good polici howev converg reason polici linear portion quadrat model initi previous learn linear model hidden unit dsbn encod use featur input whether car near nose posit also encod histori current gaze direct advantag simpl stochast polici learn via learn learner know oncom car randomli select look left right dsbn systemat look left right wast fewer action learn factor represent pomdp qcl algorithm qdr figur result new york drive task nine algorithm random learn lc linear multinomi qcr quadrat multinomi random init qcl quadrat multinomi linear init ld linear dsbn qdr quadrat dsbn random init qdl quadrat dsbn linear init human discuss dsbn perform better standard learner compar model localist represent despit use approxim infer exponenti fewer parmet encourag sinc effici encod state prerequisit tackl larger decis problem less encourag valu function approxim compar human perform clear method far optim although factor approxim dsbn hurt perform rel localist multinomi represent sensit initi quadrat approxim worrisom success initi simpler model suggest stage learn may appropri simpl model learn use initi complex model find echo context learn non factor approxim valu function number relat work field reinforc learn bayesian network use sigmoid belief network mean field approxim given discuss context time seri model fulli factor approxim approxim infer dynam bayesian network discuss addit factor valu function use context factor mdp hidden state linear learn approxim given approxim infer combin sophist valu function approxim knowledg first attempt explor practic combin techniqu order solv singl problem sever possibl extens describ represent learn dsbn tune task hand reinforc inform could use guid learn dsbn paramet also done reinforc signal would provid addit evid state pomdp could use aid infer sophist function approxim could use final although method appear work practic guarante reinforc learn converg view work encourag first step much studi requir conclus shown dynam bayesian network use construct compact represent use solv decis problem hidden state paramet dbn learn experi learn occur despit use simpl valu sallan function approxim mean field infer approxim valu function result good perform clearli far optim fulli factor assumpt made belief state valu function appear impact perform compar non factor model algorithm present run entir line perform forward infer much room futur work includ improv util factor represent learn qualiti approxim infer valu function approxim acknowledg thank geoffrey hinton zoubin ghahramani andi brown help discuss anonym refere valuabl comment critic particularli peter dayan help discuss comment earli draft paper research fund nserc canada gatsbi charit foundat refer whitehead ballard learn perceiv act trial error machin learn sondik optim control partial observ markov process infinit horizon discount cost oper research pearl probabilist reason intellig system network plausibl infer morgan kaufmann san mateo ca dean kanazawa model reason persist causat comput intellig gregori cooper comput complex probabilist infer use bayesian belief network artifici intellig neal probabilist infer use markov chain mont carlo method technic report crg tr depart comput scienc univers toronto jordan ghahramani laakkola saul introduct variat method graphic model machin learn press boyen koller tractabl infer complex stochast process proc uai neal connectionist learn belief network artifici intellig saul jaakkola jordan mean field theori sigmoid belief network journal artifici intellig research lawrenc rabin bi hwang juang introduct hidden markov model ieeeassap magazin januari watkin dayan learn machin learn mccallurn reinforc learn select percept hidden state dept comput scienc universiy rochest rochest ny ph thesi littman cassandra kaelbl learn polici partial observ environ scale proc intern confer machin learn ghahramani jordan factori hidden markov model machin learn koller parr comput factor valu function polici structur mdp proc ijcai rodriguez parr koller reinforc learn use approxim belief state solla leen mtiller editor advanc neural inform process system volum mit press cambridg chrisman reinforc learn perceptu alias perceptu distinct approach tenth nation confer ai
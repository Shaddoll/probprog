abstract alreadi known expect return polici markov decis problem alway suitabl optim criterion mani applic control strategi meet variou constraint like avoid bad state risk avoid gener high profit within short time risk seek although might probabl caus signific cost propos modifi learn algorithm use singl continu paramet determin sens result polici optim polici optim respect usual expect return criterion gener solut optim worst case analog closer risk seek polici becom contrast relat approach field mdp transform cost model increas state space order take risk account new approach evalu comput optim invest strategi artifici stock market introduct reinforc learn rl deal comput favor control polici sequenti decis task theoret framework markov decis problem mdp evalu compar polici expect sometim discount averag sum immedi return cost per time step bertseka tsitsikli numer applic requir sophist control scheme polici take account bad outcom state may possibl even rare disastr certainli avoid obviou exampl field financ main question invest resourc among variou opportun asset like stock bond etc achiev remark return simultan control risk exposur invest due chang market econom condit mani trader tri achiev markovitz like portfolio manag distribut capit accord return risk neuneier mihatsch estim asset new approach use reinforc learn techniqu addit integr trade cost market imperfect propos neuneier algorithm natur extend explicit risk control possibl investor decid much risk will accept comput optim risk avers invest strategi similar trade scenario formul robot traffic control applic area fact popular expect valu criterion alway suitabl alreadi known field ai koenig simmon control theori reinforc learn heger szepesv ri sever techniqu propos handl problem obviou way transform sum return rt use appropri util function reflect desir properti solut unfortun interest nonlinear util function incorpor varianc return rt rt rt lead non markovian decis problem popular class exponenti util function rt exp rt preserv markov properti requir time depend polici even discount infinit horizon mdp furthermor possibl formul correspond modelfre learn algorithm altern chang state space model includ past return addit state element cost higher dimension mdp furthermor alway clear way state augment one may also transform cost model punish larg loss stronger minor cost requir signific amount prior knowledg also increas complex mdp contrast approach modifi popular learn algorithm introduc control paramet determin sens result polici optim intuit loos speak algorithm simul learn behavior optimist pessimist person overweight underweight experi posit neg expect main idea made precis section mathemat thoroughli analyz section use artifici data demonstr properti new algorithm construct optim risk avoid invest strategi section risk sensit learn breviti restrict subclass infinit horizon discount markov decis problem mdp furthermor assum immedi reward determinist function current state control action let finit state space finit action space transit probabl immedi reward denot pij gi respect denot discount factor let ii set determinist polici map state control action commonli use object learn polici maxim gi tkgi ik quantifi expect reward one execut control action state follow polici thereaft well known result optim valu maz cn satisfi follow optim equat gi epij max vie ueu eu jss polici argmaxusu optim respect expect reward criterion risk sensit reinforc learn function averag outcom possibl trajectori seri state markov process gener follow polici howev outcom specif realiz markov process may deviat significantli mean valu expect reward criterion consid risk although case discount reward fall consider mean valu live interest mani applic therefor depend applic hand expect reward approach alway appropri altern heger littman szepesvfiri present perform criterion exclus focus risk avoid polici maxim gi inf kgik ik il function denot worst possibl outcom one execut control action state follow polici thereaft correspond optim equat max cn given gi min maxq cup polici satisfi arg max eu optim respect minim reward criterion real world applic approach restrict take rare event practic never happen fulli account usual lead polici lower averag perform applic requir invest manag instanc act respect pessimist object function invest handl trade suffici averag perform risk avoid risk seek behavior propos famili new optim equat parameter meta paramet pij gi maxq vi sign next section show uniqu solut equat exist obvious recov equat optim equat expect reward criterion choos posit overweight neg tempor differ gi max eu respect posit one loos speak overweight transit state futur return lower averag one hand underweight transit state promis higher return averag thu agent behav accord polici argmax risk avoid limit polici approach optim worst case polici show follow section get intuit reader may easili check optim worst case valu fulfil modifi optim equat similarli polici becom risk seek choos neg straightforward formul risk sensit learn algorithm base modifi optim equat let parametr approxim function state action encount time step simul denot ik respect time step appli follow updat rule gik maxo eu neuneier mihatsch denot stepsiz sequenc follow section analyz properti new optim equat correspond learn algorithm properti risk sensit function due space limit abl give detail proof result instead focu interpret practic consequ proof publish elsewher formul mathemat result introduc notat make exposit concis use arbitrari stepsiz defin valu iter oper correspond modifi optim equat ta po max oper ta act space function everi function everi state action pair defin set successor state max attain minimum slpij maxq eu min maxq pi let jen pij probabl transit successor state follow lemma ensur contract properti lemma contract properti let qi maxi eu iq vq oper ta contract becauseo lemma sever import consequ risk sensit optim equat ta uniqu solut valu iter procedur qnew ta converg toward exist converg result tradit learn bertseka tsitsikli tsitsikli van roy remain also valid risk sensit case particularli risk sensit learn converg probabl one case lookup tabl represent well case optim stop problem combin linear represent speed converg risk sensit valu iter learn becom wors itel remedi extent increas stepsiz appropri let greedi polici respect uniqu solut modifi optim equat argmax follow theorem examin perform risk avoid case give us feel expect outcom worst possibl outcom polici differ valu theorem clarifi limit behavior risk sensit reinforc learn theorem let follow inequ hold componentwis pair moreov lim lim differ optim expect reward optim worst case reward crucial inequ measur amount risk inher mdp hand besid valu quantiti essenti influenc differ perform polici optim perform respect expect reward worst case criterion second inequ state perform polici worst case sens tend optim worst case perform speed converg influenc quantiti probabl worst case transit realli occur note bound higher probabl worst case transit impli stronger risk avoid attitud polici experi risk avers invest decis algorithm test task construct optim invest polici artifici stock price analog empir analysi neuneier task illustr mdp fig decid time step day mayor event market whether buy stock therefor specul increas stock price keep capit cash avoid potenti loss due decreas stock price disturbanci invest financi market return investor rate price figur markov decis problem xt state market st portfolio kt polici action transit probabl return function time figur realiz artifici stock price time step obviou price follow increas trend higher valu sudden drop low valu becom probabl assum investor abl influenc market invest decis lead mdp state element uncontrol result two comput import implic first one simul invest histor data without invest potenti lose real money second one formul effici memori save robust learn algorithm due space restrict skip detail descript algorithm refer interest reader neuneier neuneier mihatsch artifici stock price rang transit probabl chosen stock market simul situat price follow increas trend higher valu drop low valu becom probabl fig state vector consist current stock price current invest amount money invest stock cash chang invest cash stock result transact cost consist variabl fix term cost essenti defin invest problem mdp coupl action made differ time step otherwis could solv problem pure predict next stock price function quantifi immedi return time step defin follow capit invest cash noth earn even stock price increas inv stor bought stock return equal rel chang stock price weight invest amount capit minu transact cost appli one chang cash stock fkqe cash cap stock rock price figur left risk neutral polici right small bia risk chang polici one invest transact cost appli case capit calera stock stock price jn stock ltock price stock stock stock stock price stock tock price figur left yield stronger risk avers attitud right polici becom also cautiou alreadi invest stock figur left lead polici invest stock case right worst case solut never invest alway posit probabl decreas stock price reinforc learn method learn interact environ stock market learn optim invest behavior thu train set data point gener train phase divid epoch consist mani trial data train set exist everi trial algorithm select randomli stock price data set choos random invest state updat tabul qvalu accord procedur given neuneier differ new risk avers learn neg experi smaller return mean overweight comparison posit experi use factor eq use differ valu recov origin learn procedur lead worst case learn plot result polici map state space control action figur obvious increas investor act cautious less state associ invest decis stock extrem case stock invest order avoid loss polici use practic support introductori comment worst case learn appropri mani task risk sensit reinforc learn qq plot distribut quantil classic approach figur quantil distribut discount sum return plot quantil classic risk neutral approach distribut differ significantli neg accumul return left tail distribut analysi specifi riski start state io sudden drop stock price near futur probabl start io comput cumul discount reward differ trajectori follow polici gener use risk neutral result three data set compar use quantil quantil plot whose purpos determin whether sampl come distribut type plot linear fig clearli show higher valu left tail distribut neg return bend indic fewer number loss hand signific differ posit quantil contrast naiv util function penal high varianc gener risk sensit learn asymmetr reduc probabl loss may suitabl mani applic conclus formul new learn algorithm continu tune toward risk seek risk avoid polici thu possibl construct control strategi suitabl problem hand small modif learn algorithm advantag approch comparison alreadi known solut neither chang cost state model prove algorithm converg usual assumpt futur work focu connect approach util theoret point view refer bertseka tsitsikli neuro dynam program athena scientif heger consider risk reinforc learn machin learn proceed lth intern confer morgan kaufmann publish koenig simmon risk sensit plan probabilist decis graph proc fourth int conf principl knowledg represent reason kr littman cs szepesvfiri gener reinforc learn model converg applic intern confer machin learn bari neuneier enhanc learn optim asset alloc advanc neural inform process system cambridg mit press puterman markov decis process john wiley son cs szepesvfiri non markovian polici sequenti decis problem acta cybernetica tsitsikli van roy approxim solut optim stop problem advanc neural inform process system cambridg mit press
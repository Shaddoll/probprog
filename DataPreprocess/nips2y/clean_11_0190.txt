abstract comput upper lower bound vc dimens feedforward network unit piecewis polynomi activ function show number layer fix vc dimens grow log number paramet network result stand opposit case number layer unbound case vc dimens grow introduct vc dimens import measur complex class binaryvalu function sinc character amount data requir learn pac set see behw vap paper establish upper lower bound vc dimens specif class multi layer feedforward neural network let jr class binari valu function comput feedforward neural network weight comput non input unit piecewis polynomi activ function goldberg jerrum gj shown vcdim cl wk constant moreov koiran sontag ks demonstr network vcdim would lead one conclud bound almost linear vc dimens bound piecewis polynomi network fact tight constant howev proof use ks establish lower bound made use fact number layer grow practic applic number often small constant thu question remain whether possibl obtain better bound realist scenario number layer fix contribut work proof upper lower bound vc dimens piecewis polynomi net upper bound behav wl wl log wl number layer fix log superior previou best result behav moreov use idea ks gj abl deriv lower bound vc dimens wl maass maa show three layer network threshold activ function binari input vc dimens log sakurai sak show also true two layer network threshold activ function real input easi show result impli similar lower bound threshold activ function replac piecewis polynomi activ function bound distinct limit limx limx thu conclud number layer fix vc dimens piecewis polynomi network layer real input piecewis polynomi network layer binari input grow log note piecewis polynomi network consid work easi show vc dimens pseudo dimens close relat see vid similar bound differ constant hold pseudo dimens independ sakurai obtain similar upper bound improv lower bound vc dimens piecewis polynomi network see sak upper bound begin technic discuss precis definit vc dimens class network consid work definit let set system subset set xl xh shatter everi subset exist set vc dimens denot vcdim largest integ exist set cardin shatter intuit vc dimens measur size largest set point possibl label may achiev set often conveni talk vc dimens class indic function jc case simpli identifi set point subset use notat vcdim jc feedforward multi layer network direct acycl graph repres parametr real valu function real input node call either input unit comput unit comput unit arrang layer edg allow input unit comput unit also edg comput unit anoth comput unit first unit lower layer second singl unit final layer call output unit input unit associ real valu one compon input vector comput unit associ real valu call unit output valu edg associ real paramet comput unit output comput unit given weze sum rang set edg lead bartlett maiorov meir unit paramet weight associ edg ze output valu unit edg emerg paramet bia associ unit rr call activ function unit argument rr call net input unit suppos unit except output unit activ function fix piecewis polynomi function form qsi ti ti set tp polynomi degre say rr break point degre activ function output unit ident function let ki denot number comput unit layer suppos total paramet weight bias comput unit input paramet vector tm let denot output network let jr tm denot class function comput architectur vari paramet first discuss comput vc dimens thu consid class function sgn sgn rw give main theorem section present follow result slight improv result due warren see abar chapter lemma suppos fl fix polynomi degre variabl number distinct sign vector sgn sgn fm gener vari eml main result theorem posit integ consid network real input paramet comput unit arrang layer singl output unit ident activ function comput unit piecewis polynomi activ function degre break point let class real valu function comput network vcdim sgn wllog ewlpk wl og sinc fix impli vcdim sgn wl log wl present proof outlin main idea construct fix input output network correspond piecewis polynomi function paramet degre larger recal last layer linear thu paramet domain tm split region function polynomi lemma possibl obtain upper bound number sign assign attain vari paramet set polynomi theorem establish combin bound bound number region proof theorem arbitrari choic point xm wish bound sgn sgn xm almost linear vc dimens bound piecewis polynomi network fix point consid partit se sn paramet domain clearli sgn xi sgn xm choos partit within region xi zm fix polynomi degre lemma term sum remain point construct partit determin upper bound size partit construct recurs use follow procedur let partit constant bn sgn pn ti bn wherej andi ti breakpoint piecewis polynomi activ function pn affin function describ net input th unit first layer respons xj ph ah xj ah weight th unit first layer note partit determin sole paramet correspond first hidden layer input layer unaffect paramet clearli output first layer unit respons xj fix polynomi let number variabl use comput unit output layer respect let number comput unit layer respect recal choos number sign assign possibl mklp arlin function variabl lemma show erakip defin follow assum xj net input everi unit layer respons xj fix polynomi function degre let partit refin constant bn sgn pn ti bh pn polynomi function describ net input th unit th layer respons xj sinc impli output th layer unit respons xj fix polynomi degre final choos number sign assign mk polynomi variabl degre lemma emk notic also net input everi unit layer bartlett maiorov meir respons xj fix polynomi function degre proceed way get partit network output respons xj fix polynomi degre furthermor emklp erakip emkip ii wi multipli bound give result emkip wi sinc point xl xm chosen arbitrarili give bound maxim number dichotomi induc point upper bound vc dimens obtain comput largest valu number least yield empki zwilog wlog wlog empk logarithm base conclud see exampl vid lemma vcdim wlog wlog ewlpk briefli mention applic result problem learn regress function yix input output pair xi drawn independ random unknown distribut case quadrat loss one show exist constant cl mpdim log el inf yix nois varianc yix approxim error function class jr approxim minim sampl averag quadrat loss make use recent deriv bound mm approxim error inff equal logarithm factor obtain network unit standard sigmoid function combin consider lower pseudo dimens bound piecewis polynomi network obtain much better error rate current avail sigmoid network lower bound comput lower bound vc dimens neural network continu activ function result gener lower bound ks sinc hold number layer almost linear vc dimens bound piecewis polynomi network theorem suppos follow properti lima ct lim ct differenti point xo deriv xo feedforward network follow properti network layer paramet output unit linear unit comput unit activ function set sgn function comput network vcdim sgn luj largest integ less equal proof ks proof follow theorem gj show function describ comput network keep track number paramet layer requir first prove lower bound network contain linear threshold unit linear unit ident activ function show except output unit replac unit activ function result network still shatter set detail proof see full paper bmm fix posit integ construct set mn point may shatter network weight layer let ai denot set paramet ai bit binari represent ai jai ai bit base two represent ai ai ai ai ai consid input bm ei ei th bit bit bm defin similarli show extract bit ai input et network output al sinc nm input form al take possibl valu result follow three stage comput al comput extract everi select al among ks suppos network input et em use one linear unit comput uiai involv paramet one comput unit one layer fact need paramet need extra paramet show linear unit replac unit activ function consid paramet ck ck jat sinc ck iff al clearli sgn ck al also ck ck thu consid recurs ck ck al sgn ck initi condit al sgn clearli comput anoth layer use paramet comput unit could comput way follow approach give fewer layer setb sgn cm vi ifm mthenb ifm input vector thu vi impli sgn cm sgn bartlett maiorov meir order conclud proof need show variabl al may recov depend input vm al vim vi sinc boolean sgn xi sgn xi see comput al involv addit paramet comput unit add anoth layer total layer paramet network shatter set size nm clearli add paramet layer without affect function network set least provid case vc dimens least network construct use linear threshold unit linear unit howev easi show see ks theorem unit except output unit replac unit activ function network still shatter set size mn linear unit input output weight scale linear function approxim sut ficient accuraci neighborhood point linear threshold unit input weight scale behavior infin accur approxim linear threshold function refer abar behw bmm ks maa mm sak sak vap vid anthoni bartlett neural network learn theoret foundat cambridg univers press appear blumer ehrenfeucht haussler warmuth learnabl vapnik chervonenki dimens acm bartlett maiorov meir almost linear vc dimens bound piecewis polynomi network neural comput goldberg jerrum bound vc dimens concept class parameter real number machin learn koiran sontag neural network quadrat vc dimens journal comput system scienc maass neural net superlinear vc dimens neural comput maiorov meir near optim stochast approxim smooth function neural network submit public sakurai tighter bound vc dimens three layer network world congress neural network volum page hillsdal erlbaum sakurai tight bound vc dimens piecewis polynomi network advanc neural inform process system volum mit press vapnik estim depend base empir data springer verlag new york vidyasagar theori learn gener springer verlag new york
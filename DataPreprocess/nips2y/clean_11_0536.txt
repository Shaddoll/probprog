abstract kernel pca nonlinear featur extractor proven power preprocess step classif algorithm also consid natur gener linear princip compon analysi give rise question use nonlinear featur data compress reconstruct de nois applic common linear pca nontrivi task result provid kernel pca live high dimension featur space need pre imag input space work present idea find approxim pre imag focus gaussian kernel show experiment result use pre imag data reconstruct de nois toy exampl well real world data introduct princip compon analysi pca orthogon basi transform new basi found diagon center covari matrix data set xk rvlk defin xi xk xi xk coordin eigenvector basi call princip compon size eigenvalu correspond eigenvector equal amount varianc direct furthermor direct first eigenvector correspond biggest eigenvalu cover much varianc possibl orthogon direct mani applic contain interest inform instanc data compress project onto direct biggest varianc retain much inform possibl de nois deliber drop direct small varianc clearli one cannot assert linear pca alway detect structur given data set use suitabl nonlinear featur one extract inform kernel pca well suit extract interest nonlinear structur data purpos work therefor consid nonlinear de nois base kernel pca ii clarifi connect featur space expans meaning pattern input space kernel pca first map data featur space via usual nonlinear function perform linear pca map data featur space might high dimension map space possibl th order monomi input space kernel pca employ mercer kernel instead carri kernel pca de nois featur space map explicitli mercer kernel function data set xi give rise posit matrix kij one show use instead dot product input space correspond map data featur space kernel proven use includ gaussian kernel exp ix ull polynomi kernel clearli algorithm formul term dot product support vector machin carri featur space without map data explicitli algorithm construct solut expans potenti infinit dimension featur space paper organ follow next section briefli describ kernel pca algorithm section present algorithm find approxim pre imag expans featur space experiment result toy real world data given section follow discuss find section kernel pca reconstruct perform pca featur space need find eigenvalu eigenvector satisfi av xk xk substitut eigenvector equat note solut must lie span imag train data impli consid equival system xk xk exist coeffici oil ot xi substitut defin matrix kij xi xj arriv problem cast term dot product solv xot kot oil ot detail see normal solut translat extract nonlinear princip compon imag test point comput project onto th compon xi featur extract thu evalu vk el kernel function instead dot product expens high dimension gaussian kernel infinit dimension reconstruct imag vector project onto first princip compon assum eigenvector order decreas eigenvalu size defin project oper pn pn larg enough take account direct belong eigenvector nonzero eigenvalu pn otherwis kernel pca still satisfi overal squar reconstruct error ii pn ii minim ii retain varianc maxim among project onto orthogon direct common applic howev interest reconstruct input space rather present work attempt achiev comput vector satisfi pn hope kernel use good approxim input space howev alway exist ii exist simplic assum map data center otherwis go algebra use xi mika et al need uniqu exampl consid possibl represent one show thought map hilbert space function cti mi dot product satisfi gt gt call reproduc kernel hilbert space gaussian kernel contain linear superposit gaussian bump plu limit point wherea definit singl bump pre imag vector pn pre imag tri approxim minim ii pn special case reduc set method replac term independ obtain ii pn substitut arriv express written term dot product consequ introduc kernel obtain formula thu vz reli carri explicitli mi pre imag gaussian kernel optim employ standard gradient descent method restrict attent kernel form llm thu satisfi const optim determin follow cf deduc maxim pn ft ik xi fy ft independ extremum set gradient respect vanish ik llz lead necessari condit extremum gaussian kernel exp im get exp iiz mill mi ei exp llz note denomin equal pn cf make assumpt pn pn pn pn smooth conclud exist neighborhood extremum denomin thu devis iter scheme exp llzt ii zt ei exp ilzt numer instabl relat pn small dealt restart iter differ start valu furthermor note fix point linear combin kernel pca train data mi regard context cluster see resembl iter step estim kernel allow reconstruct dot product input space assumpt pre imag exist possibl construct explicitli cf clearli condit hold true gener kernel pca de nois featur space center singl gaussian cluster weight probabl reflect anti correl amount eigenvector direct contribut eigenvector cluster center drawn toward train pattern posit push away neg fix point influenc train pattern smaller distanc tend bigger experi test feasibl propos algorithm run sever toy real world experi perform use gaussian kernel form exp llx equal dimens input space mainli focus applic de nois differ reconstruct fact allow make use origin test data start point iter toy exampl first experi tabl gener data set eleven gaussian io zero mean varianc compon select sourc point train set point test set center gaussian randomli chosen appli kernel pca train set comput project point test set carri de nois yield approxim pre imag io test point procedur repeat differ number compon reconstruct differ valu kernel use compar result provid algorithm linear pca via mean squar distanc de nois test point correspond center tabl show ratio valu ratio larger one indic kernel pca perform better linear pca almost everi choic kernel pca better note use compon linear pca basi transform henc cannot de nois extrem superior kernel pca small due fact test point case locat close eleven spot input space linear pca cover less ten direct kernel pca move point correct sourc even use small number compon tabl de nois gauss an io see text perform ratio larger one indic much better kernel pca compar linear pca differ choic gaussian std dev differ number compon use reconstruct get intuit understand low dimension case figur depict result de nois half circl squar plane use kernel pca nonlinear autoencod princip curv linear pca princip curv algorithm iter estim curv captur structur data data project closest point curv algorithm tri construct point averag data point project onto shown straight line satisfi latter princip compon princip curv gener latter algorithm use smooth paramet anneal iter nonlinear autoencod algorithm bottleneck layer network train reproduc input valu output use autoassoci mode hidden unit activ third layer form lower dimension represent data close relat mika et al pca see instanc train done conjug gradient descent algorithm paramet valu select best possibl de nois result obtain figur show close squar problem kernel pca subject best follow princip curv nonlinear autoencod linear pca fail complet howev note algorithm except kernel pca actual provid explicit one dimension parameter data wherea kernel pca provid us mean map point de nois version case use four kernel pca featur henc obtain four dimension parameter kernel pca nonlinear autoencod princip curv linear pca ii figur de nois see text depict data set small point de nois version big point join solid line linear pca use one compon reconstruct use two compon reconstruct perfect thu de nois note algorithm except approach problem captur circular structur bottom exampl usp exampl test approach real world data also appli algorithm usp databas dimension handwritten digit ten digit randomli chose exampl train set exampl test set use gaussian kernel equal twice averag data varianc dimens figur give two possibl depictlob figur visual eigenvector see text depict th eigenvector left right first row linear pca second third row differ visual kernel pca eigenvector found kernel pca compar found linear pca usp set second row show approxim pre imag eigenvector found algorithm third row imag comput follow pixel project imag th canon basi vector input space onto correspond eigenvector featur space upper left el lower right linear case method would simpli yield eigenvector linear pca depict first row sens may consid gener eigenvector input space see first eigenvector almost ident except sign also see eigenvector linear pca start concentr highfrequ structur alreadi smaller eigenvalu size understand note linear pca maximum number eigenvector contrari kernel pca give us number train exampl possibl eigenvector kernel pca de nois featur space figur reconstruct usp data depict reconstruct first digit test set origin last column first compon linear pca first row kernel pca second row case number denot fraction squar distanc measur toward origin exampl small number compon algorithm nearli compon see linear pca yield result resembl origin digit wherea kernel pca give result resembl prototyp three also explain result found work usp set experi linear kernel pca train origin data ad addit gaussian nois zero mean standard deviat ii speckl nois probabl pixel flip black white probabl test set noisi test set comput project onto first linear nonlinear compon carri reconstruct case result compar take mean squar distanc reconstruct digit noisi test set origin counterpart third experi origin test set henc reconstruct de nois latter case task reconstruct given exampl exactli possibl linear pca better least use compon figur due fact linear pca start earlier account fine structur time start reconstruct nois see figur kernel pca hand yield recogniz result even small number compon repres prototyp desir exampl one reason approach better linear pca de nois exampl figur take mean squar distanc measur whole test set optim number compon linear kernel pca approach better factor gaussian nois time better speckl nois optim number compon linear pca kernel pca respect take ident number compon algorithm kernel pca becom time better linear pca howev note kernel pca come higher comput complex discuss studi problem find approxim pre imag vector featur space propos algorithm solv algorithm appli reconstruct de nois former case result compar linear pca latter case obtain significantli better result interpret find follow linear pca extract compon dimension data basi transform compon togeth fulli describ data data noisi impli certain fraction compon devot extract nois kernel pca hand allow extract featur number train exampl accordingli kernel pca provid larger number featur carri inform structur data experi addit structur extract nonlinear linear pca must necessarili fail illustr toy exampl method along depict pre imag vector featur space provid understand kernel method recent attract increas attent open question includ kind result kernel gaussian provid mikaet gaussian nois speckl nois noisi figur de nois usp data see text left half show top first occurr digit test set second row upper digit addit gaussian nois follow five row reconstruct linear pca use compon last five row result approach use number compon right half show speckl nois probabl ii whether effici way solv either iii comparison connect altern nonlinear de nois method cf refer boser guyon vapnik train algorithm optim margin classifi haussler editor proc colt page pittsburgh acm press burg simplifi support vector decis rule saitta editor prooceed th icml page san mateo ca diamantara kung princip compon neural network wiley new york hasti stuetzl princip curv jasa mallat zhang match pursuit time frequenc dictionari ieee transact signal process decemb saitoh theori reproduc kernel applic longman scientif technic harlow england sch kopf support vector learn oldenbourg verlag munich sch kopf knirsch smola burg fast approxim support vector kernel expans interpret cluster approxim featur space levi et al editor dagm page berlin springer sch kopf smola mtiller nonlinear compon analysi kernel eigenvalu problem neural comput
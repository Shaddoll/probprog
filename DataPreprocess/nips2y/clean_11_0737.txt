abstract paper introduc method regular hmm system avoid paramet overfit caus insuffici train data regular done augment em train method penalti term favor simpl smooth hmm system penalti term construct mixtur model neg exponenti distribut assum gener state depend emiss probabl hmm new method success transfer well known regular approach neural network hmm domain interpret gener tradit state tie hmm system effect regular demonstr continu speech recognit task improv overfit triphon model speaker adapt limit train data introduct one gener problem construct statist pattern recognit system ensur capabl gener well system must abl classifi data contain train data set henc classifi learn true underli data distribut instead overfit data exampl seen system train one way cope problem overfit balanc system complex flexibl limit amount data avail train neural network commun well known amount inform use system train requir good gener perform larger number adjust weight baum common method train larg size neural network suffici well reduc number adjust paramet either remov weight seem less import le cun sensit individu network weight estim second order gradient share neukirchen rigoil weight among mani network connect lang connect share ident weight valu determin advanc use prior knowledg invari problem solv second approach avoid overfit neural network make use regular method reguladz add extra term train object function penal network complex simplest regular method weight decay plaut assign high penalti larg weight complex regular term use soft weight share nowlan favor neural network weight fall finit set small weight cluster tradit neural weight share techniqu interpret special case soft weight share regular cluster varianc tend toward zero continu speech recognit hidden markov model hmm method common use detail context depend triphon hmm number hmm state paramet estim state depend probabl densiti function pdf increasingli larg overfit becom seriou problem common approach balanc complex triphon hmm system train data set reduc number paramet tie paramet share young popular share method state tie select hmm state tie advanc either data driven state cluster base pdf depend distanc metric young construct binari decis tree incorpor higher phonet knowledg bahl method number state cluster decis tree size respect must chosen adequ match train data size howev possibl drawback method two differ state may select tie pdf forc ident although enough train data estim differ pdf state suffici well follow method reduc complex gener hmm system base regular term present due close relationship soft weight share method neural network novel approach interpret soft state tie maximum likelihood train hmm system tradit method commonli use determin set adjust paramet hmm system maximum likelihood ml estim via expect maxim em algorithm train observ vector sequenc denot correspond hmm denot ml estim given argm ax logpo xlw follow total number differ hmm state given emiss pdf th state denot bk continu hmm bk mixtur gaussian pdf commonli case discret hmm observ vector map vector quantiz vq discret vq label emiss pdfi replac discret output probabl forward backward algorithm probabilist state count determin train observ log likelihood train data decompos auxiliari function optim em step state transit probabilitieg neglect logb sometim observ vector split sever independ stream total number stream given featur th stream compris subvector case applic vq correspond vq label denot control complex hmm system regular observ subvector differ stream assum statist independ thu state pdf written complex measur hmm system use regular method train hmm system tradit object train function augment complex penal term new optim problem becom reg argm ft regul term small hmm system high complex paramet overfit becom problem larg hmm state pdf shape smoothli system gener work well constant control paramet adjust tradeoff pure ml solut smooth penal eqn term becom larger data use train make ml estim becom reliabl influenc term ft get less import rel basic idea construct express regul ft favor smooth hmm system case simpl smooth system state depend emiss pdf bk fall sever group similar pdf contrast tradit state tie forc ident pdf group follow cluster similar emiss pdf describ probabilist mixtur model pdf assum gener mixtur differ mixtur compon pi case probabl densiti gener emiss pdf bk given pi mixtur weight ci constrain ci ci th mixtur compon pi use model th cluster hmm emiss pdf cluster repres prototyp pdf denot th cluster distanc use suitabl metric hmm emiss pdfb th prototyp pdf denot di distanc small hmm emiss probabl sever small cluster emiss probabl regul term larg assum distanc follow neg exponenti distribut deviat paramet ai yield express mixtur compon pi bk ai exp ai zw eqn gener case independ stream given henc hmm emiss pdf cluster prototyp pdf split differ pdf respect stream depend distanc di paramet ai use regul term log likelihood mixtur model eqn emiss pdf hmm system use logp neukirchen rigol regular exampl discret hmm exampl paramet estim regular framework discret hmm system differ vq stream consid vq subdivid featur space jz differ partit th codebook size jz vq partit label denot observ subvector th vq partit vq output fn llt sinc discret kind hmm output probabl rh use regul prototyp discret probabl mr distanc metric hmm emiss probabl prototyp probabl use eqn asymmetr kullback leibler diverg appli estim hmm paramet use regular paramet set hmm system estim mainli consist discret hmm emiss probabl transit probabl subject regular get iter paramet estim em style eqn must maxim set deriv eqn respect hmm paramet mj zero applic lagrang multipli regard constraint ja jz lead quit complex solut solv numer optim problem simplifi mixtur eqn replac maximum approxim maximum compon sum consid correspond index maximum compon denot bk ci pi bk max ci pi bk simplifi case hmm paramet estim given weight sum well known ml solut regul prototyp probabl select maximum search eqn larger valu constant stronger forc push estim hmm emiss probabil toward prototyp probabl situat tend toward iti raj infin correspond case tradit state tie differ state fall cluster make use emiss probabl th stream estim regul paramet paramet set regul consist mixtur weight ci deviat parmet hi discret prototyp probabl ra case regul control complex hmm system regular discret hmm paramet set advanc make use prior knowledg prototyp probabl obtain simpl hmm system use small number state altern regul paramet estim similar way nowlan maxim eqn sinc direct solut optim problem maxim must perform em like iter procedur use hmm emiss pdf bk train data mixtur model increas follow auxiliari function step ilbk logp bk ilbk log ci pi bk posterior probabl use weight factor given ci pi bk ilbk cl pl bk maxim eqn perform set deriv respect regul paramet zero consider constraint jz iz lagrang multipli esi ci applic timat regul paramet yield ilbk ilbk ek di lp ilbk ogbk exp yt ekk ibk llbk logb ek exp ek llbk estim fii interpret averag probabl hmm emiss probabl fall th mixtur cluster invers weight averag distanc emiss probabl prototyp probabl estim averag obabi ty emiss probabl vq label rnj weight log domain euclidean distanc discret probabl use instead eqn measur differ hmm emiss probabl prototyp rnj estim prototyp probabl given averag hmm probabl weight origin space mj neukirchen rigol experiment result investig perform regular method describ hmm speech recognit system speaker independ resourc manag rm continu speech task built train sentenc differ speaker use recognit result given word error rate averag offici darpa rm test set feb oct feb sep consist sentenc differ speaker total recognit done via beam search guid viterbi decod use darpa rm word pair grammar perplex acoust featur everi ms mfcc coeffici rel signal power extract speech signal along dynam featur compris featur per frame hmm system make use standard state discret probabl phonet model four differ neural network train mmi method describ rigol extend neukirchen use vq quantiz featur differ stream discret label codebook size stream set simpl system model monophon promin function word total state yield word error rate system make use detail unti word intern triphon model total state yield word error henc hmm overfit insuffici train data sever problem case tradit method overcom effect overfit like interpol triphon monophon bahl data driven state cluster decis tree cluster yield error rate respect must note contrast usual train procedur rigol smooth method appli hmm emiss probabl first seri experi unti triphon system regul quit simpl mixtur densiti compon number cluster penalti term ident number state monophon system case prototyp probabl initi emiss probabl monophon system mixtur weight deviat paramet regul set uniform initi order test inluenc tradeoff paramet set respect correspond word error rate respect case larg vs regular degrad tie triphon state monophon state error rate tend toward monophon system perform smaller vs good tradeoff data fit hmm smooth yield improv system perform initi prototyp probabl set provid monophon system seem chang much regul paramet estim sinc system perform chang slightli regul paramet reestim incorpor preliminari experi regular method also use speaker adapt speaker independ system train wall street journal wsj databas yield error rate nov test set differ non nativ speaker speaker independ hmm emiss probabl use initi prototyp probabl regul speaker depend system built speaker use fast enrol sentenc train along regular set error rate drop better speaker adapt method describ rottland yield linear featur space transform combin method achiev word error summari discuss method avoid paramet overfit hmm system applic regular term favor smooth simpl model present complex control complex hmm system regular measur appli hmm base finit mixtur neg exponenti distribut gener state depend emiss probabl kind regular term interpret soft state tie sinc forc hmm emiss probabl form finit set cluster effect regular demonstr rm task improv overfit triphon model wsj non nativ speaker adapt task limit train data regular outperform featur space transform eqn may also interpret perspect bayesian infer term ft play role set prior distribut hmm paramet estim henc use mixtur model ft equival use special kind prior framework map estim hmm gauvain refer bahl jelinek mercer maximum likelihood approach continu speech recognit ieee tran pattern analysi machin intellig vol mar pp bahl de souza gopalakrishnan nahamoo picheni context depend model phone continu speech use decis tree proc darpa speech natur languag process workshop baum haussler size net give valid gener neural comput le cun denker solla howard jackel optim brain damag advanc neural inform process system san mateo ca morgan kauffman gauvain lee maximum posteriori estim multivari gaussian mixtur observ markov chain ieee transact speech audio proc vol lang waibel hinton time delay neural network architectur isol word recognit neural network ch neukirchen willett eickel miiller exploit acoust featur correl joint neural vector quantiz design discret hmm system proc icassp nowlan hinton simplifi neural network soft weight share neural comput plaut nowlan hinton experi learn backpropag technic report cmu cs carnegi mellon univers pittsburgh pa rigoil ch neukirchen rottland new hybrid system base mmi neural network rm speech recognit task proc icassp rigol ch neukirchen new approach hybrid hmm ann speech recognit use mutual inform neural network advanc neural inform process system cambridg mit press rottland ch neukirchen rigol sl eaker adapt hybrid mmiconnectionist speech recognit system proc icassp young gener use tie phonem base hmm speech recogn proc icassp young woodland use state tie continu speech recognit proc eurospeech
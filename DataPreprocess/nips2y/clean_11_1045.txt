abstract present method automat construct macro action scratch primit action reinforc learn process overal idea reinforc tendenc perform action action pattern action reward test method bicycl task car hill task race track task grid world task bicycl race track task use macro action approxim halv learn time one grid world task learn time reduc factor method work car hill task reason discuss conclus introduct macro action sequenc action chosen primit action problem lurep action togeth macro great help solv larg problem korf gullap sometim greatli speed learn iba mcgovern sutton fagg mcgovern sutton sutton precup singh sutton singh precup ravindran macro action might essenti scale reinforc learn larg problem construct macroact hand requir insight problem hand would eleg use agent could decid action lump togeth iba mcgovern sutton sutton precup singh hauskrecht et al iba mcgovern sutton sutton precup singh hauskrecht et al special case definit macro action seen elsewher research take macro action consist polici termin condit input set precup sutton sutton precup singh sutton singh precup ravindran other defin local polici hauskrecht et al randlov action action map reinforc learn want learn map state action maxim total expect reward sutton barto sometim might use learn map action action well believ act accord action action map use three reason earli stage learn agent enter area state space never visit agent act accord action action map might guid area yet clear choic action otherwis word much like action action map could guid agent perform almost optim state never visit random polici situat instanc emerg use perform certain open loop sequenc action without guid state inform consid instanc agent learn balanc bicycl randl alstrom bicycl unbalanc state agent must forget posit bicycl carri sequenc action balanc bicycl state inform posit bicycl rel goal matter might actual distract agent histori recent action might contain need inform pick next action action action map might lead agent explor relev area state space effici way instead hit chanc therefor expect learn action action map addit state action map lead faster overal learn even though system markov properti may use rememb bit action histori want agent perform sequenc action awar develop state control state mani peopl tri deal imperfect state inform ad memori previou state action inform agent receiv andrea cashin mccallum hansen barto zilberstein burgard et al work special concern non markov problem howev result paper suggest method partial observ mdp could appli mdp result faster learn difficult part combin suggest made action action map convent state action map obvious want learn map st tabular form sinc would destroy possibl use action action map generalis state space approach decid learn two valu map map qs convent valu normal use state action map map qa repres valu belong action action map make choic add valu suggest made two map normal use new valu pick action usual way valu actual use pick next action paramet determin influenc action action map back usual valu idea reinforc tendenc perform action action pattern action reward way agent form habit macro action sometim act accord learn macro action reinforc learn result implement action action map valu mani algorithm develop find near optim state action map trial error basi exampl algorithm sarsa develop rummeri niranjan rummeri niranjan rummeri use sarsa replac elig trace singh sutton tabl look elig trace attach qa valu one action action pair learn qs qa valu adjust accord overal td error rt st st updat qa valu form descript action action weight state action weight neuron input layer figur one think action action map term weight output neuron network calcul valu sarsa algorithm see rummeri sutton barto figur show idea term neural network hidden layer new qa valu correspond weight output neuron output neuron bicycl first test new valu bicycl system solv problem agent learn balanc bicycl second therebi ride km time step agent receiv inform state bicycl angl angular veloc handlebar angl angular veloc angular acceler angl bicycl vertic agent choos two basic action torqu appli handl bar much centr mass displac bicycl plan total possibl action randl rv alstmm reward time step unless bicycl fallen case agent use descript equat system refer reader origin paper figur show learn time vari valu error bar show standard error graph small valu agent learn task faster usual sarsa expect larg valu slow learn figur learn time function paramet bicycl experi point averag run car hill second exampl boyan moor mountain car task boyan moor singh sutton sutton consid drive power car steep mountain road problem graviti stronger car engin car cannot acceler slope agent must first move car away goal one action taken state allow trace action continu decay instead cut contrari singh sutton singh sutton randlov opposit slope appli full throttl build enough momentum reach goal reward time step agent reach goal receiv reward agent must choos one three possibl action time step full thrust forward thrust full thrust backward refer singh sutton equat task use one sarsa agent five cmac tile thoroughli examin singh sutton agent paramet greedi select action best valu found singh sutton singh sutton treatment problem agent tri trial trial one run randomli select start state goal agent use set start state perform measur averag trial time first trial figur show result two simul obvious action action weight use agent sinc lowest point oo oo oo io figur averag trial time trial function paramet car hill point averag run race track problem race track problem origin present barto bradtk singh agent control car race track agent must guid car start line finish line least number step possibl exact posit start line randomli select state given posit veloc px vx integ valu total number reachabl figur exampl nearoptim path race track problem start line left finish line upper right state track shown fig step car acceler dimens thu agent possibl combin action choos figur show posit near optim path agent receiv reward step make without reach goal hit boundari track besid punish hit boundari track fact agent choic action alway carri problem state barto bradric singh rummeri agent paramet ot learn process divid epoch consist trial consid task learn agent navig car start goal averag less time step one full epoch learn time deftn number first epoch criterion met learn criterion emphas stabl leam agent need abl solv problem sever time row figur learn time function paramet race track point averag run learn macro action reinforc learn figur learn time function paramet grid world task dimension grid world state left dimension grid world state right point averag run figur show learn time vari valu larg rang small valu see consider reduct learn time epoch epoch larg valu slow learn grid world task tri new method set grid world problem dimens problem start point locat dimens goal locat dimens dimens dimension problem agent action choos action move ith dimens action move ith dimens agent receiv reward step make without reach goal reach goal agent tri step outsid boundari world maintain posit dimension problem take place grid world dimension world dimens size learn process divid epoch consist trial task consid learn agent navig start goal averag less fix number dimens dimens one full epoch agent use ot figur show result grid world task learn time reduc lot useful new method seem improv number action action better work figur show one clear untyp set valu action action weight dimension figur learn time function paramet dimension grid world state point averag run figur valu action action weight darker squar stronger relationship randlev problem recommend action mark white agent learn two macro action agent perform action number continu perform action thing equal macro action consist cycl action reason choic one rout goal consist perform action task mani action final tri problem larg number action world time meter squar instead pick dimens advanc agent choos direct angular space consist part exact posit agent discreet box time meter goal squar center side measur agent move per time step receiv reward reach goal otherwis task consid learn agent navig start goal averag less time step one full epoch trial looo figur learn time function paramet point averag run note logarithm scale figur show learn curv learn time reduc factor real differ compar grid world problem number action result therefor indic larger number action better method work conclus discuss present new method calcul valu mix convent valu state action map valu action action map test method number problem found problem except one method reduc total learn time furthermor agent found macro learn valu function base valu state action action action pair guarante converg inde larg valu method seem unstabl larg varianc learn time good strategi could start high initi gradual decreas valu empir result indic th use method depend number action action better work also intuit reason inform content knowledg particular action perform higher agent action choos acknowledg author wish thank andrew barto preben alstrom doina precup ami mcgovern use comment suggest earlier draft paper richard sutton matthew schlesing help discuss also lot thank david cohen patienc later last minut correct learn macro action reinforc learn refer andrea cashin learn machin monologu intern journal man machin studi barto bradtk singh learn act use real time dynam program artifici intellig boyan moor gener reinforc learn safe approxim valu function nip pp mit press burgard cremer fox haehnel lakemey schulz steiner thrun interact museum tour guid robot fifteenth nation confer artifici intellig gullap reinforc learn applic control phd thesi univers massachusett coin technic report hansen barto zilberstein reinforc learn mix open loop close loop control nip mit press hauskrecht meuleau boutili kaelbl dean hierarch solut markov decis process use macro action proceed fourteenth intern confer uncertainti artifici intellig iba heurist approach discoveri macro oper machin learn korf learn solv problem search macro oper research note artifici intellig korf macro oper weak method learn artifici intellig mccallum reinforc learn select percept hidden state phd thesi univers rochest mcgovern sutton macro action reinforc learn empir analysi technic report univers massachusett mcgovern sutton fagg role macro action acceler reinforc learn grace hopper celebr women comput precup sutton multi time model tempor abstract plan nip press randlen alstr learn drive bicycl use reinforc learn shape proceed th intern confer machin learn rummeri problem solv reinforc learn phd thesi cambridg univers engin depart rummeri niranjan line learn use connectionist system technic report cu infeng tr engin depart cambridg univers singh sutton reinforc learn replac elig trace machin learn sutton gener reinforc learn success exampl use spars coars code nip pp mit press sutton barto introduct reinforc learn mit press bradford book sutton precup singh mdp semi mdp learn plan repres knowledg multipl tempor scale technic report um cs depart comput scienc umass sutton singh precup ravindran improv switch among tempor abstract action nip mit press
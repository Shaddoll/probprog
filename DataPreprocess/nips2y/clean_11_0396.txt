abstract infer key compon learn probabilist model partial observ data learn tempor model mani infer phase requir travers entir long data sequenc furthermor data structur manipul exponenti larg make process comput expens describ approxim infer algorithm monitor stochast process prove bound approxim error paper appli algorithm approxim forward propag step em algorithm learn tempor bayesian network provid relat approxim backward step prove error bound combin algorithm show empir real life domain em use infer algorithm much faster em use exact infer almost degrad qualiti learn model extend analysi onlin learn task show bound error result restrict attent small window observ present onlin em learn algorithm dynam system show learn much faster standard offiin em introduct mani real life situat face task induc dynam complex stochast process limit observ state time hidden markov model hmm play largest role represent learn model stochast process recent howev increas use structur model stochast process factori hmm dynam bayesian network dbn structur decompos represent allow complex process larg number state encod use much smaller number paramet therebi allow better gener limit data furthermor natur structur process make easier human expert incorpor prior knowledg domain structur model therebi improv induct bia approxim learn dynam model paramet structur learn algorithm dynam model use probabilist infer crucial compon infer routin call multipl time order fill miss data expect valu accord current hypothesi result expect suffici statist use construct new hypothesi infer step use mani time iter entir sequenc behavior problemat two import respect first mani set may access entir sequenc advanc second variou structur represent stochast process admit effect infer procedur messag propag exact infer algorithm includ entri possibl state system number state exponenti size model render type comput infeas smallest problem paper describ analyz approach help us address problem propos new approach approxim infer stochast process approxim distribut admit compact represent maintain propag approach achiev exponenti save exact infer dbn show empir practic dbn approach result factor reduct run time small cost accuraci also prove accumul error aris repeat approxim remain bound indefinit time result reli analysi show transit stochast process contract rel entropi kl diverg appli approach paramet learn task applic complet straightforward sinc algorithm associ analysi appli forward propag messag wherea infer use learn algorithm requir propag inform entir sequenc paper provid analysi error accumul approxim infer process backward propag phase infer analysi quit differ contract analysi forward phase combin two result prove bound error expect suffici statist relay learn algorithm stage present empir result practic dbn illustr perform approxim learn algorithm show speedup obtain easili discern loss qualiti learn hypothesi theoret analysi also suggest way deal problemat need reason entir sequenc tempor observ contract result show legitim ignor observ far futur thu comput accur approxim backward messag consid small window observ futur idea lead effici onlin learn algorithm show converg good hypothesi much faster standard offiin em algorithm even set favor latter preliminari model dynam system specifi tupl repres qualit structur model appropri parameter dbn instantan state process specifi term set variabl xi set encod network fragment specifi time variabl parent parent exampl fragment shown figur paramet defin condit probabl tabl parent simplic assum variabl partit state variabl never observ observ variabl alway observ also assum observ variabl time depend state variabl time use denot transit matrix state variabl stochast process transit probabl boyen koller state si state sj note concept well defin even dbn although case matrix repres implicitli via paramet use denot observ matrix oi probabl observ respons rj state si goal learn model stochast process partial observ data simplifi discuss focu problem learn paramet known structur use em expect maxim algorithm discuss appli equal context em iter procedur search space paramet vector one local maximum likelihood functionth probabl observ data given describ em algorithm task learn hmm extens dbn straightforward em algorithm start initi often random paramet vector specifi current estim transit observ matric process em algorithm comput expect suffici statist ess use comput expect case hmm ess averag joint distribut variabl time variabl time new paramet vector comput ess simpl maximum likelihood step two step iter appropri stop condit met entir sequenc comput simpl forward backward algorithm let respons observ time let likelihood vector oi forward messag ot propag follow ot oc outer product backward messag propag estim belief time simpli ot suitabl renorm similarli joint belief proport messag pass algorithm obviou extens dbn unfortun feasibl small dbn essenti messag pass algorithm entri everi possibl state time dbn number state exponenti number state variabl render explicit represent infeas case furthermor even highli structur process admit compact represent messag belief state approxim describ new approach approxim infer dynam system avoid problem explicitli maintain distribut larg space maintain belief state distribut current state use comput tractabl represent distribut propag time approxim belief state transit model condit evid time approxim result time distribut use one admit compact represent allow algorithm continu also show error aris repeat approxim accumul unboundedli stochast process attenu effect particular dbn consid belief state approxim certain subset less correl variabl group distinct cluster approxim independ case approxim step consist simpl project onto relev margin use factor represent time approxim belief state algorithm implement effici use cliqu tree algorithm comput fit gener cliqu tree two time slice dbn ensur time time cluster appear subset cliqu incorpor fit time cliqu fit obtain approxim learn dynam model calibr tree infer read relev margin tree implicitli defin product result directli applic learn task belief state forward messag forward backward algorithm thu appli approach forward step guarante approxim lead big differ ess howev techniqu resolv comput problem backward propag phase expens forward phase appli idea backward propag maintain propag compactli repres approxim backward messag implement idea simpl extens algorithm forward messag comput simpli incorpor cliqu tree two time slice read relev margin comput howev extend analysi straightforward complet straightforward appli techniqu get rel error bound backward messag furthermor even bound rel entropi error forward backward messag bound error follow solut turn use altern notion distanc combin addit bayesian updat albeit cost weaker contract rate definit let two posit vector dimens project distanc defin proj maxi ln pi pi pi pi note project distanc weak upper bound rel entropi lemma proj proj proj base result show project distanc contract messag propag stochast transit matrix either direct cours rate contract depend ergod properti matrix lemma let min go ti ti ti ti defin dproj dproj jduroj proj show approxim introduc larg error expect suffici statist remain close correct valu theorem let ess comput via exact infer let approxim vard backward app roxim step guarante introduc project error dproj therefor dkl note even small fluctuat suffici statist caus em algorithm reach differ local maximum thu cannot analyt compar qualiti result algorithm howev experiment result show diverg exact em aproxim em practic test algorithm task learn paramet bat network shown figur use traffic monitor train set fix sequenc slice gener correct network distribut test metric averag log likelihood per slice fix test sequenc slice experi conduct use three differ random start point paramet boyen koller slice shoe evid refer dbn exacl em em lteratmn figur bat dbn structur approxim batch em experi ran em differ type structur approxim evalu qualiti model iter algorithm use four differ structur approxim exact propag ii cluster ten state variabl iii cluster iv variabl separ cluster result one random start point shown figur see impact even sever structur approxim learn accuraci neglig run approxim algorithm track exact one close largest differ peak log likelihood phenomenon rather remark especi view substanti save caus approxim sun ultra ii comput cost learn min iter exact case vs min iter cluster less min iter two onlin learn analysi also give us tool address anoth import problem learn dynam model need reason entir tempor sequenc one consequ contract result effect approxim done far away sequenc decay exponenti time differ particular effect approxim ignor observ far futur also limit therefor infer time slice base small window observ futur result still fairli accur precis assum time consid window size view uniform messag bad approxim propag approxim backward messag error decay exponenti base insight experi variou onlin algorithm use small window approxim onlin algorithm base approach ess updat exponenti decay everi data case paramet updat correspondingli main problem frequent paramet updat onlin set requir recomput messag comput use old paramet long sequenc comput cost scheme would prohibit algorithm simpli leav forward messag unchang assumpt recent time slice use paramet close new one contract result tell us use old paramet far back sequenc neglig effect messag tri sever scheme updat backward messag dynam approach use backward messag comput slice closer messag recomput frequent paramet chang base cach messag use older paramet approxim learn dynam model figur tempor approxim batch set onlin set closest messag updat everi paramet updat next everi updat etc approach closest realist altern full updat backward messag static approach use long window slice recomput messag window end use current paramet comput messag entir next window static approach use short window slice final static approach lookahead past present evid use comput joint belief latter case often use context kalman filter onlin learn process paramet minim comput burden test conduct use structur approxim run time variou algorithm sec slice batch em dynam static static static evalu tempor approxim onlin batch set batch experi use step sequenc use result shown figur see dynam algorithm reach qualiti model standard batch em converg sooner differ due frequent updat suffici statist base accur paramet interestingli see static algorithm use lookahead also reach accuraci thu approxim ignor evid far futur good one even weak notion far contrast see qualiti reach static approach significantli lower suffici statist use em algorithm case consist wors ignor futur evid thu network window size good full forward backward wherea one size clearli wors onlin learn experi shown figur use singl long sequenc slice see static approach almost indistinguish term accuraci dynam approach converg rapidli static algorithm thu frequent updat short window better infrequ updat longer one final see static algorithm converg hypothesi much lower qualiti thu even short window allow rapid converg best possibl answer window size conclus extens paper suggest use simpl structur approxim infer algorithm use step result suggest even sever structur approxim almost neglig effect accuraci learn advantag approxim infer learn set even pronounc infer task small error caus approxim neglig compar larger one boyen koller induc learn process techniqu provid new simpl approach learn structur model complex dynam system result advantag gener abil incorpor prior knowledg also present new algorithm onlin learn task show learn high qualiti model use small time window futur observ work compar variat approach approxim infer appli learn factori hmm done direct empir comparison seem like variat approach would work better dens connect model wherea approach would domin structur model one experi inde model algorithm track exact em close signific improv accuraci unlik algorithm also simpler easier implement importantli applic task onlin learn obviou extens result integr idea structur learn algorithm dbn believ result algorithm abl learn structur model real life complex system acknowledg thank tim huang provid us bat network nir friedman leonid gurvit use discuss research support aro muri program integr approach intellig system darpa contract daca subcontract iet inc refer artzrouni li note coeffici ergod column allow nonneg matrix linear algebra applic boyen koller tractabl infer complex stochast process proc uai page cover thoma element inform theori wiley dean kanazawa model reason persist causat comp int dempster laird rubin maximum likelihood incomplet data via em algorithm journal royal statist societi forb huang kanazawa russel batmobil toward bayesian autom taxi proc ijcai friedman murphi russel learn structur dynam probabilist network proc uai page ghahramani jordan factori hidden markov model nip kalman new approach linear filter predict problem basic engin lauritzen spiegelhalt local comput probabl graphic structur applic expert system roy stat soc neal hinton view em algorithm justifi increment spars variant jordan editor learn graphic model kluwer rabin juang introduct hidden markov model ieeeacoust speech signal process zweig russel speech recognit dynam bayesian network proc aaai page
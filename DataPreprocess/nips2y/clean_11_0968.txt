abstract simpl learn rule deriv vap algorithm instanti gener wide rang new reinforcementlearn algorithm algorithm solv number open problem defin sever new approach reinforc learn unifi differ approach reinforc learn singl theori algorithm guarante converg includ modif sever exist algorithm known fail converg simpl mdp includ qlearn sarsa advantag learn addit valu base algorithm also gener pure polici search reinforc learn algorithm learn optim polici without learn valu function addit allow policysearch valu base algorithm combin thu unifi two differ approach reinforc learn singl valu polici search vap algorithm algorithm converg pomdp without requir proper belief state simul result given sever area futur research discuss introduct mani reinforc learn algorithm known use parameter function approxim repres valu function adjust weight increment learn exampl includ learn sarsa advantag learn simpl mdp origin form algorithm fail converg summar tabl case algorithm guarante converg reason assumpt gradient descent gener reinforc learn tabl current converg result increment valu base rl algorithm residu algorithm chang everi first two column new algorithm propos taper chang evei fix fix usuallydistribut di str ution greedi polici distribut lookup tabl markov averag chain linear nonlinear lookup tabl mdp averag linear nonlinear lookup tabl pomdp averag nonlinear converg guarante counterexampl known either diverg oscil best worst possibl polici decay learn rate case known counterexampl either diverg oscil best worst possibl polici differ valu happen even infinit train time slowli decreas learn rate baird gordon first two column chang made converg use modifi form algorithm residu form baird possibl learn fix train distribut rare practic larg problem use explor polici usual greedi respect current valu function chang valu function chang case rightmost column chart current converg guarante good one way guarante converg three column modifi algorithm perform stochast gradient descent averag error function averag weight state visit frequenc current usual greedi polici weight chang polici chang might appear gradient difficult comput consid qlearn explor boltzman distribut usual greedi respect learn function seem difficult calcul gradient sinc chang singl weight chang mani valu chang singl valu chang mani action choic probabl state chang singl action choic probabl may affect frequenc everi state mdp visit although might seem difficult surprisingli unbias estim gradient visit distribut respect weight calcul quickli result algorithm put everi case tabl deriv vap equat consid sequenc transit observ follow particular stochast polici mdp let st xo uo ro xl ul rl xt ut rt xt ut rt sequenc state action reinforc time perform action state yield reinforc transit state baird moor stochast polici may function vector weight assum mdp singl start state name mdp termin state termin state let st set possibl sequenc time let st given error function calcul error time step squar bellman residu time error occur time function weight must smooth function weight consid period time start time end probabl endlst sequenc st occur probabl must expect squar period length finit let expect total error period expect weight accord state visit frequenc gener given polici period end time trajectori stag st ut rt st end note first line particular st error st ad everi sequenc start st term weight probabl complet trajectori start st sum probabl trajectori start st simpli probabl st observ sinc period assum end eventu probabl one second line equal first third line probabl sequenc lx factor might function probabl must smooth function weight nonzero everywher partial deriv respect particular element weight vector tgst ln space limit may clear short sketch deriv sum entir period give unbias estim expect total error period increment algorithm perform stochast gradient descent weight updat given left side tabl summat previou time step replac trace tt weight algorithm gener previous publish algorithm form function previou state action reinforc rather current reinforc allow vap valu polici search everi algorithm propos paper special case vap equat left side tabl note model need algorithm probabl need algorithm polici transit probabl mdp stochast gradient descent updat rule correct observ transit sampl trajectori found follow gradient descent gener reinforc learn tabl gener vap algorithm left sever instanti right singl algorithm includ valu base polici search approach combin ive guarante converg ever case aw ln te max ev current stochast polici smooth function given vector bound algorithm simpl actual gener larg class differ algorithm depend choic trace reset zero singl sequenc sampl follow current polici sum aw along sequenc give unbias estim true gradient finit varianc therefor learn weight updat made end trial weight stay within bound region learn rate approach zero converg probabl one ad weight decay term constant time norm weight vector onto prevent weight diverg small initi learn rate guarante global minimum found use gener function approxim least converg true backprop well instanti vap algorithm mani reinforc learn algorithm valu base tri learn valu function satisfi bellman equat exampl learn learn valu function actor critic algorithm learn valu function polici greedi respect td learn valu function base futur reward algorithm pure polici search algorithm directli learn polici return high reward includ reinforc william backprop time learn automata genet algorithm algorithm propos combin two approach perform valu polici search vap gener vap equat instanti choos express bellman residu yield valu base reinforc yield polici search linear combin two yield valu polici search singl vap updat rule left side tabl gener varieti differ type algorithm describ follow section reduc mean squar residu per trial mdp termin state trial time start termin state reach possibl minim expect total error per trial reset trace zero start trial converg form sarsa learn increment valu iter advantag learn gener choos squar bellman residu shown right side tabl case expect valu taken possibl xt ut rt baird moor triplet given st polici must smooth nonzero function weight could greedi polici choos greedi action probabl choos uniformli otherwis would caus discontinu gradient two valu state equal polici could someth approach greedi posit temperatur approach zero number possibl action state instanc tabl valu iter gradient estim use two independ unbias estim expect valu exampl esar esa qb estim true gradient qb residu algorithm describ baird retain guarante converg may learn quickli pure gradient descent valu qb note gradient time use prime variabl mean new state action time gener independ state action time cours mdp determinist prime variabl unprim mdp nondeterminist model known model must evalu one addit time get state model known three choic first model could learn past data evalu give independ sampl second issu could ignor simpli reus unprim variabl place prime variabl may affect qualiti learn function depend random mdp stop converg accept approxim practic third past transit could record prime variabl could found search time xt tlt seen randomli choos one transit use successor state action prime variabl equival learn certainti equival model sampl special case first choic extrem larg state action space mani start state like give result practic simpli reus unprim variabl prime variabl note weight effect polici algorithm reduc standard residu algorithm baird also possibl reduc mean squar residu per step rather per trial done make period length independ polici minim error per period also minim error per step exampl period might defin first step trace reset state return start state note everi state action pair posit chanc seen first step solv finit horizon problem actual solv discount infinit horizon problem reduc bellman residu everi state weight residu determin happen first step mani differ problem solv vap algorithm instanti definit period differ way polici search valu base learn also possibl add term tri maxim reinforc directli exampl could defin esa cy rather esa tabl gradient descent gener reinforc learn ooo oo beta figur pomdp number trial need learn vs combin polici search valu base rl outperform either alon trace reset zero termin state reach constant affect expect gradient affect nois distribut discuss william algorithm tri learn function satisfi bellman equat directli learn polici minim expect total discount reinforc result function may even close contain true valu satisfi bellman equat give good polici algorithm tri satisfi bellman equat give good greedi polici similar modif made algorithm tabl special case algorithm reduc reinforc algorithm william reinforc rederiv special case gaussian action distribut tresp hofman extens appear marbach case pure polici search particularli interest need kind model gener two independ successor algorithm propos find polici directli given gullap variou algorithm learn automata theori summar narendra thathachar vap algorithm propos appear first one unifi two approach reinforc learn find valu function approxim bellman equat solut directli optim greedi polici figur show simul result combin algorithm run said learn greedi polici optim consecut trial graph show averag plot run differ initi random weight learn rate optim separ valu leav state leav state enter end otherwis algorithm use modifi learn tabl explor equat state share paramet ordinari sarsa greedi learn could never converg shown gordon pure valu base new algorithm converg cours cannot learn optim polici start state sinc two valu learn equal pure polici search learn converg optim slowli sinc valu function cach result long sequenc state near end combin two approach new algorithm learn much quickli either alon interest vap algorithm describ last three section appli directli partial observ markov decis process pomdp true state hidden avail time step baird moor ambigu observ function true state normal algorithm sarsa guarante converg appli mdp vap algorithm converg case conclus new algorithm present special case give new algorithm similar learn sarsa advantag learn guarante converg wider rang problem previous possibl includ pomdp first time guarante converg even explor polici chang learn special case allow new approach reinforc learn tradeoff satisfi bellman equat improv greedi polici one mdp simul show combin algorithm learn quickli either approach alon unifi theori unifi first time valu base policysearch reinforc learn theoret interest also practic valu simul perform futur research unifi framework may abl empir analyt address old question better learn valu function better learn polici directli may also shed light new question best acknowledg research sponsor part air forc refer baird residu algorithm reinforc learn function approxim armand priediti stuart russel ed machin learn proceed twelfth intern confer juli morgan kaufman publish san francisco ca gordon stabl fit reinforc learn tesauro mozer hasselmo ed advanc neural inform process system pp mit press cambridg gullap reinforc learn applic control dissert coin technic report univers massachusett amherst kaelbl littman cassandra plan act partial observ stochast domain artifici intellig appear avail http www cs brown edu peopl lpk marbach simul base optim markov decis process thesi lid th massachusett institut technolog mccallum reinforc learn select percept hidden state dissert depart comput scienc univers rochest rochest ny narendra thathachar learn automata introduct prentic hall englewood cliff nj tresp hofman miss noisi data nonlinear time seri predict proceed neural network signal process girosi makhoul manolako wilson ed ieee signal process societi new york new york pp william toward theori reinforc learn connectionist system technic report nu cc northeastern univers boston
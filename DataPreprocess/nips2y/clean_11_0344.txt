abstract consid problem calcul learn curv averag gener perform gaussian process use regress simpl express gener error term eigenvalu decomposit covari function deriv use start point sever approxim scheme identifi becom exact compar exist bound learn curv new approxim use input space dimens gener get substanti closer truth introduct within neural network commun last year good deal excit use gaussian process altern feedforward network advantag gaussian process prior assumpt problem learn encod transpar way infer least case regress consid rel straightforward one crucial question applic fast gaussian process learn mani train exampl need achiev certain level gener perform typic oppos worst case behaviour captur learn curv give averag gener error function number train exampl sever worker deriv bound studi larg asymptot illustr howev exist bound often far tight asymptot result necessarili appli realist sampl size main aim paper therefor deriv approxim get closer true learn curv exist bound appli small larg simplest form regress problem consid tri learn function map input real valu vector realvalu scalar output given set train data consist present address depart mathemat king colleg london strand london wc ls email peter sollich kcl ac uk learn curv gaussian process input output pair xl yl train output yt may differ clean target output xt due corrupt nois given test input ask come predict correspond output express either simpl form mean predict plu error bar comprehens term predict distribut ix bayesian set specifi prior hypothesi function likelihood dio could gener train data deduc posterior distribut cr case feedforward network hypothesi function parameter set network weight predict distribut need extract integr posterior either comput intens mont carlo techniqu approxim lead analyt tractabl integr gaussian process hand obtain predict distribut trivial see one reason prior defin directli input output function done uniqu determin output valu input domain gaussian process simpli assum joint gaussian distribut henc name distribut specifi mean valu assum zero follow commonli done covari call covari function gaussian process encod easili interpret way prior assumpt function learn smooth exampl control behaviour ornsteinuhlenbeck ou covari function cr exp ix produc rough non differenti function function sampl squar exponenti se prior cr exp infinit differenti length scale paramet hand correspond directli distanc input space expect function vari significantli complex properti also encod replac differ length scale input compon exampl relev small irrelev larg input distinguish infer gaussian process work give brief summari refer exist review subject see detail simplest assum output gener clean valu hypothesi function ad gaussian nois independ varianc joint distribut set train output yt function valu also gaussian covari given ylym xl xm lm ylo xl defin matrix depend compon vector posterior distribut oid obtain simpli condit yl gaussian mean varianc ol tk olr eq solv infer problem gaussian process provid us directli predict distribut posterior varianc eq fact also give us expect gener error teacher squar deviat mean predict teacher output averag posterior distribut teacher give underli assumpt assum gaussian process one also one measur gener squar deviat predict noisi teacher output simpli add term eq ollich prior true one teacher actual gener use correct nois model otherwis complic express expect gener error result line work subject consid correct prior case follow averag gener error distribut input give tk form gener error well known still depend train input fact train output drop alreadi signatur fact gaussian process linear predictor compar averag data set yield quantiti averag expect gener error drop averag expect follow depend number train exampl function call learn curv exact calcul difficult joint averag eq train input xl test input learn curv start point approxim calcul first deriv represent gener error term eigenvalu decomposit covari function mercer theorem see tell us covari function decompos eigenvalu ki eigenfunct simpli analogu eigenvalu decomposit finit symmetr matrix eigenfunct taken normal write data depend gener error tr perform averag second term ajc xl xm ij suggest introduc diagon matrix ij kidij design matrix ti oi one also tr matrix express ident matrix collect result tra tr simplifi use woodburi formula matrix invers see appli case give line algebra one obtain final result trcr cr tr cr exact represent gener error one main result paper advantag averag test input alreadi carri remainin depend train data contain entir matrix also includ special case well known result linear regress see interpret suitabl gener version weight decay matrix input correl matrix start one deriv approxim express learn learn curv gaussian process curv naiv approach entir neglect fluctuat differ data set replac averag simpli ij ic xt qsj xt nsij lead naiv approxim en tr cr ni gener good howev becom exact larg nois limit oo constant fluctuat element matrix becom vanishingli small order cr replac averag justifi deriv better approxim use see matrix qa chang new exampl ad train set one cr bt bt term vector element second ident use woodburi formula get exact learn curv one would averag updat formula new train input previou one difficult progress made neglect fluctuat averag approxim replac pt averag simpli ident matrix averag previou train input replac averag yield approxim trg iter one see remain diagon trivial implement numer call result ed tr discret approxim learn curv still correctli treat variabl discret integ valu one approxim take continu vari replac differ left hand side deriv dg dn result differenti equat readili solv take trace one obtain gener error euc tr determin self consist equat tr ln comparison thought effect number train exampl subscript uc stand upper continu approxim name suggest anoth lower approxim also deriv treat continu form differ self consist equat deriv follow introduc auxiliari offset paramet whose use becom clear shortli vi end calcul set zero start also hold nonzero approxim tr averag retain possibl fluctuat numer give tr take trace yield updat formula genex error extra paramet let us rewrit averag right hand side tr tr oe ov treat continu thu arriv partial differenti equat solv use method characterist give lower continu approxim learn curv elc tr ry lc compar deriv easi show alway lower uc approxim one also check three approxim deriv lc uc converg exact result larg nois limit defin sollich comparison bound simul compar lc uc approxim exist bound true learn curv obtain simul lower bound gener error given michelli wahba deriv noiseless case allow gener observ project along first eigenfunct unlik tight case real observ discret input point base inform theoret method differ lower bound obtain opper elo tr ni na alway lower naiv approxim incorrectli suggest decreas zero cr fix clearli case compar also upper bound due opper euo cr tr ln cr na tr cr ni modifi version rescal version use becom ident limit small gener error cr never get larger cr small particular therefor actual much larger bound upper bound deriv william vivarelli one dimension input stationari covari function function alon consid gener error would obtain individu train exampl took minimum exampl train set averag lower envelop evalu explicitli term integr covari function result upper bound ewv never decay cr therefor complement rang applic uo bound exampl fig consid simpl input domain uniform input distribut also restrict stationari covari function fact use physicist call period boundari condit simpli trick make easi calcul requir eigenvalu spectra covari function otherwis littl effect result long length scale covari function smaller size input domain cover two extrem rough smooth gaussian prior consid ou exp se exp covari function prior varianc valu function learn simpli one gener expect prior ignor significantli larger nois train data consid valu cr also fix covari function length scale result qualit similar sever observ made figur mw lower bound tight expect bracket opper lower upper bound lo uo rather wide order magnitud give good represent overal shape learn curv asymptot regim clearli visibl se covari function drop cr wv upper bound avail work dimens exampl period continu stationari covari function written ooc term make signific contribut except within opposit end input space definit eigenvalu given fourier transform odx exp riqx integ learn curv gaussian process ou lc uc lj se ou lo se wv lc lo ou lo se lo figur learn curv comparison simul result thick solid line small fluctuat indic order magnitud error bar approxim deriv paper thin solid line discret uc lc upper lower continu exist upper dash uo upper opper wv william vivarelli lower dot dash lo lower opper mw michelli wahba bound type covari function ornstein uhlenbeck squar exponenti length scale dimens input space nois level shown note logarithm axe scale plot uc coincid except simul result essenti top lc curv sollich well ou covari function less se case expect use asymptot regim alway remain rr discret upper continu uc approxim similar fact indistinguish scale plot make uc version prefer practic evalu chosen without step smaller valu exampl true learn curv lie uc lc curv fact would conjectur two approxim provid upper lower bound learn curv least stationari covari function final lc approxim come clear winner rr fig lc indistinguish true learn curv even case repres overal shape learn curv well small asymptot regim largest deviat occur crossov region two regim summari deriv exact represent averag gener error gaussian process use regress term eigenvalu decomposit covari function start obtain three differ approxim learn curv becom exact larg nois limit practic one gener expect opposit case rr comparison simul result show even regim new approxim perform well lc approxim particular repres overal shape learn curv well rough ou smooth se gaussian prior small well larg number train exampl perfect get substanti closer true learn curv exist bound futur work show well new approxim work non stationari covari function non uniform input distribut whether treatment fluctuat gener error due random select train set improv analog fluctuat correct linear perceptron learn acknowledg would like thank chri william manfr opper stimul discuss provid copi paper prior public grate royal societi financi support dorothi hodgkin research fellowship refer see mackay gaussian process tutori nip recent paper goldberg william bishop nip william barber william nip william rasmussen nip michelli wahba design problem optim surfac interpol ziegler editor approxim theori applic page academ press mopper regress gaussian process averag case perform kwok yee wong yeung editor theoret aspect neural comput multidisciplinari perspect springer william vivarelli upper bound learn curv gaussian process submit public william predict gaussian process linear regress linear predict beyond jordan editor learn infer graphic model kluwer academ press wong stochast process inform dynam system mcgraw hill new york press teukolski vetterl flanneri numer recip nd ed cambridg univers press cambridg sollich finit size effect learn gener linear perceptton journal physic
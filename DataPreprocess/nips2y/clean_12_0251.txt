abstract present three simpl approxim calcul posterior mean gaussian process classif first two method relat mean field idea known statist physic third approach base bayesian onlin approach motiv recent result statist mechan neural network present simul result show mean field bayesian evid may use hyperparamet tune onlin approach may achiev low train error fast introduct gaussian process provid promis non parametr bayesian approach regress classif statist model assum likelihood output target variabl given input written yla function gaussian prior distribut priori assum gaussian random field mean finit set field variabl xi jointli gaussian distribut given covari xi xj xi xj also assum zero mean throughout paper predict novel input set train exampl xi yi given comput posterior distribut variabl xl major technic problem gaussian process model difficulti comput posterior averag high dimension integr likelihood gaussian happen exampl classif problem far varieti approxim techniqu discuss mont carlo sampl map approach bound likelihood tap mean field approach paper introduc three differ novel method approxim posterior mean random field think simpl enough use practic applic two techniqu csat fokoud opper schottki lt nther base mean field idea statist mechan contrast previous develop tap approach easier implement also yield simpl approxim total likelihood data evid use tune hyperparamet covari kernel bayesian evid mlii framework aim maxim likelihood data special case binari classif problem simplic class label assum nois free likelihood chosen ya unit step function equal zero els interest comput effici approxim posterior mean use predict label via sign denot posterior expect posterior distribut symmetr around mean give bay optim predict start let us add two comment likelihood first map approach predict field maxim posterior would applic give trivial result second nois easili introduc within probit model subsequ calcul slightli alter moreov gaussian averag involv definit probit likelihood alway shift likelihood gaussian process prior redefinit field chang predict leav us simpl likelihood modifi process covari exact result first glanc may seem order calcul deal joint posterior field ai xi togeth field test point would impli test point differ new dimension averag perform actual show case let denot expect gaussian prior posterior expect point say ij yjlay integr part likelihood written np yjlaj xj jyj yj oaj show depend test point therefor necessari comput dimension averag everi predict chosen specif definit order stress similar predict support vector machin likelihood aj come nonneg next section develop three approach approxim comput aj mean field method ensembl learn first goal approxim true posterior distribut ald vr detk effici approach gaussian process classif simpler tractabl distribut denot covari matrix element kij xi xj variat mean field approach known ensembl learn neural comput commun rel entropi distanc kl da minim famili product distribut hj qj aj contrast variat bound likelihood comput get kl daiqi ai ln qi ai yilai ai denot expect set function deriv kl respect qi equal zero find best product distribut gaussian prior time origin likelihood qi cr yila rni ai jv ij aj ai use specif form approxim posterior replac averag true posterior approxim get use likelihood set rn nonlinear equat unknown np yjlaj oand rnj kjiyi ti jyjotj zoodt use byproduct variat approxim upper bound bayesian evid da dla deriv denot gaussian process prior dla ij yjlaj bound written term mean field free energi lnp sqlnq eqln vr dla eln rnj der hi use yardstick select appropri hyperparamet covari kernel ensembl learn approach littl drawback requir invers covari matrix free energi one must comput determin second simpler approxim avoid comput mean field theori ii naiv approach second mean field theori aim work directli variabl otj start point consid partit function evid dze yjlzj csat fokoud opper schottki winther follow standard gaussian integr introduc fourier eaeiazp imaginari transform likelihood ylz unit tempt view normal partit function gaussian process zi covari matrix likelihood unfortun real number preclud proper probabilist interpret nevertheless deal formal complex measur defin integr part show one yjaj zj bracket denot averag complex measur suggest simpl approxim calcul aj one may think tri saddl point steepest descent approxim replac zj valu zj complex plane make integrand stationari therebi neglect fluctuat zj henc approxim would treat expect product zizj zi zj may reason definit self correl accord gener formal mean field theori outlin one separ improv idea treat self interact done replac zi except form new variabl insert dirac function represent integr variabl exactli integr factor final perform saddl point integr variabl detail calcul given elsewher within saddl point approxim get system nonlinear equat mj kji kjiyioq iyj yj form aj replac simpler kjj equat also deriv us use callen ident present deriv allow also approxim evid plug saddlepoint valu back partit function get lnp ln yi yic ki sijkii yjc also simpler comput give bound true evid sequenti approach previou algorithm give explicit express posterior mean requir solut set nonlinear equat must obtain iter procedur present differ approach approxim comput posterior mean base singl sequenti sweep whole dataset give explicit updat posterior algorithm base recent propos bayesian approach onlin learn see articl opper winther solla basic idea appli gaussian process scenario follow suppos qt gaussian approxim posterior seen exampl mean approxim posterior process gaussian process mean covari kt start new data point yt observ posterior updat accord bay rule new non gaussian posterior project back famili gaussian choos closest gaussian qt minim rel entropi kl qt effici approach gaussian process classifican order keep loss inform small project equival match first two moment qt first moment get yt lla xt kt xt second line follow integr part zt kt xt xt recurs correspond one kt solv ansatz vector matrix also nonzero element updat kt kt vector element kt denot element wise product vector sequenti algorithm defin advantag requir matrix invers also need solv numer optim problem time approach differ updat gaussian posterior approxim propos sinc requir linear likelihood method equival extend kalman filter approach sinc possibl comput evid new datapoint yt base old posterior comput approxim log evid data via simul present two set simul mean field approach first test bayesian evid framework tune hyperparamet covari function kernel second test abil sequenti approach achiev low train error stabl test error fix hyperparamet evid framework give simul result mean field free energi singl data set pima indian diabet train test exampl input dimension result therefor taken conclus evid merit approach simpli indic may give reason result use eldwl radial basi function covari function exp diagon term ad covari matrix correspond gaussian nois ad field varianc free energi lnp minim gradient descent respect lengthscal paramet wx wd mean field equat solv iter updat hyperparamet detail given elsewher figur show evolut naiv mean free energi test error start uniform csat fokou opper schottki winther ws typic requir order iter step equat hyperparamet updat also use hybrid approach free energi minim one mean field algorithm hyperparamet use may seen tabl naiv mean field theori overestim free energi sinc ensembl free energi upper bound free energi overestim nearli sever minimum naiv mean field free energi anoth interest observ long hyperparamet use actual perform measur test error sensit algorithm use also seem case tap mean field approach support vector machin iter iter figur hyperparamet optim pima indian data set use naiv mean field free energi left figur free energi function number hyperparamet updat right figur test error count function number hyperparamet updat tabl pima indian dataset hyperparamet found free energi minim left column give free energi use hyperparamet optim test error count rang previous report ensembl mf naiv mf free energi minim error error ensembl mean field eq naiv mean field eq sequenti algorithm studi sonar crab dataset sinc comput approxim evid far simpl fix polynomi kernel use although probabilist justif algorithm valid singl sweep data use independ data assum tempt reus data iter procedur heurist two plot show way small improv obtain seem method rather effici extract inform data singl present sonar dataset singl sweep enough achiev zero train error acknowledg bs would like thank leverhulm trust support work also support epsrc grant gr effici approach gaussian process classif train error test error iter iter figur train test error learn sonar left crab dataset right vertic dash dot line mark end train set start point reus kernel function use order dimens input refer william rasmussen gaussian process regress neural inform process system touretzki mozer hasselmo ed mit press neal mont carlo implement gaussian process model bayesian regress classif technic report depart statist univers toronto gibb mackay variat gaussian process classifi preprint cambridg univers william barber bayesian classif gaussian process ieee tran pattern analysi machin intellig opper winther gaussian process classif mean field algorithm submit neural comput http www thep lu se tf staff winther zinn justin quantum field theori critic phenomena clarendon press oxford ripley pattern recognit neural network cambridg univers press opper onlin versu offiin learn random exampl gener result phi rev lett onlin learn neural network cambridg univers press saad ed gorman sejnowski analysi hidden unit layer network train classifi sonar target neural network jaakkola haussler probabilist kernel regress onlin proceed th int workshop ai statist http uncertainti microsoft com proceed htm
abstract studi approxim function two layer feedforward neural network focus increment algorithm greedili add unit estim singl unit paramet stage oppos standard algorithm fix architectur optim stage perform small number paramet mitig mani difficult numer problem inher high dimension non linear optim establish upper bound error incur algorithm approxim function sobolev class therebi extend previou result provid rate converg function certain convex hull function space compar result recent deriv lower bound show greedi algorithm nearli optim combin estim error result greedi algorithm strong case made type approach introduct major problem applic neural network real world problem excess long time requir train larg network fix architectur moreov theoret result establish intract train worst case addit problem determin architectur size network requir solv certain task left open due problem sever author consid increment algorithm construct network addit hidden unit estim unit paramet increment approach possess two desir attribut first optim done step wise small number paramet need optim stage second structur network work support part grant israel scienc foundat author partial support center absorpt scienc ministri immigr absorpt state israel meir maiorov establish concomitantli learn rather specifi advanc howev recent algorithm rather heurist natur guarante perform bound establish note recent surg interest type algorithm fact date back work done earli seventi see histor survey first theoret result establish perform bound increment approxim hilbert space given jone work later extend barron appli neural network approxim function character certain condit fourier coeffici work barron extend two main direct first lee et al consid approxim gener function use hilbert space techniqu donahu et al provid power extens jone barron result gener banach space one impress result latter work demonstr iter algorithm mani case achiev nearli optim rate converg approxim convex hull paper concern mainli issu approxim comment highli relev statist problem learn data neural network first lee et al give estim error bound algorithm perform increment optim respect train error certain regular condit abl achiev rate converg compar obtain much comput demand algorithm empir error minim moreov well known upper bound approxim error need order obtain perform bound parametr nonparametr estim latter achiev use method complex regular final point donahu et al lower bound approxim error crucial establish worst case speed limit learn main contribut paper follow function belong sobolev class see definit establish appropri condit near optim rate converg increment approach obtain explicit bound paramet valu network latter bound often crucial establish estim error rate contrast work character approxim rate function belong standard smooth class sobolev class former work establish rate converg respect convex hull certain subset function relat simpl way standard function class lipschitz sobolev der etc far awar result report first report bound increment neural network procedur detail version work complet detail proof avail problem statement make use nomenclatur definit let banach space function norm ii ii concret assum henceforth norm given lq norm oo denot iiq let lin consist sum form aig gi arbitrari ai co set sum ai ai distanc measur lq norm function given dist lin inf lib fllq lin dist co inf lib fllq linear span given lin lin convex hull co co follow standard notat denot closur set bar co closur convex hull work focu special case optim increment neural network algorithm correspond basic build block multilay neural network restrict ila demand mani sigmoid function express sum function bound norm obviou lin correspond two layer neural network linear output unit activ function singl hidden layer co equival restrict form network restrict place hidden output weight term definit introduc well known properti univers function approxim campacta state linn class continu real valu function defin compact subset necessari suffici condit establish leshno et al essenti requir local integr non polynomi comment oo unrestrict sign conoo linnoo distinct becom import oo case con linn purpos increment approxim turn use consid convex hull con rather usual linear span power algorithm perform bound develop case context sever author consid bound approxim function belong con sequenc function belong co howev clear gener well convex hull bound function approxim gener function one contribut work show one may control rate growth bound gener function belong certain smooth class sobolev may well approxim fact show increment approxim scheme describ achiev nearli optim approxim error function sobolev space follow danahu et al consid greedi algorithm let posit sequenc similarli sequenc function greedi respect ii fllq inf fllq set simplic set although scheme also possibl clear stage function belong co observ also step infimum taken respect function fix term neural network impli optim hidden unit paramet perform independ other note pass greatli facilit optim process practic theoret guarante made convex singl node error function see counter exampl variabl slack variabl allow extra freedom approxim minim paper optim rather fix sequenc advanc forfeit gener price simpler present event rate obtain unchang restrict sequel consid greedi approxim smooth function belong sobolev class function max cl cd cl ka ier partial deriv oper order function defin compact domain upper bound fo norm first consid approxim function use norm distinct lq norm exist inner product case defin meir maiorov simplif essenti proof case begin recal result demonstr function may exactli express convex integr represent form depend probabl densiti function pdf respect multi dimension variabl thu may write qe ew denot expect oper respect pdf moreov shown use radon wavelet transform function taken ridg function ca arx case neural network type convex represent first exploit barron assum belong class function character certain moment condit fourier transform later delyon et al maiorov meir extend barron result case wavelet neural network respect obtain rate converg function sobolev class basic idea point gener approxim hn base draw random variabl result random function hn throughout paper conform standard notat denot random variabl uppercas letter realiz lower case letter let iin wi repres product pdf first result demonstr averag procedur lead good approxim function belong theorem let compact set exist constant cn max implic upper bound expect valu exist set valu rate achiev moreov long function oi bound norm bound impli bound size function hn proof sketch proof proce express sum two function fl function fl best approxim class multi variat spline degre know exist paramet ilfx hn cn moreov use result shown cn use two observ togeth triangl inequ hnl fl hnl yield desir result next show given approxim rate attain theorem rate may obtain use greedi algorithm moreov sinc establish optim upper bound logarithm factor conclud greedi approxim inde yield near optim perform time much attract comput fact section use weaker algorithm perform full minim stage optim increment neural network algorithm increment algorithm let let chosen satisfi ill qh ewl ill qh ox assum gener select obey anq ew defin ewr measur error incur th stage increment procedur main result section follow theorem error increment algorithm bound cn finit constant proof sketch claim establish upon show name error incur increment procedur ident nonincrement one describ preced theorem result follow upon use der inequ upper bound remain detail straightforward tediou found full paper upper bound gener lq norm establish rate converg increment approxim norm move gener lq norm first note proof theorem reli heavili exist inner product use tool longer avail case gener banach space lq order extend result latter norm need use advanc idea theori geometri banach space particular make use recent result work donahu et al second must keep mind approxim sobolev space use lq norm make sens embed condit rid hold sinc otherwis lq norm may infinit embed condit guarante finit see detail first present main result section follow sketch proof full detail rather technic proof found note case need use greedi algorithm rather algorithm section meir maiorov theorem let embed condit rid hold assum ii qd obtain via increment greedi algorithm sn proof sketch main idea proof theorem two part approxim scheme first base show may well approxim function convex class co appropri valu see lemma defin argu make use result particular use corollari increment greedi algorithm use approxim closur class co class co proof complet use triangl inequ proof along line done case case simpl use der inequ form ilfllq ikix fl ki volum region yield desir result given lower bound nearli optim discuss present theoret analysi increasingli popular approach increment learn neural network extend previou result shown near optim rate converg may obtain approxim function sobolev class result extend clarifi previou work deal sole approxim function belong closur convex hull certain set function moreov given explicit bound paramet use algorithm shown restrict co stringent case rate obtain good logarithm factor rate obtain gener spline function known optim approxim sobolev space rate obtain case sub optim compar spline function shown provabl better linear approach event shown rate obtain equal logarithm factor approxim lin size chosen appropri impli posit input output weight suffic approxim open problem remain point demonstr whether increment algorithm neural network construct shown optim everi valu fact even known stage neural network approxim gener refer auer herbster warmuth exponenti mani local minima singl neuron touretzki mozer hasselmo editor advanc neural inform process system page mit press barron univers approxim bound superposit sigmoid function ieee tran inf th barron barron statist learn network unifi view wegman editor comput scienc statist proceed th symposium interfac page washington amer stati assoc optim increment neural network algorithm blum rivest train node neural net np complet touretzki editor advanc neural format process system page morgan kaufmann de boor fix spline approxim quasi interpol approx theor delyon juditski benvenist accuraci nalysi wavelet approxim ieee transact neural network donahu gurvit darken sontag rate convex approxim non hilbert space construct approx jone simpl lemma greedi approxim hilbert space converg rate project pursuit regress neural network train ann stati judd neural netxvork design complexit learn mit press boston usa lee bartlett williamson effici agnost learn neural network bound fan ieee tran inf theor leshno lin pinku schocken multilay feedforward network nonpolynomi activ function approxim function neural netxvork maiorov meir near optim stochast approxim smooth function neural network technic report cc technion depart electr engin novemb submit advanc comput mathemat meir maiorov optim neural network approxim use increment algorithm submit public octob ftp dumbo technion ac il pub paper increment pdf triebel theori function space birkhaus basel
abstract present mont carlo gener em equat learn nonlinear state space model difficulti lie mont carlo step consist sampl posterior distribut hidden variabl given observ new idea present paper gener sampl gaussian approxim true posterior easi obtain independ sampl paramet gaussian approxim either deriv extend kalman filter fisher score algorithm case posterior densiti multimod propos approxim posterior sum gaussian mixtur mode approach show sampl approxim posterior densiti obtain algorithm lead better model use point estim hidden state experi fisher score algorithm obtain better approxim posterior mode ekf multimod distribut mixtur mode approach gave superior result introduct nonlinear state space model nssm gener framework repres nonlinear time seri particular narmax model nonlinear auto regress move averag model extern input translat equival nssm mathemat nssm describ system equat rt fw rt ut et xt denot hidden state variabl et denot zero mean uncorrel gaussian nois covari qt ut exogen determinist input vector time seri measur yt relat unobserv hidden state xt observ equat yt gv xt ut vt vt uncorrel gaussian nois covari vt follow assum nonlinear map fw gv neural network weight vector respect initi state assum gaussian distribut mean ao covari qo variabl gener multidimension two challeng briegel tresp nssm interrel task infer learn infer tri estim state unknown variabl rs given measur typic state past present futur valu ofxt learn want adapt unknown paramet model neural network weight vector given set measurementsfi special case linear state space model gaussian nois effici algorithm infer maximum likelihood learn exist latter implement use em updat equat step implement use forward backward kalman filter shumway stoffer system nonlinear howev problem infer learn lead complex integr usual consid intract anderson moor use approxim present section show learn equat nssm implement use two step repeat converg first mont carlo step random sampl gener unknown variabl hidden variabl xt given measur second step gener step sampl treat real data use adapt fw use version backpropag algorithm problem lie first step sinc difficult gener independ sampl gener multidimension distribut sinc difficult gener sampl proper distribut next best thing might gener sampl use approxim proper distribut idea pursu paper first thing might come mind approxim posterior distribut hidden variabl multidimension gaussian distribut sinc gener sampl distribut simpl first approach use extend kalman filter smoother obtain mode covari gaussian altern estim mode covari posterior distribut use effici implement fisher score deriv fahrmeir kaufmann use paramet gaussian case approxim posterior mode singl gaussian might consid crude therefor third solut approxim posterior distribut sum gaussian mixtur mode approach mode covari gaussian obtain use fisher score algorithm weight gaussian deriv likelihood observ data given individu gaussian follow section deriv gradient log likelihood respect weight fw section show network weight updat use mont carlo step gener step furthermor deriv differ gaussian approxim posterior distribut introduc mixtur mode approach section valid algorithm use standard nonlinear stochast time seri model section present conclus gradient nonlinear state space model given assumpt write joint probabl complet data xr ur paper focu case smooth offlin learn respect independ work singl gaussian approxim step use ekf propos ghahramani rowei special case rbf network show one obtain close form step adapt linear paramet hold nonlinear paramet fix although avoid sampl comput load step seem signific follow probabl densiti condit current model notat conveni indic fact explicitli fisher score mixtur mode infer learn nssm ut ui set known input mean ut irrelev follow sinc yt yl yt ut observ log likelihood model log gradient respect log ow log ov log xy insert gaussian nois assumpt obtain log likelihood respect neural network weight vector tresp hofmann ff ofw xt xt fw xt xt xt llvt ct dxt ld og xt ut yt xt ut xt yt ut dxt ov approxim step mont carlo gener em learn integr previou equat solv use mont carlo integr lead follow learn algorithm gener sampl xt iyt ut assum current model correct mont carlo step ogl new treat sampl real data updat new ld ow alog stepsiz ld ov ogl ofw xt ltt ologl ov gener step go back step one fw ut yt ut second step simpli stochast gradient step comput difficulti lie first step method produc sampl multivari distribut gibb sampl markov chain mont carlo method least two problem first sampl process forget initi condit mean first sampl discard simpl analyt tool avail determin mani sampl must discard secondli subsequ sampl highli correl mean mani sampl gener suffici amount independ sampl avail sinc difficult sampl correct posterior distribut xt iyt ut idea paper gener sampl approxim distribut easi draw sampl next section present approxim use multivari gaussian mixtur gaussian approxim mode estim use extend kalman filter wherea kalman filter optim state estim linear state space model extend kalman filter suboptim state estim nssm base local linear nonlinear extend kalmanfilt smoother ekf algorithm note includ paramet nssm addit state estim done author puskoriu feldkamp briegel tresp forward backward algorithm deriv approxim posterior mode estim gaussian error sequenc sage melsa applic framework amount approxim ae smooth estim ct obtain forward backward extend kalman filter set measur yt ae mode posterior distribut tlyr ur use center approxim gaussian ekf also provid estim error covari state vector time step use form covari matrix approxim gaussian ekf equat found anderson moor gener sampl recurs appli follow algorithm given ampl gau ssi approxim ut time draw sampl ctl yt ut last condit densiti gaussian mean covari calcul ekf approxim lag one error covari deriv shumway stoffer respect exact mode estim use fisher score algorithm system highli nonlinear howev ekf perform badli find posterior mode due fact use first order taylor seri expans nonlinear fw illustr see figur usc comput tractabl altern ekf comput exact posterior mode maxim ogp xt yt ut respect xt suitabl way determin stationari point log posterior equival xt yrlut deriv drop ut fs old appli fisher score current estim get better estim xf new xtf old jr unknown state sequenc xt solut fs old fs old sta score function ologp xt ytiur expect inform matrix oxt xt xr yrlut oxrox extend argument given fahrmeir kaufmann nonlinear state space model turn solv equat comput invers expect inform matrix perform choleski decomposit one forward backward passfi forward backward step implement fast ekf like algorithm iter obtain maximum posterior estim ae see appendix figur show estim obtain fisher score procedur bimod posterior densiti fisher score success find exact mode ekf algorithm sampl approxim gaussian gener way last section mixtur mode approach previou two approach posterior mode smooth view singl gaussian approxim mode xtiyt ut case approxim posterior densiti singl gaussian might consid crude particular posterior distribut multimod section approxim posterior weight sum gaussian xt yt ut km xti xtlk th gaussian individu gaussian model differ mode abl model multimod posterior distribut accur approxim individu mode local maxima fisher score algorithm und start algorithm use differ initi condit given differ gaussian optim weight factor yr yt yt fp yrlxr xrll dxr note differ etween fisher score gauss newton updat former take expect inform matrix oth expect inform matrix posit definit block tridiagon matrix fisher score mixtur mode infer learn nssm likelihood data given mode approxim integr insert fisher score solut time step linear nonlinear fisher score solut obtain close form solut comput see appendix result estim weight sum singl fisher score estim ekm ok tf mixtur mode algorithm found appendix learn task sampl mixtur gaussian base sampl singl gaussian obtain way subsect experiment result first experi want test well differ approach approxim posterior distribut nonlinear time seri infer time seri model chose xt co xt xt xt ttt xt xt covari qt vt initi condit ao qo consid hard infer problem kitagawa time step calcul expect valu hidden variabl xt base set measur oo yx optim estim mean squar sens base differ approxim present last section note singl mode approxim de best estim xt base approxim gaussian mixtur mode approach best estim et vs vs mode th gaussian dimens xt figur left show mean squar error mse smooth estim use differ approach fisher score fs algorithm significantli better ekf approach experi mixtur mode mm approach significantli better ekf fisher score reason posterior probabl multimod shown figur second experi use time seri model train neural network approxim fw covari assum fix known adapt use learn rule section use variou approxim posterior distribut xt figur right show result experi show truli sampl approxim gaussian give significantli better result use expect valu point estim furthermor use mixtur mode approach conjunct sampl gave significantli better result approxim use singl gaussian use infer network train use mixtur mode approach significantli wors true model signific level base experi conclus paper present novel approach infer learn nssm applic fisher score mixtur mode approach nonlinear model present paper new also idea sampl approxim posterior distribut hidden variabl present first time result indic fisher score algorithm give better estim expect valu hidden variabl ekf base approxim note fisher score algorithm complex requir typic forward backward pass instead one forward backward pass ekf approach experi also show posterior distribut multimod mixtur mode approach give significantli better estim compar approach base singl gaussian approxim learn experi show import sampl approxim distribut suffici simpli substitut point estim base briegel tresp figur approxim posterior distributionp xt iy continu line show posterior distribut base gibb sampl use sampl consid close approxim true posterior ekf approxim dot converg mode fisher score solut dashdot find largest mode mixtur mode approach mode dash correctli find two mode sampl approach also possibl estim hyperparamet covari matric done paper approach also extend toward onlin learn estim variou way miss data problem appendix mixtur mode algorithm mixtur mode estim deriv weight sum individu fisher score mode estim obtain fisherscor algorithm subsect first one perform set forward recurs singl mode estim fs fs tlt llt qt lo fs kxz lrt xt fs fs fs kx initi qo en one peffo set backward smooth recurs llt lt lb initi individu mode estim obtain iter applicayf fs yf fs fs tion updat mle stepsiz whe converg obtain mixtur mode estim weight sum fs weight coeffici comput recurs start unifo prior wlp stand gaussian center covafi evalu sj fs kx fs fisher score mixtur mode infer learn nssm uj figur left infer height bar indic mean squar error true et know sinc simul system estim use variou approxim error bar show standard deviat deriv repetit experi base pair test fisher score significantli better ekf mixtur mode approach significantli better ekf fisher score base reject region mixtur mode approxim mode mm significantli better approxim use mode improv approxim use mode mm significantli better approxim mm mode use reject region right learn height bar indic mean squar error true known approxim use multi layer perceptron hidden unit shown result use ekf approxim left fisher score approxim center mixtur mode approxim right two bar experi left bar show result expect valu et calcul use approxim gaussian use singl sampl gener step word use point estim et use point estim result three approxim significantli differ base signific level right bar show result sampl gener approxim gradient use gaussian approxim result use sampl significantli better result use point estim signific level sampl approach use mixtur mode approxim significantli better two sampl base approach signific level compar infer result experi shown left achiev mean squar error mixtur mode approach mode significantli wors result true model signific level refer anderson moor optim filter prentic hall new jersey fahrmeir kaufmann kalman filter posterior mode estim fisher score dynam exponenti famili regress metrika pp ghahramani rowei learn nonlinear stochast dynam use gener em algorithm advanc neural inform process system et ls keam solla cohn mit press cambridg kitagawa non gaussian state space model nonstationari time seri comment jasa pp puskoriu feldkamp neurocontrol nonlinear dynam system kalman filter train recurr network ieee transact neural network pp sage melsa estim theori applic commun control mcgraw hill new york shumway stoffer time seri smooth forecast use em algorithm technic report divis statist uc davi tresp hofmann miss noisi data nonlinear time seri predict neural network signal process ieee sig pmc soc pp
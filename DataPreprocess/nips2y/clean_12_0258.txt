abstract recent interpret adaboost algorithm view perform gradient descent potenti function simpli chang potenti function allow one creat new algorithm relat adaboost howev new algorithm gener known formal boost properti paper exmin question potenti function lead new algorithm booster two main result gener set condit potenti one set impli result algorithm booster impli algorithm condit appli previous studi potenti function use logitboost doom ii introduct first boost algorithm appear rob schapir thesi algorithm abl boost perform weak pac learner result algorithm satisfi strong pac learn criteria call method build strong pac learn algorithm weak pac learn algorithm pac boost algorithm freund schapir later found improv pac boost algorithm call adaboost also tend improv hypothes gener practic learn algorithm adaboost algorithm take label train set produc master hypothesi repeatedli call given learn method given learn method use differ distribut train set produc differ base hypothes master hypothesi return adaboost weight vote base hypothes adaboost work iter determin exampl poorli classifi current weight vote select distribut train set emphas exampl recent sever research notic adaboost perform constrain gradient descent exponenti potenti function margin exampl margin exampl yf valu label exampl net weight vote master hypothesi adaboost seen way clear algorithm may deriv chang potenti function potenti booster exponenti potenti use adaboost properti influenc data point increas exponenti repeatedli misclassifi base hypothes concentr hard exampl allow ariaboost rapidli obtain consist hypothesi assum base hypothes certain properti howev also mean incorrectli label noisi exampl quickli attract much distribut appear lack noisetoler one adaboost drawback tt sever research propos potenti function concentr much hard exampl howev gener show deriv algorithm pac boost properti paper return origin motiv behind boost algorithm ask potenti function gradient descent lead pac boost algorithm booster creat strong pac learn alg orithm arbitrari weak pac learner give necessari condit met propos potenti function notabl logitboost potenti introduc friedman et al furthermor show simpl gradient descent propos potenti function sigmoid potenti use mason et al cannot convert arbitrari weak pac learn algorithm strong pac learner aim work identifi properti potenti function requir pac boost order guid search effect potenti potenti function addit tunabl paramet chang time result yet appli dynam potenti pac boost defin notion pac learn boost defin notat use throughout paper concept subset learn domain random exampl pair drawn distribut otherwis concept class set concept definit strong pac learner concept class properti everi distribut concept probabl least algorithm output hypothesi pd learn algorithm given abil draw random exampl distribut must run time bound poli definit weak pac learner similar strong pa learner except need satisfi condit particular co pair rather pair definit pac boost algorithm gener algorithm leverag weak pa learner meet strong pa learn criteria remaind paper emphas boost accuraci much easier boost confid see haussler et al freund detail furthermor emphas boost sampl strong pac learner draw larg sampl iter weak learn algorithm call distribut sampl xto simplifi present omit instanc space dimens taxget represent length paramet duj helmbold throughout paper use follow notat cardin fix sampl xl yl xm ht valu weak hypothesi creat iter weight vote ht master hypothesi may may normal tt oft ft ott ht master hypothesi iter ui yi tt ott ht margin xi iter subscript often omit note margin posit master zt hypothesi correct normal margin ui potenti instanc margin total potenti zim ui es probabl respect unknown distribut domain probabl aud expect respect uniform distribut sampl respect result appli total potenti function form posit strictli decreas leverag learner gradient descent adaboost recent interpret gradient descent independ sever group interpret adaboost seen minim total potenti ui im exp ui via feasibl direct gradient descent iter adaboost choos direct steepest descent distribut sampl call weak learner obtain new base hypothesi ht weight new weak hypothesi calcul minim result potenti im ui im exp ui ott lyiht xi gradient descent idea gener potenti function duffi et al prove bound similar gradient descent techniqu use non componentwis non monoton potenti function note weak learner return good hypothesi ht train error dt xi yiht xi set assum base hypothesi produc satisfi zim dt xi yiht xi paper consid gener gradient descent approach appli variou potenti ui note potenti function two correspond gradient descent algorithm see un normal algorithm like adaboost continu add new weak hypothes preserv old normal algorithm scale alway sum gener call algorithm leverag algorithm reserv term boost actual pac boost properti potenti boost section describ suffici condit potenti function correspond leverag algorithm pac boost properti predict master hypothesi instanc sign ft sour current proof requir actual greater constant say therefor minim may need reduc potenti booster appli condit show two potenti literatur lead boost algorithm theorem let potenti function deriv increas decreas neither normal un normal leverag algorithm spond potenti pa boost properti co rre theorem proven adversari argument whenev concept class suffici rich adversari keep constant fraction sampl correctli label master hypothesi thu error toler goe zero master hypothes suffici accur appli theorem two potenti function literatur friedman et al describ potenti call squar error potentialatxii yi er ef potenti written asp ui eu corollari potenti squar error lead boost algorithm proof potenti satisfi condit theorem strictli decreas second condit hold mason et al examin normal algorithm use potenti pa tanh au algorithm optim choic via cross valid use weak learner slightli differ properti howev plug potenti directli gradient descent framework examin result algorithm corollari doomii potenti pd lead boost algorithm fix proof potenti strictli decreas second condit theorem hold techniqu show potenti sigmoid natur lead algorithm pac boost properti sinc sigmoid potenti gener better estim loss potenti use adaboost result impli boost algorithm must use potenti subtl properti simpli upper bound loss potenti function boost section give suffici condit potenti function correspond un normal algorithm pac boost properti result impli adaboost logitboost pac boost properti although previous known adaboost believ new result logitboost vc dimens concept class consist pair interv real line suffici adversari duj helmbold one set condit potenti impli decreas roughli exponenti un normal margin larg margin exponenti region idea similar use adaboost analysi show minimum normal margin quickli becom bound away zero allow us bound gener error use theorem bartlett et al second set condit govern behavior potenti function un normal margin larg enough condit impli total potenti decreas constant factor iter therefor much time spent margin enter exponenti region margin valu bound exponenti region ui margin ui remain exponenti region follow theorem give condit ensur ui quickli becom less theorem follow condit hold strictli decreas qp vu im ui er ti bq ln mp iter proof theorem approxim new total potenti old potenti minu ct time linear term plu error bound error function ct minim demonstr valu ct give suffici decreas total potenti theorem follow condit hold iter tx vq whenev im ui strictli decreas ff cp ff vu yft nential txp fro tx ln qv ln decreas expoth proof theorem gener adaboost proof combin two theorem gener bound theorem bartlett et al give follow result vc dimens weak hypothesi class theorem edg exist tx poli ur satisfi condit theorem poli poli time poli exampl norpotenti booster maliz margin least ln pv yfr ln ln log choos appropri make error rate suffici small algorithm correspond pa boost properti appli theorem show adaboost logitboost potenti lead boost algorithm boost potenti section show direct consequ theorem potenti function adaboost logitboost lead boost algorithm note logitboost algorithm analyz exactli describ friedman et al weak learner optim squar loss appear better fit potenti first deriv boost properti adaboost corollari adaboost potenti boost proof prove simpli need show potenti exp satisfi condit theorem done set ln fi andt corollari log likelihood potenti use logitboost boost proof case ln set ln andq exp ur lv thex orem show poli iter condit theorem satisfi conclus paper examin leverag weak learner use gradient descent approach approach direct gener adaboost algorithm adaboost exponenti potenti function replac altern potenti demonstr properti potenti suffici show result algorithm pac booster properti impli result algorithm pac booster appli result sever potenti function literatur new insight gain examin criteria care condit show boost leav tremend freedom choic potenti function valu less perhap freedom use choos potenti function overli concentr noisi exampl still signific gap two set properti still long way classifi arbitrari potenti function boost properti class leverag algorithm one class look distanc success distribut anoth class chang potenti duf helmbom time criteria boost may chang significantli differ approach exampl freund recent present boost algorithm use time vari sigmoid potenti would interest adapt techniqu dynam potenti refer robert schapir design analysi cient learn algorithm mit press michael kearn lesli valiant cryptograph limit learn boolean formula finit automata journal acm januari valiant theori learnabl commun acm novemb yoav freund robert schapir decis theoret gener line learn applic boost journal comput system scienc august eric bauer ron kohavi empir comparison vote classif algorithm bag boost variant machin learn leo breiman arc edg technic report depart statist univers california berkeley avail www stat berkeley edu jerom friedman trevor hasti robert tibshirani addit logist regress statist view boost technic report stanford univers itsch onoda mfiller soft margin adaboost machin learn appear nigel duffi david helmbold geometr approach leverag weak learner paul fischer han ulrich simon editor comput learn theori jth european confer eurocolt page springer verlag march llew mason jonathan baxter peter bartlett marcu frean boost algorithm gradient descent appear nip thoma dietterich experiment comparison three method construct ensembl decis tree bag boost random machin learn appear yoav freund adapt version boost major algorithm proc th annu conf cornput learn theori page acm david haussler michael kearn nick littleston manfr warmuth equival model polynomi learnabl inform comput decemb freund boost weak learn algorithm major inform comput septemb robert schapir yoav freund peter bartlett wee sun lee boost margin new explan effect vote method annal statist robert schapir yoram singer improv boost algorithm use confid rate predict machin learn decemb jyrki kivinen manfr warmuth boost entropi project proc th annu conf cornput learn theori page acm john lafferti addit model boost infer gener diverg proc th annu conf cornput learn theori page acm
abstract partial observ markov decis process pomdp constitut import class reinforc learn problem present uniqu theoret comput difficulti absenc markov properti popular reinforc learn algorithm learn may longer effect memori base method remov partial observ via state estim notori expens altern approach seek stochast memoryless polici observ environ prescrib probabl distribut avail action maxim averag reward per timestep reinforc learn algorithm learn local optim stochast memoryless polici propos jaakkola singh jordan empir verifi present variat algorithm discuss implement demonstr viabil use four test problem introduct reinforc learn techniqu proven quit effect solv markov decis process mdp control problem exact state environ avail learner expect result action depend present state algorithm learn learn optim determinist polici mdp rule everi state prescrib action maxim expect futur reward mani import problem howev exact state environ either inher unknow prohibit expens obtain limit possibl stochast observ environ avail william singh partial observ markov decis process pomdp often much difficult mdp solv distinct sequenc observ action preced given observ pomdp may lead differ probabl occupi underli exact state mdp efficaci action depend hidden exact state environ optim choic may requir know past histori well current observ problem longer markov light difficulti one approach solv pomdp explor environ build memori past observ action reward allow estim current hidden state method produc determinist polici comput expens may scale well problem size furthermor polici requir state estim use memori may complic implement memoryless polici particularli appropri problem state expens obtain inher difficult estim advantag extrem simpl act upon pomdp optim memoryless polici gener stochast polici one observ environ prescrib probabl distribut avail action fact exampl pomdp construct stochast polici arbitrarili better optim determinist polici algorithm propos jaakkola singh jordan jsj investig learn memoryless stochast polici pomdp pomdp differenti reward valu assum environ discret state learner choos action set state transit depend current state action taken markov properti occur probabl result expect reward pomdp learner cannot sens exactli state environ rather perceiv observ messag set accord condit probabl distribut learner gener know size underli state space transit probabl reward function condit distribut messag mdp alway exist polici simultan maxim expect futur reward state case pomdp appropri altern measur merit stochast pomdp polici asymptot averag reward per timestep achiev seek optim stochast polici jsj algorithm make use valu determin infinit horizon differenti reward observ action pair particular denot reward obtain time may defin differenti reward valu qx lm observ oper note rt summand converg zero valu function may defin similarli polici improv jsj algorithm consist method evalu mechan use improv current polici roughli speak action realiz higher differenti reward averag observ assign slightli greater probabl increas averag reward per timestep interpret quantiti compris gradient polici space project onto probabl simplex may written algorithm learn stochast memoryless polici pomdp one vector inner product number action suffici small improv polici may obtain increment alm alm em practic also enforc al tm pm guarante continu explor origin jsj algorithm prescrib use place equat follow renorm method advantag given valu yield increment regardless current valu polici ensur step correct direct also requir differenti reward valu estim evalu pomdp simul fix stochast polici everi occurr observ action pair begin sequenc reward use estim exploit fact defin sum jsj qevalu method recurs averag estim sequenc use socal everi visit mont carlo method order reduc bia varianc caus depend evalu sequenc factor use discount share tail specif time learner make observ mr take action obtain reward number visit mt increment tail discount rate follow updat perform indic function zt mr otherwis fl tt tt fl gt rt zt tail discount factor estim cumul discount effect rt estim rota ro estim correct schedul possibl see correct provid need perform everi step delay estim need evalu method use given polici iter type algorithm independ step evalu interspers polici improv prescrib section howev onlin version algorithm perform polici improv everi step requir old experi gradual forgotten estim respond recent experi achiev multipli previou estim fl timestep decay factor updat via equat replac equat rt altern method also work reason well multipli timestep instead lliam singh numbor iter number iter figur schemat confound two state pomdp evolut estim evolut solid rr dash empir result present result singl run onlin algorithm includ modifi jsj polici improv evalu procedur describ result polici iter version qualit similar statist perform multipl run verifi shown repres algorithm behavior simplifi present fix constant learn rate decay factor problem use pm throughout note howev appropri schedul onlin heurist decreas pm increas would improv perform necessari ensur converg except first problem choos initi polici uniform last two problem valu alm round zero renorm learn polici evalu confound two state problem two state mdp diagram figur becom pomdp two state confound singl observ learner may take action receiv reward either state transit determinist indic diagram note either stationari determinist polici result wherea optim stochast polici assign action probabl result evolut estim polici start initi polici rr shown figur clearli learn polici approach optim stochast polici nr matrix game scissor paper stone glass water scissor paper stone glass water spsgw extens well known scissorspap stone syrranetr zero sum matrix game learner select row oppon select column learner payoff determin matrix entri game theoret solut stochast mix polici guarante learner expect payoff least zero shown use linear program uniqu optim strategi spsgw yield play stone water probabl play scissor paper glass probabl stationari determinist polici result sinc oppon eventu learn anticip learner choic exploit algorithm learn stochast memoryless polici pomdp stone water paper scisso number iter number iter figur diagram scissor paper stone glass water payoff matrix evolut estim evolut zr stone zr water solid zr scissor zr paper zr glass dash formul spsgw pomdp necessari includ state suffici inform allow oppon exploit sub optim strategi thu choos state learner past action frequenc multipli timestep decay factor one observ learner act select row scissor paper stone glass water produc determinist state transit simul oppon play column maxim expect payoff estim learner strategi obtain state learner reward obtain appropri entri payoff matrix polici learn iter see figur close optim polici zr parr russel grid world parr russel grid world consist state grid singl obstacl shown figur learner sens wall immedi east west whether goal state upper right comer penalti state directli goal result possibl observ indic diagram avail action move probabl slip either side move desir direct movement wall result bounc back origin state learner receiv reward transit goal state transit penalti state transit goal penalti state connect cost free absorb state learner reach either teleport immedi new start state chosen uniform probabl result shown figur separ step evalu final learn polici result contrast optim determinist polici indic arrow figur yield parr russel memori base spova rl algorithm achiev learn iter multi server queue timestep arriv job type probabl respect must assign server see figur server optim particular job type complet expect time liarn singh rr number iter alm lo figur parr russel grid world observ shown lower right corner optim determinist memoryless polici repres arrow evolut estim result learn polici observ across column action row timestep job type requir longer job server queue handl parallel capac server finish probabl timestep wheref product expect time job number job server queue state pomdp combin wait job server occup three job type learner observ restrict type wait job state transit obtain remov job finish ad wait job chosen server space avail reward job success place drop result shown figur separ step evalu learn polici obtain correspond success place job contrast optim determinist polici assign job server optim attain success thu learn polici halv drop rate conclus onlin version algorithm propos jaakkola singh jordan effici learn stochast memoryless polici either provabl optim least superior determinist memoryless polici four test problem mani enhanc possibl includ appropri learn schedul improv perform ensur converg estim time observ action visit obtain better discount rate therebi enhanc estim bia varianc reduct see multipl start simul anneal avoid local minima addit observ could extend includ past histori appropri pomdp algorithm use memori attempt learn optim determinist polici base belief state stochast memoryless polici learn jsj algorithm may alway good simpler act upon adapt smoothli non stationari environ moreov search space stochast polici jsj algorithm potenti find optim memoryless polici consider along success simpl implement suggest algorithm may viabl candid solv real world pomdp includ distribut control network admiss rout problem number state enorm complet state inform may difficult obtain estim time manner algorithm learn stochast memoryless polici pomdp job arriv type erver ta orserv ta server tc number iter al esumat figur schemat multi server queue evolut result learn polici observ across column action row acknowledg would like thank mike mozer tim brown help discuss satind singh fund nsf grant ii refer chrisman reinforc learn perceptu alias perceptu distinct approach proceed tenth nation confer artifici intellig jaakkola singh jordan reinforc learn algorithm partial observ markov decis problem advanc neural inform process system littman cassandra kaelbl partial observ environ scale intern confer machin learn learn polici proceed twelfth littman memoryless polici theoret limit practic result proceed third intern confer simul adapt behavior anim animat loch singh use elig trace find best memoryless polici partial observ markov decis process machin learn proceed fifteenth intern confer lovejoy survey algorithm method partial observ markov decis process annal oper research morri introduct game theori springer verlag new york parr russel approxim optim polici partial observ stochast domain proceed intern joint confer artifici intellig singh jaakkola jordan learn without stateestim partial observ markovian decis process machin learn proceed eleventh intern confer sutton barto reinforc learn introduct mit press
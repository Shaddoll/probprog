abstract present unsupervis classif algorithm base ica mixtur model ica mixtur model assum observ data categor sever mutual exclus data class compon class gener linear mixtur independ sourc algorithm find independ sourc mix matrix class also comput class membership probabl data point approach extend gaussian mixtur model class non gaussian structur demonstr method learn effici code repres imag natur scene text learn class basi function yield better approxim underli distribut data thu provid greater code effici believ method well suit model structur high dimension data mani potenti applic introduct recent blind sourc separ bss independ compon analysi ica shown promis signal process applic includ speech enhanc system telecommun medic signal process ica techniqu find linear non orthogon coordin system multivari data direct axe coordin system determin data second higher order statist goal ica linearli transform data transform variabl statist independ unsupervis classif non gaussian mixtur model use ica possibl bell sejnowski cardoso laheld lee et al ica gener techniqu princip compon analysi pca like pca proven use tool find structur data one limit ica assumpt sourc independ present approach relax assumpt use mixtur model mixtur model duda hart observ data categor sever mutual exclus class class variabl model multivari gaussian densiti call gaussian mixtur model gener gaussian mixtur model model class independ variabl ica mixtur model allow model class non gaussian platykurt leptokurt structur algorithm learn paramet deriv use expect maxim em algorithm lee et al demonstr approach show improv perform data classiftc problem appli algorithm learn effici code repres differ type imag ica mixtur model assum data gener mixtur densiti duda hart xio xick ok ck ok unknown paramet call compon densiti assum number class priori probabl ck class known case gaussian mixtur model xlck ok tk zk assum form compon densiti non gaussian data within class describ ica model xk ak bk ak scalar matrix call basi mix matrix bk bia vector class vector sk call sourc vector also coeffici basi vector assum individu sourc si within class mutual independ across data ensembl simplic consid case ak full rank number sourc equal number mixtur figur show simpl exampl dataset describ ica mixtur model class gener eq use differ class gener two uniform distribut sourc wherea class gener two laplacian distribut sourc exp isl task model unlabel data point determin paramet class ak bk probabl class ck ix data point learn algorithm deriv expect maxim approach ghahramani implement follow step comput log likelihood data class logp ck ok logp sk log det ate bk sk comput probabl class given data vector xl ck ck cklx xlok ck ck lee lewicki sejnowsla figur simpl exampl classifi ica mixtur model two class class gener two independ variabl two bia term two basi vector class gener two uniform distribut sourc indic next data class class gener two laplacian distribut sourc sharp peak bia heavi tail inset graph show distribut sourc variabl si basi vector adapt basi function bia term class basi function adapt use gradient ascent aak oc oa gp lx logp xlc note simpli weight standard ica algorithm gradient ct ix gradient also sum multipl data point bia term updat accord bk xtp cklxt ol ckixt data index three step learn algorithm perform gradient ascent total likelihood data eq extend infomax ica learn rule abl blindli separ mix sourc sub super gaussian distribut achiev use simpl type learn rule first deriv girolami learn rule lee et al use stabil analysi cardoso laheld switch sub super gaussian regim learn rule express term call filter matrix aw oc tanh uu unsupervis classif non gaussian mixtur model use ica logp cr log cosh ki element dimension diagon matrix wx unmix sourc sourc estim bell sejnowski ki lee et al ki sign sech ui ui tanh ui sourc distribut super gaussian ki sub gaussian ki log likelihood estim eq term log approxim follow sn super gaussian logp zlogcosh sub gaussian super gaussian densiti approxim densiti model heavier tail gaussian densiti sub gaussian densiti approxim bimod densiti girolami although sourc densiti approxim crude demonstr suffici standard ica problem lee et al learn spars represent laplacian prior exp isl use weight updat simplifi infomax learn rule aw cr sign logp cr isnl laplacian prior learn effici code imag recent sever approach propos learn imag code util set linear basi function olshausen field use spars criterion found code similar local orient recept field similar result present bell sejnowski use infomax algorithm lewicki olshausen use bayesian approach appli ica mixtur model present result show higher degre flexibl encod imag use imag natur scene obtain olshausen field text imag scan newspap articl train set consist pixel patch select randomli imag type figur illustr exampl imag patch two complet basi vector randomli initi gradient eq stepsiz comput function amplitud basi vector number iter algorithm converg iter learn two class basi function shown figur figur top show basi function correspond natur imag basi function show gaborl like structur previous report olshausen field bell sejnowski lewicki olshausen howev figur bottom show basi function correspond text imag basi function resembl bar differ length width captur high frequenc structur present text imag compar code effici compar code effici ica mixtur model similar model use shannon theorem obtain lower bound number bit gaussian modul siusoid lee lewicki sejnowski figur exampl natur scene text imag pixel imag patch randomli sampl imag use input ica mixtur model requir encod pattern bit log nlog dimension input pattern rr code precis standard deviat nois introduc error encod tabl compar code effici five differ method show number bit requir encod three differ test data set imag patch natur scene imag patch text imag imag patch imag type use five differ encod method ica mixtur model natur train ica text train ica natur text train ica pca train three test set clear ica basi function train natur scene imag exhibit best encod natur scene present column natur appli text imag column text note text train yield reason basi data set natur train give good basi natur ica mixtur model show encod power individu test data set give best encod imag type present case encod differ ica mixtur model pca signific ica mixtur yield small improv ica train imag type expect size improv greater situat greater differ among class advantag mixtur model imag patch automat classifi discuss new algorithm unsupervis classif present base maximum likelihood mixtur model use ica model structur class demonstr algorithm learn effici code repres differ imag type natur scene text imag case learn class basi function show improv pca encod ica mixtur model show better imag compress rate tradit compress algorithm jpeg ica mixtur model nonlinear model class model linear process choic class model use probabl model unsupervis classif non gaussian mixtur model use ica jjjlg jjjjj jjjjjjll jj jljj jjjjj jjjj jjjjl im jjjjjjiii jj jjjjj jjijjjlljjjjj figur left basi function class correspond natur imag right basi function class correspond text imag tabl compar code effici test data train set model natur text natur text ica mixtur natur train ica text train ica natur text train ica pca code effici bit per pixel five method compar three test set code precis set bit natur er text er therefor seen nonlinear ica model furthermor one way relax independ assumpt whole data set ica mixtur model condit independ model independ assumpt hold within class may depend among class differ view ica mixtur model think class overcomplet represent compar approach lewicki sejnowski main differ basi function learn mutual exclus class use set basi function method similar approach includ mixtur densiti network bishop neural network use find arbitrari densiti function algorithm reduc gaussian mixtur model sourc prior gaussian pure gaussian structur howev rare real data set use prior form super gaussian sub gaussian densiti could extend propos attia propos model use learn complet set basi function without addit nois howev method extend take account addit gaussian nois overcomplet set basi vector lewicki sejnowski lee et al perform sever experi benchmark data set classif problem result compar improv obtain autoclass stutz cheeseman use gaussian mixtur lee lewicki sejnowski model furthermor show algorithm appli blind sourc separ nonstationari environ method switch automat learn mix matric differ environ lee et ed may prove use automat detect sleep stage observ eeg signal method identifi stage due chang sourc prior mix potenti applic propos method includ problem nois remov problem fill miss pixel believ method provid greater flexibl model structur high dimens data mani potenti applic refer attia blind separ noisi mixtur em algorithm independ factor analysi neural comput press bell sejnowski inform maxim approach blind separ blind deconvolut neural comput bell sejnowski independ compon natur scene edg filter vision research bishop mixtur densiti network technic report ncrg cardoso laheld equivari adapt sourc separ ieee tran duda hart pattern classif scene analysi wiley new york ghahramani solv invers problem use em approach densiti estim proceed connectionist model summer school page girolami altern perspect adapt independ compon analysi algorithm neural comput lee girolami bell sejnowski unifi framework independ compon analysi intern journal mathemat comput model press lee girolami sejnowski independ compon analysi use extend infomax algorithm mix sub gaussian supergaussian sourc neural comput lee lewicki sejnowski ica mixtur model unsupervis classif automat context switch intern workshop ica aussoi press lewicki olshausen infer spars overcomplet imag code use effici code framework advanc neural inform process system page lewicki sejnowski learn nonlinear overcomplet represen effici code advanc neural inform process system page olshausen field emerg simpl cell recept field properti learn spars code natur imag natur stutz cheeseman autoclass bayesian approach classif maximum entropi bayesian method kluwer academ publish
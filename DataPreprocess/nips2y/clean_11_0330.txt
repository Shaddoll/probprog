abstract new algorithm support vector regress describ priori chosen automat adjust flexibl tube minim radiu data fraction data point lie outsid moreov shown use parametr tube shape non constant radiu algorithm analys theoret experiment introduct support vector sv machin compris new class learn algorithm motiv result statist learn theori vapnik origin develop pattern recognit repres decis boundari term typic small subset sch kopf et al train exampl call support vector order properti carri case sv regress vapnik devis call insensit loss function max ly penal error chosen priori algorithm henceforth call svr seek estim function base data minim regular risk function iiw top constant determin trade niinim train error minim model complex term ilwll ernp paramet use desir accuraci approxim specifi beforehand case howev want estim accur possibl without commit certain level accuraci present modif svr algorithm automat minim thu adjust accuraci level data hand shrink tube new support vector regress algorithm sv regress sv regress estim function empir data proceed follow sch kopf et al point xi allow error everyth captur slack variabl shorthand impli variabl without asterisk penal object function via regular constant chosen priori vapnik tube size trade model complex slack variabl via constant minim subject xi yi understood bold face greek letter denot dimension vector correspond variabl introduc lagrangian multipli cr tli obtain wolf dual problem moreov boser et al substitut kernel dot product correspond dot product featur space relat input space via nonlinear map lead svr optim problem maxim ct cti yi subject regress estim shown take form ct cti xi comput take account substitut ctj ctd xd understood becom equal point respect due karush kuhn tucker condit cf vapnik latter moreov impli kernel expans cr nonzero correspond constraint precis met respect pattern xi refer support vector give theoret result explain signific paramet follow observ concern help sinc pay increas cf still happen data nois free perfectli interpol low capac model case howev interest correspond plain lt loss regress use term error refer train point lie outsid tube term fraction error sv denot rel number error sv divid proposit assum follow statement hold upper bound fraction error ii lower bound fraction sv sch lkopf bartlett smola williamson iii suppos data gener iid distribut ylx continu probabl asymptot equal fraction sv fraction error first two statement proposit proven structur dual optim problem play crucial role present instead give graphic proof base primal problem fig understand third statement note error also sv sv error name lie exactli edg tube asymptot howev sv form neglig fraction whole sv set set error one sv essenti coincid due fact class function well behav capac sv regress function distribut satisfi continu condit number point tube edg pass cannot asymptot increas linearli sampl size interestingli proof sch kopf et al use uniform converg argument similar spirit use statist learn theori due proposit use control number error note impli sinc ct vapnik moreov sinc constraint impli equival conclud proposit actual hold upper lower edg tube separ asid note argument number sv two edg standard svr tube asymptot agre moreov note bear robust svr first glanc svr seem robust use insensit loss function pattern outsid tube contribut empir risk term wherea pattern closest estim regress zero loss howev mean outlier determin regress fact contrari case one show local movement target valu ti point xi outsid tube influenc regress sch kopf et al henc svr gener estim mean random variabl throw away largest smallest exampl fraction either categori estim mean take averag two extrem one remain exampl close spirit robust estim like trim mean let us briefli discuss new algorithm relat svr vapnik rewrit constrain optim problem deriv dual much like svr figur graphic depict trick imagin increas start first term cf increas proport second term decreas proport fraction point outsid tube henc grow long latter fraction larger optimum therefor must proposit next imagin decreas start larg valu chang first term proport time chang second term proport fraction sv even point edg tube contribut henc shrink long fraction sv smaller eventu lead proposit ii shrinla ng tube new tpport vector reg ession lgorithm one arriv follow quadrat program maxim cu ct cti ctt cti yi ct cti ctj ctj xi xd subject compar addit term make plausibl constraint need follow sens svr includ svr note gener case use kernel vector featur space proposit svr lead solut svr set priori valu solut proof minim fix minim remain variabl solut chang parametr insensit model gener svr consid tube given instead estim model paramet far retain assumpt insensit zone tube slab shape go one step use parametr model arbitrari shape let understood set posit function consid follow quadrat program given minim iiwll subjectto xi yi qsq xi calcul analog sec show wolf dual consist maxim subject instead modifi constraint xi gq experi sec use simplifi version optim problem drop term object function use sq render problem symmetr respect two edg tube addit use lead wolf dual except last constraint becom cf advantag set sinc use side tube comput straightforward instanc solv linear system use two condit describ follow otherwis gener statement harder make linear system zero determin depend whether function evalu xi linearli depend latter occur instanc use constant function case pointless use two differ valu constraint impli sum bound min conclud section give without proof gener proposit iii optim problem constraint sch lkopf bartlett smola lliamson proposit assum suppos data gener iid distribut ylx vlx continu probabl asymptot fraction sv error equal dp asymptot distribut sv experi discuss experi use optim loqo http www princeton edufrvdb serendipit advantag primal variabl recov dual variabl wolf dual doubl dual variabl fed optim fig task estim regress noisi sinc function given exampl drawn uniformli ti sin ci ci vi vi drawn gaussian zero mean varianc use default paramet rbf kernel figur give illustr one make use parametr insensit model propos sec use proper model estim get much better parametr case use sin due dp correspond standard choic svr cf proposit experiment find consist asymptot predict theoret even assum uniform distribut sv got fraction sv error respect method allow incorpor prior knowledg loss function although approach first glanc seem fundament differ incorpor prior knowledg directli kernel sch kopf et al point view statist figur left sv regress top bottom larger allow point lie outsid tube see sec algorithm automat adjust top bottom shown sinc function dot regress tube middl sv regress data nois top bottom case tube width automat adjust nois top bottom right sv regress vapnik data nois rr top bottom case choic specifi priori ideal neither case top figur regress estim bias bottom figur match extern nois cf smola et al shrink tube new support vector regress algorithm figur toy exampl use prior knowledg depend nois addit nois multipli sin left function use parametr insensit tube sec right svr standard tube tabl result boston hous benchmark top svr bottom svr mse mean squar error std standard deviat thereof trial error fraction train point outsid tube sv fraction train point sv automat mse std error sv mse std error sv learn theori two approach close relat case structur loss function induc class function object interest gener error bound custom first case chang loss function second case chang class function estim taken empir studi use svr report excel perform wide use boston hous regress benchmark set stitson et al due proposit differ svr standard svr lie fact differ paramet vs specifi priori consequ experi interest paramet simpli adjust width exp iix yl sch kopf et al use input dimension origin valu correct sinc present case maxim valu perform run time overal set exampl randomli split train set exampl test set exampl tabl show wide rang note make sens obtain perform close best perform achiev select priori look test set final note although use valid techniqu select optim valu obtain perform state art stitson et al report mse svr use anova kernel bag tree tabl moreov show use control fraction sv error discuss theoret experiment analysi suggest provid way control upper bound number train error tighter one use soft margin hyperplan vapnik mani case make paramet conveni one svr asymptot directli control sch lkopf bartlett smola williamson number support vector latter use give leav one gener bound vapnik addit character compress ratio suffic train algorithm sv lead solut sch kopf et al svr tube width must specifi priori svr gener idea trim mean comput automat desir properti svr includ formul definit quadrat program spars sv represent solut retain optimist mani applic svr robust svr among reduc set algorithm osuna girosi approxim sv pattern recognit decis surfac svr give direct handl desir speed one immedi question approach sv regress rais whether similar algorithm possibl case pattern recognit question recent answer affirm sch kopf et al sinc pattern recognit algorithm vapnik use paramet dispos use regular constant lead dual optim problem homogen quadrat form lower bound sum lagrang multipli whether could abolish regress case open problem acknowledg work support arc dfg ja refer boser guyon vapnik train algorithm optim margin classifi haussler editor proceed th annual acm workshop comput learn theori page pittsburgh pa acm press osuna girosi reduc run time complex support vector machin sch kopf burg smola editor advanc kernel method support vector learn page mit press cambridg sch kopf burg vapnik extract support data given task fayyad uthurusami editor proceed first intern confer knowledg discoveri data mine aaai press menlo park ca sch kopf bartlett smola williamson support vector regress automat accuraci control niklasson bod ziemk editor proceed th intern confer artifici neural network perspect neural comput page berlin springer verlag sch kopf simard smola vapnik prior knowledg support vector kernel jordan kearn solla editor advanc neural inform process system page cambridg mit press sch kopf smola williamson bartlett new support vector algorithm neurocolt tr cf http iwww neurocolt com sch kopf sung burg girosi niyogi poggio vapnik compar support vector machin gaussian kernel radial basi function classifi ieee tran sign process smola murata sch kopf moller asymptot optim choic loss support vector machin niklasson bod ziemk editor proceed th intern confer artifici neural network perspect neural comput page berlin springer verlag stitson gammerman vapnik vovk watkin weston support vector regress anova decomposit kernel sch kopf burg smola editor advanc kernel method support vector learn page mit press cambridg vapnik natur statist learn theori springer verlag new york
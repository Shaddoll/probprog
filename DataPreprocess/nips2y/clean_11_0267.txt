abstract analyz asymptot behavior autoregress neural network ar nn process use techniqu markov chain non linear time seri analysi shown standard ar nn without shortcut connect asymptot stationari linear shortcut connect allow shortcut weight determin whether overal system stationari henc standard condit linear ar process use introduct paper consid popular class nonlinear autoregress process driven addit nois defin stochast differ equat form et iid nois process feedforward neural network paramet weight vector call equat autoregress neural network process order short ar nn follow ar nn natur gener classic linear autoregress ar process ft see brockwel davi comprehens introduct ar arma autoregress move averag model leisch trapletti hornik one central question linear time seri theori stationar model whether probabilist structur seri constant time least asymptot constant start equilibrium surprisingli question gain much interest nn literatur especi knowledg result give condit stationar arnn model result stationar hopfield net wang sheng net cannot use estim condit expect time seri predict rest paper organ follow section recal result time seri analysi markov chain theori defin relationship time seri associ markov chain section use result establish standard ar nn model without shortcut connect stationari also give condit ar nn model shortcut connect stationari section examin nn model import class non stationari time seri name integr seri proof defer appendix time seri markov chain theori stationar let denot time seri gener possibl nonlinear autoregress process defin eet equal condit expect le tl best predict mean squar sens interest long term properti seri may ask whether certain featur mean varianc chang time remain constant time seri call weakli stationari cov vt mean covari depend time stronger criterion whole distribut mean covari process depend time case seri call strictli stationari strong stationar impli weak stationar second moment seri exist detail see standard time seri textbook brockwel davi strictli stationari vt call stationari distribut seri obvious seri stationari begin start stationari distribut start constant call seri asymptot stationari converg stationari distribut lim lp oo time seri markov chain use notat xt xt xt et et write scalar autoregress model order order vector model xt xt et first tationar stabil autoregress neural network process xt et chan tong write aix pl probabl go point set step xt form markov chain state space irp borel set usual lebesgu ure markov chain xt call irreduc finit measur vx whenev mean essenti part state space reach markov chain irrespect start point anoth import properti markov chain aperiod loos speak mean infinit often repeat cycl see tong detail markov chain xt call geometr ergod exist probabl measur vx pllp denot total variat satisfi invari equat dx va close relationship time seri associ markov chain markov chain geometr ergod distribut converg rr time seri asymptot stationari time seri start distribut seri strictli stationari stationar ar nn model appli concept defin section case defin neural network let denot dimension input vector consid follow standard network architectur singl hidden layer perceptron iz scalar weight ai dimension weight vector bound sigmoid function tanh singl hidden layer perceptron shortcut connect io oq ix addit weight vector shortcut connect input output case defin characterist polynomi associ linear shortcut cpz zgc leisch trapletti hornik radial basi function network rail rn center vector one usual bound radial basi function exp lemma let zt defin let leletl let pdf et posit everywher defin markov chain xt irreduc aperiod lemma basic say state space markov chain point reach cannot reduc depend start point exampl reduc markov chain would seri alway posit neg otherwis cannot happen ar nn case due unbound addit nois term theorem let defin xt let ml tl pdf et posit everywher network without linear shortcut defin xt geometr ergod asymptot stationari network linear shortcut defin addit lz xt geometr ergod asymptot stationari time seri remain stationari allow one hidden layer multi layer perceptron mlp non linear output unit long overal map bound rang mlp shortcut connect combin possibl non stationari linear ar process non linear stationari nn part thu nn part use model non linear fluctuat around linear process like random walk part network control whether overal process stationari linear shortcut connect present shortcut process alway stationari shortcut usual test stabil linear system appli integr model import method classic time seri analysi first transform nonstationari seri stationari one model remaind stationari process probabl popular model kind autoregress integr move averag arima model transform stationari arma process simpl differenc let denot th order differ oper ag stationar stabil autoregress neural network process standard random walk lnt et non stationari grow varianc transform iid henc stationari nois process et take first differ time seri non stationari transform stationari seri take th differ call seri integr order standard mlp rbf without shortcut asymptot stationari therefor import take care network use model stationari process cours network train mimic non stationari process finit time interv sampl predict perform poor network inher cannot captur import featur process one way overcom problem first transform process stationari seri differenc integr seri train network transform seri chng et al differenc linear oper transform also easili incorpor network choos shortcut connect weight input hidden unit accordingli assum want model integr seri integr order hk hk hk ak stationari equival model mlp shortcut connect defin shortcut weight vector fix forn akxt alway possibl basic obtain ad weight input first hidden layer ar nn model integr seri integr order order integr known shortcut weight either fix differenc seri use input order unknown also train complet network includ shortcut connect implicitli estim order integr train final model check stationar look characterist root polynomi defin shortcut connect fraction integr consid integr seri posit integ order integr last year model fraction integr order becam popular seri integr order shown exhibit self similar fractal behavior long memori type process introduc mandelbrot seri paper model river flow see mandelbrot ness recent self similar process use model ethernet traffic leland et al also financi time seri foreign exchang data seri exhibit long memori self similar leisch trapletti hornik fraction differenc oper defin seri expans et et obtain taylor seri first use equat seri fraction remaind practic comput seri cours truncat term arnn model shortcut connect approxim seri first term summari shown ar nn model use standard nn architectur without shortcut asymptot stationari linear shortcut input output includ mani popular softwar packag alreadi implementedthen weight shortcut connect determin overal system stationari also possibl model mani integr time seri kind network asymptot behavior ar nn especi import paramet estim predict larger interv time use network gener artifici time seri limit normal distribut paramet estim guarante stationari seri therefor alway recommend transform non stationari seri stationari seri possibl differenc train network anoth import aspect stationar singl trajectori display complet probabl law process observ one long enough trajectori process theori estim interest quantiti process averag time need true non stationari process gener quantiti may estim averag sever independ trajectori one might train network avail sampl use train network afterward driven artifici nois random number gener gener new data similar properti train sampl asymptot stationar guarante ar nn model cannot show explos behavior grow varianc time current work extens paper sever direct ar nn process shown strong mix memori process vanish exponenti fast autocorrel go zero exponenti rate anoth question thorough analysi properti paramet estim weight test order integr final want extend univari result multivari case special interest toward cointegr process acknowledg piec research support austrian scienc foundat fwf grant sfb adapt inform system model econom manag scienc stationar stabil autoregress neural network process appendix mathemat proof proof lemma easili shown xt irreduc support probabl densiti function pdf et whole real line pdf posit everywher chan tong case everi non null dimension hypercub reach step posit probabl henc everi non null borel set necessari suffici condit xt aperiod exist set posit integ tong case true due unbound addit nois proof theorem use follow result nonlinear time seri theori theorem chan tong let xt defin let compact preserv compact set decompos gn gd andgd bound rang gn continu homogen gn ax agn origin fix point gn gn uniform asymptot stabl ie atl pdf et posit everywher ii xt geometr ergod nois process fulfil condit assumpt clearli network continu compact function standard mlp without shortcut connect rbf bound rang henc gn ga seri asymptot stationari allow linear shortcut connect input output get gn ga ier ix gn linear shortcut part network ga standard mlp without shortcut connect clearli gn continu homogen origin fix point henc seri asymptot stationari gn asymptot stabl characterist root gn magnitud less uniti obvious true rbf shortcut connect note model reduc standard linear ar model ga refer brockwel davi time seri theori method springer seri statist new york usa springer verlag chan tong use determinist lyapunov function ergod stochast differ equat advanc appli probabl chng chen mulgrew gradient radial basi function network nonlinear nonstationari time seri predict ieee transact neural network husmeier taylor predict condit probabl densiti stationari stochast time seri neural network jone nonlinear autoregress process proceed royal societi london leland taqqu willing wilson self similar natur ethernet traffic extend version ieee acm transact network mandelbrot ness fraction browninn motion fraction nois applic siam review tong non linear time seri dynam system approach new york usa oxford univers press wang sheng asymptot stationar discret time stochast neural network neural network
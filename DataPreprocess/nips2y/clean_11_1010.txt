abstract agent act real world confront problem make good decis limit knowledg environ partial observ markov decis process pomdp model decis problem agent tri maxim reward face limit sensor feedback recent work shown empir reinforc learn rl algorithm call sarsa effici find optim memoryless polici map current observ action pomdp problem loch singh sarsa algorithm use form short term memori call elig trace distribut tempor delay reward observ action pair lead reward paper explor effect elig trace abil sarsa algorithm find optim memoryless polici variant sarsa call step truncat sarsa appli four test problem taken recent work littman littman cassandra kaelbl parr russel chrisman empir result show elig trace significantli truncat without affect abil sarsa find optim memoryless polici pomdp introduct agent oper real world mobil robot must use sensor best give partial inform state environ inform robot surround necessarili incomplet due noisi imperfect sensor occlud object inabl robot know precis agentenviron system model partial observ markov decis process pomdp sondik varieti algorithm develop solv pomdp lovejoy howev techniqu scale well problem involv dozen state due comput complex solut method cassandra littman therefor find effici reinforc learn effect elig trace find optim memoryless polici method solv pomdp great practic interest artifici intellig engin field recent work shown empir sarsa algorithm effici find best determinist memoryless polici sever pomdp problem recent literatur loch singh empir result loch singh suggest elig trace necessari find best optim memoryless polici reason variant sarsa call step truncat sarsa formul explor effect elig trace abil sarsa find best memoryless polici main contribut paper show empir variant sarsa use truncat elig trace find optim memoryless polici sever pomdp problem literatur specif show step truncat sarsa method find optim memoryless polici four pomdp problem test sarsa pomdp environ defin finit set state agent choos finit set action agent sensor provid observ finit set execut action state agent receiv expect reward rs environ transit state probabl pass probabl agent observ given state xl straightforward way extend rl algorithm pomdp learn valu function observ action pair simpli treat agent observ state describ standard sarsa algorithm appli pomdp time step qvalu function denot qt elig trace function denot lt reward receiv denot experienc transit xt rt xt follow updat perform order lt xt rh lt xt qt qt fit rh fit rt qt xt qt xt step size learn rate elig trace initi zero episod task reiniti zero everi episod greedi polici time step assign observ action argmax qt sarsa use truncat elig trace sarsa truncat elig trace use paramet set elig trace observ action pair zero observ action pair visit within last time step thu step truncat sarsa equival sarsa step truncat sarsa updat valu current observ action pair immedi preced observ action pair loch empir result truncat sarsa algorithm appli ident manner four pomdp problem taken recent literatur complet descript state action observ reward problem provid loch singh describ aspect empir result common four problem step agent select random action probabl equal explor rate paramet select greedi action otherwis initi explor rate use decreas linearli action step th action onward explor rate remain fix valu initi step size tx valu held constant experi discount factor valu use four problem sutton grid world sutton grid world littman agent environ system state observ action state transit observ determinist step truncat elig trace equival sarsa abl find polici could reach goal start state within step goal state shown figur optim memoryless polici yield total step goal state found step step step truncat elig trace method shown figur figur sutton grid world littman total step goal perform function number learn step step elig trace effect elig trace find optim memoryless polici chrisman shuttl problem chrisman shuttl problem agent environ system state observ action state transit observ stochast step truncat elig trace equival sarsa unabl find polici could could reach goal state figur optim memoryless polici yield averag reward per step found step step step truncat elig trace method shown figur fibr sm shuttl problem averag rew per step ffo ce ion number ofl ng step step eli iw ace littman cassandra kaelbl state offic world littman al state offic world littman agent environ system state observ action state transit observ stochast step truncat elig trace equival sarsa abl find polici could reach goal state trial figur step step step truncat elig trace method converg best memoryless polici found loch singh yield success rate reach goal state figur loch oq tl fi linm et state offic world cent success tfi reach go peffom ce nction number ofl ng step step eli bili tr parr russei grid world parr russel grid world parr russel agent environ system state observ action state transit stochast observ determinist optim memoryless polici yield averag reward per step found step step truncat elig trace method figur polici found step step method optim result attribut sharp elig trace cutoff effect observ smoothli decay elig trace effect elig trace find optim memoryless polici ro figur parr russel grid world averag reward per step perform function number learn step step elig trace discuss empir result present shown step truncat sarsa algorithm abl find best optim determinist memoryless polici result surpris sinc expect length elig trace requir find good optim polici would vari wide depend problem specif factor landmark uniqu observ space delay critic decis reward sever addit pomdp problem formul attempt creat pomdp would requir valu greater find optim polici howev trial pomdp test optim memoryless polici could found conclus futur work abil sarsa algorithm step truncat sarsa algorithm find optim determinist memoryless polici class pomdp problem import sever reason pomdp good memoryless polici sarsa algorithm provid effici method find best polici space perform memoryless polici unsatisfactori observ action space agent modifi produc agent good memoryless polici design autonom system agent modifi observ loch space agent either ad sensor make finer distinct current sensor valu addit design add attribut past observ current observ space action space modifi ad lower level action ad new action space thu one method design capabl agent iter select observ action space agent use sarsa find best memoryless polici space repeat satisfactori perform achiev suggest futur line research autom process observ action space select acheiv accept perform level avenu research includ explor theoret reason sarsa step truncat sarsa abl solv pomdp addit research need conduct short elig trace work well wide class pomdp refer cassandra optim polici partial observ markov decis process technic report cs brown univers depart comput scienc provid ri littman wit algorithm solv partial observ markov decis process technic report cs brown univers depart comput scienc provid ri littman cassandra kaelbl learn polici partial observ environ scale proceed twelfth intern confer machin learn page san francisco ca morgan kaufmann loch singh use elig trace find best memoryless polici partial observ markov decis process appear proceed fieenth intern confer machin learn tg madison wi morgan kaufmann avail http www cs colorado edu baveja paper html lovejoy survey algorithm method partial observ markov decis process annal oper research parr russel approxim optim polici partial observ stochast domain proceed intern joint confer artifici intellig sondik optim control partial observ markov decis process infinit horizon discount cost inoper research sutton integr architectur learn plan react base approxim dynam program proceed seventh intern confer machin learn page san mateo ca morgan kaufman littman memoryless polici theoret limit practic result anim animat proceed third internatiot confer simul adapt behavior cambridg mit press
abstract base simpl convex lemma develop bound differ type bayesian predict error regress gaussian process basic bound formul fix train set simpler express obtain sampl input distribut equal weight function covari kernel yield asymptot tight result result compar numer experi introduct nonparametr bayesian model base gaussian prior function space becom increasingli popular neural comput commun see sinc model class consid approach infinit dimension applic vapnik chervonenki type method determin bound learn curv nontrivi perform far knowledg method target function learnt fix input data drawn independ random fix unknown distribut approach paper differ assum target actual drawn random known prior distribut interest develop simpl bound averag predict perform respect prior hold fix set input later stage averag input distribut made gener bound bay error regress gaussian process regress gaussian process explain gaussian process scenario regress problem assum observ input point corrupt valu function independ gaussian nois varianc appropri stochast model given likelihood po ylx rcr goal learner give estim function base set observ exampl data yt prior inform unknown function asum realiz gaussian random field zero mean covari use expand random function wkok complet set determinist function random gaussian coeffici well known chosen orthonorm eigenfunct integr equat qb dx ak eigenvalu nonneg weight function priori statist wt simpl independ gaussian variabl satisfi lg wt predict bay error usual posterior mean chosen predict new point base dataset dn xn yn explicit form easili deriv use expans tb fact gaussian random variabl mean coincid probabl valu maxim log posterior respect one find infinit dimension vector result av xi xi yi zi fix set input bayesian predict error point given evalu yield work express zlz av matrix ot properti ei zi dz defin bayesian train empir averag error datapoint train set bayesian gener averag error weight function get opper lh varelli entrop error order understand next type error assum data arriv sequenti one predict distribut train data new input xt posterior expect likelihood ylxt ot po xt lot let lt bayesian averag rel entropi kullback leibler diverg predict distribut true distribut po data gener lt dki also shown lt henc predict error small cumul entrop error ee defin sum loss give integr learn curv time one show lt xt dt lgd iip trln av ii po yil xi ii po yilx first equal may found second follow direct calcul bound fix set input order get bound use lemma use quantum statist mechan get bound free energi lemma special function prove sir rudolf peierl order keep paper self contain includ proof appendix lemma let real symmetr matrix convex real function wr tk note concav function bound goe direct immedi get xk ln ln nv rightmost inequ assum input compact region defin supx entrop case may also prove hadamard inequ gener bound bay error regress gaussian process averag case bound next assum input data drawn random denot expect respect distribut assum independ fact margin distribut input ident use jensen inequ akuk xn ln nu result especi simpl weight function probabl densiti input margin distribut case simpli case train gener error sandwich bound expect bound eb becom asymptot exact oo intuit clear train gener error approach asymptot fact may also understood show asymptot equal cumul entrop error within factor cumul gener error integr lower bound obtain precis upper bound factor show upper lower bound show behaviour simul compar bound simul averag train error gener error case data drawn result entrop error given elsewher special case covari kernel rbf form exp xt follow zhu et al th eigenvalu spectrum oo written ab lengthscal process estim averag generalis error train set base exact analyt express distribut dataset use mont carlo approxim begin let us consid sampl dimension input space gener train set whose data point normal distribut around zero unit varianc gener expect train generalis error gp evalu use data point set valu lengthscal let nois level assum sever valu figur show result obtain valu lengthscal effect stretch train learn curv thu result experi perform differ qualit similar present opper lq varelli lo loo looo figur figur show graph train learn curv bound obtain nois level set figur figur graph drawn solid line confid interv sign dot curv bound drawn dash dot line figur figur bound lie within train learn curv upper bound et lower bound bound tighter process higher nois level particular larg dataset error bar curv et eg overlap bound curv et eg approach zero log bound also appli higher dimens use covari exp obvious integr kernel direct product rbf kernel one coordin eigenvalu problem immedi reduc one singl variabl eigenfunct eigenvalu simpli product singl coordin problem henc use bit combinator bound ev written cr adb eb nadbk defin perform experi correl length along direct input space set nois level rr graph curv error bar report figur figur rs discuss base minim requir train input covari conjectur bound cannot improv much without make detail assumpt model distribut observ simul tight bound eb depend dimens input space particular larg dataset tighter small dimens input space figur show quit clearli sinc overlap error bar gener bound bay error regress gaussian process figur figur show graph train learn curv bound obtain squar exponenti covari function input space figur figur figur et eg drawn solid line confid interv sign dot curv bound drawn dash dot line train learn curv larg numer simul perform use modifi bessel covari function order describ random process time mean squar differenti shown bound eb becom tighter smoother process acknowledg grate mani inspir discuss william would like thank peter sollich conjectur exact lower bound gener error motiv part work support studentship british aerospac appendix proof lemma let complet set orthonorm eigenvector ei correspond set eigenvalu properti hk ei ij get xk second equal follow orthonorm inequ use fact complet may regard probabl convex jensen inequ use use eigenvalu equat sum carri help complet relat order obtain last line opper varelli refer mackay gaussian process replac neural network nip tutori may obtain http wo ra phi cam ac uk pub mackay neal bayesian learn neural network lectur note statist springer william comput infinit network neural inform process system mozer jordan petsch ed mit press william rasmussen gaussian process regress neural inform process system touretzki mozer hasselmo ed mit press neal mont carlo implement gaussian process model bayesian regress classif technic report crg tr dept comput scienc univers toronto gibb mackay variat gaussian process classifi preprint cambridg univers barber william gaussian process bayesian classif via hybrid mont carlo neural inform process system mozer jordan petsch ed mit press william barber bayesian classif gaussian process preprint aston univers haussler opper mutual inform metric entropi cumul rel entropi risk annal statist vol peierl phi rev zhu william rohwer morciniec gaussian regress optim finit dimension linear model technic report ncrg aston univers
abstract provid abstract character boost algorithm gradient decsent cost function inner product function space prove converg function gradient descent algorithm quit weak condit follow previou theoret result bound gener perform convex combin classifi term gener cost function margin present new algorithm doom ii perform gradient descent optim cost function experi sever data set uc irvin repositori demonstr doom ii gener outperform adaboost especi high nois situat overfit behaviour adaboost predict cost function introduct consider interest recent vote method pattern classif predict label particular exampl use weight vote set base classifi recent theoret result suggest effect algorithm due tendenc produc larg margin classifi loos speak combin classifi correctli classifi train data larg margin error probabl small gave improv upper bound misclassif probabl combin classifi term averag train data certain cost function margin paper also describ doom algorithm directli minim margin cost function adjust weight associ boost algorithm gradient descent base classifi base classifi suppil doom doom exhibit perform improv adaboost even use base hypothes provid addit empir evid margin cost function appropri quantiti optim paper present gener class algorithm call anyboost gradient descent algorithm choos linear combin element inner product function space minim cost function normal oper weak learner shown equival maxim certain inner product prove converg anyboost weak condit section show gener class algorithm includ special case nearli exist vote method section present experiment result special case anyboost minim theoret motiv margin cost function experi show new algorithm typic outperform adaboost especi true label nois addit theoret motiv cost function provid good estim error adaboost sens use predict overfit behaviour anyboost let denot exampl space measur typic ir space label usual discret set subset ir let denot class function base hypothes map lin denot set linear combin function let inner product lin lin cost function lin aim find function lin minim proceed iter via gradient descent procedur suppos lin wish find new add cost el decreas small valu view function space term ask direct el rapidli decreas desir direct simpli neg function deriv vc oc al vc oc indic function sinc restrict choos new function gener possibl choos vc instead search greatest inner product vc choos maxim vc motiv observ first order el vc henc greatest reduct cost occur maxim vc reason becom obviou later algorithm choos attempt maxim vc describ weak learner preced discuss motiv algorithm anyboost iter algorithm find linear combin base hypothes minim cost function note allow base hypothes take valu arbitrari set restrict form cost inner product specifi step size appropri choic mason baxter bartlett andm frean thing made appli algorithm concret situat note also algorithm termin vc ft ft weak learner return base hypothesi ft longer point downhil direct cost function thu algorithm termin first order step function space direct base hypothesi return would increas cost algorithm anyboost requir inner product space contain function map set class base classifi differenti cost function lin weak learner accept lin return larg valu vc let fo fort ototdo let ft vu ft ft return ft end choos wt let ft ft wtq lfiq end return ft gradient descent view vote method restrict attent base hypothes map inner product xi xi lin xl yl set train exampl gener accord unknown distribut aim find lin pr sgn minim sgn sgn otherwis word sgn minim misclassif probabl margin exampl defin yf consid margin cost function defin differenti real valu function margin definit quick calcul show ra vc rn yif xi yif xi sinc posit margin correspond exampl correctli label sgn neg margin incorrectli label exampl sensibl cost function boost algorithm gradient descent tabl exist vote method view anyboost margin cost function algorithm cost function step size adaboost yf line search arc yf confidenceboost yf line search logitboost ln yf newton raphson margin monoton decreas henc yif xi alway posit divid eim yif xi see find maxim vc equival find minim weight error yir xi eim yif xi mani success vote method appropri choic margin cost function step size specif case anyboost algorithm see tabl detail analysi found full version paper converg anyboost section provid converg result anyboost algorithm quit weak condit cost function prescript given step size wt result converg guarante practic almost alway smaller necessari henc fix small step form line search use follow theorem proof omit see suppli specif step size anyboost character limit behaviour step size theorem let lin lower bound lipschitz differenti cost function exist vc vc ii lin let fo sequenc combin hypothes gener anyboost algorithm use step size vc ft ft wtq lllf anyboost either halt round ft ftq ft converg finit valu case limt ivc ft ft next theorem proof omit see show weak learner alway find best weak hypothesi ft round anyboost cost function convex accumul point sequenc ft gener anyboost step size global minimum cost eas exposit assum rather termin ft ft anyboost simpli continu return ft subsequ time step theorem let lin convex cost function properti theorem let ft sequenc combin hypothes gener anyboost algorithm step size given assum weak hypothesi class negat close round mason baxter bartlett andm frean anyboost algorithm find function ft maxim vc ft ft accumul point sequenc ft satisfi supf vc inf experi adaboost perceiv resist overfit despit fact produc combin involv larg number classifi howev recent studi shown case even base classifi simpl decis stump overfit attribut use exponenti margin cost function recal tabl result show overfit may avoid use margin cost function form qualit similar tanh ayif xi adjust paramet control steep margin cost function tanh theoret analysi appli must convex combin base hypothes rather gener linear combin henceforth refer normal sigmoid cost function anyboost cost function inner product refer doom ii implement doom ii use fix small step size experi detail algorithm reader refer full version paper compar perform doom ii ariaboost select nine data set taken uci machin learn repositori variou level label nois appli simplifi matter binari classif problem consid experi axi orthogon hyperplan also known decis stump use weak learner full detail experiment setup may found summari experiment result shown figur improv test error exhibit doom ii ariaboost shown data set nois level doom ii gener outperform adaboost improv pronounc presenc label nois effect use normal sigmoid cost function rather exponenti cost function best illustr compar cumul margin distribut gener adaboost doom ii figur show comparison two data set label nois appli given margin valu curv correspond proport train exampl margin less equal valu curv show tri increas margin neg exampl adaboost will sacrific margin posit exampl significantli contrast doom ii give exampl larg neg margin order reduc valu cost function given adaboost suffer overfit guarante minim exponenti cost function margin cost function certainli relat test error valu propos cost function correl adaboost test error figur show variat normal sigmoid cost function exponenti cost function test error adaboost two uci data set round strong correl normal sigmoid cost adaboost test error data set minimum boost algorithm gradient descent sonar cleve ionospher vote credit breazt cancer data set nois nois ama md an hypol splice figur summari test error advantag standard error bar doom ii adaboost vari level nois nine uci data set breast cancer wisconsin nois dooivl ii nois adaboost nois doom ii margin splice ilcils adaboost nois doom ii noi ad oost gin figur margin distribut adaboost doom ii label nois breast cancer splice data set adaboost test error minimum normal sigmoid cost nearli coincid show sigmoid cost function predict ariaboost start overfit refer bartlett sampl complex pattern classif neural network size weight import size network ieee transact inform theori march breiman bag predictor machin learn breiman predict game arc algorithm technic report depart statist univers california berkeley keogh blake merz uci repositori machin learn databas http www ic uci edu mlearn mlrepositori html dietterich experiment comparison three method construct ensembl decis tree bag boost random technic report comput scienc depart oregon state univers mason baxter bartlett andm frean labor adaboost re tor exponenti cc st normal sigmoid cost round vote ariaboy st st exponenti cost norm dize sigmoid cost oo round figur adaboost test error exponenti cost normal sigmoid cost round adaboost labor votel data set cost scale case easier comparison test error drucker cort boost decis tree advanc neural inform process system page duffi helmbold geometr approach leverag weak learner comput learn theori dth european confer appear freund adapt version boost major algorithm proceed twelfth annual confer comput learn theori appear freund schapir experi new boost algorithm machin learn proceed thirteenth intern confer page freund schapir decis theoret gener line learn applic boost journal comput system scienc august friedman greedi function approxim gradient boost machin technic report stanford univers friedman hasti tibshirani addit logist regress statist view boost technic report stanford univers grove schuurman boost limit maxim margin learn ensembl proceed fifteenth nation confer artifici intellig page mason bartlett baxter improv gener explicit optim margin machin learn appear llew mason jonathan baxter peter bartlett marcu frean function gradient techniqu combin hypothes alex smola peter bartlett bernard schslkopf dale schurmann editor larg margin classifi mit press appear quinlan bag boost proceed thirteenth nation confer artifici intellig page itsch onoda iller soft margin adaboost technic report nc tr depart comput scienc royal holloway univers london egham uk schapir freund bartlett lee boost margin new explan effect vote method annal statist octob schapir singer improv boost algorithm use confid rate predict proceed eleventh annual confer comput learn theori page
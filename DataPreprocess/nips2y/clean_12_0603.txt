abstract present variat bayesian method model select famili kernel classifi like support vector machin gaussian process algorithm need user interact abl adapt larg number kernel paramet given data without sacrific train case valid open possibl use sophist famili kernel situat small standard kernel class clearli inappropri relat method work done gaussian process clarifi relat support vector machin certain gaussian process model introduct bayesian techniqu wide success use neural network statist commun appeal conceptu simplic gener consist solv learn problem paper present new method appli bayesian methodolog support vector machin briefli review gaussian process support vector classif section clarifi relationship point common root although focu classif straightforward appli method regress problem well section introduc algorithm show relat exist method final present experiment result section close discuss section let measur space ir xn tn xi ti noisi sampl latent function tli denot nois distribut given point wish predict minim error probabl difficult estim probabl generar bayesian method attack problem place stochast process prior space latent function seeger comput posterior predict distribut ly dy xi likelihood dli rli ti normal constant tlx obtain averag lx gaussian process gp spline smooth model use gaussian process prior seen function set random variabl finit correspond variabl jointli gaussian see introduct gp determin mean function posit definit covari kernel gaussian process classif gpc amount specifi avail prior knowledg choos class kernel vector hyperparamet hyperprior usual choic guid simpl attribut smooth trend differenti gener approach kernel design also consid class classif common nois distribut binomi one ty exp logist function logit log target distribut nois model integr analyt tractabl rang approxim techniqu base laplac approxim markov chain mont carlo variat method mean field algorithm known follow laplac approach gpc approxim posterior yld gaussian distribut argmaxp yld posterior mode vy logp evalu easi show predict distribut gaussian mean varianc lg covari matrix xi xj ij xi prime denot transposit final discrimin therefor linear combin xi discrimin approach predict problem choos loss function approxim misclassif loss ty search discrimin minim point interest see support vector classif svc use insensit loss svc loss ty ui upper bound misclassif loss reproduc kernel hilbert space rkh kernel hypothesi space inde support vector model laplac method gaussian process special case spline smooth model rkh aim minim function ti yi denot norm rkh shown minim written maxim ti yi ay ic fact found term depend log posterior gp framework choos log consid gp mean function follow denot indic function set bayesian model select support vector machin absorb svc loss transform dual problem via iccp vector dual variabl effici solv use quadrat program techniqu excel refer note svc loss cannot written neg log nois distribut cannot reduc svc special case gaussian process classif model although generar model svc given easier less problemat regard svc effici approxim proper gaussian process model variou model propos see work simpli normal svc loss pointwis use gaussian process model normal vc loss ty log exp exp note close approxim unnorm svc loss reader might miss svm bia paramet drop clariti straightforward appli semiparametr extens gp model variat method kernel classif real bayesian way deal hyperparamet averag lx posterior oid order obtain predict distribut lx approxim markov chain mont carlo method simpli argmaxp old latter approach call maximum posteriori map justifi limit larg often work well practic basic challeng map calcul evid fp dy exp ti yi dy plan attack variat approach let densiti model class chosen approxim posterior yld logp log yio dy ult call ep log epoog variat free energi second term well known kullback leibler diverg posterior nonneg equal zero iff yld almost everywher respect distribut thu upper bound logp chang decreas enlarg evid decreas diverg posterior approxim favour idea introduc ensembl learn success appli mlp latter work also introduc model class use name class gaussian mean factor analyz covari diagon posit element hinton athi random effect model improp prior work place flat improp prior bia paramet averag differ discrimin given ensembl although danger overfit use full covari would render optim difficult time memori consum seeger van camp use diagon covari would set choos small abl track import correl compon posterior use mn paramet repres agre criterion gradient respect paramet easili effici comput except gener term sum one dimension gaussian expect depend actual either analyt tractabl approxim use quadratur algorithm exampl expect normal svc loss decompos expect unnorm svc loss log see end section former comput analyt latter expect handl replac log piecewis defin tight bound integr solv analyt gpc loss cannot solv analyt experi approxim gaussian quadratur optim use nest loop algorithm follow inner loop run optim minim fix use conjug gradient optim sinc number paramet rather larg outer loop optim minim chose quasi newton method sinc dimens usual rather small gradient costli evalu use result minim two differ way natur discard plug origin architectur predict use mode approxim true posterior mode benefit kernel adapt given data particularli interest support vector machin due spars final kernel expans typic small fraction compon weight vector xp non zero correspond datapoint term support vector allow effici predict larg number test point howev also retain use gaussian approxim posterior use varianc approxim predict distribut ix deriv error bar predict although interpret figur somewhat complic case kernel discrimin like svm whose loss function correspond nois distribut relat method let us look altern way maxim loss twice differenti everywher progress made replac second order taylor expans around mode integrand known laplac approxim use maxim approxim howev techniqu cannot use nondifferenti loss insensit type nevertheless svc loss evid approxim laplacelik fashion interest compar result work approxim evalu effici continu nondifferenti cannot ignor sinc probabl one nonzero number sit exactli maxgin locat although continu accomplish modif see bayesian model select support vector machin difficult optim dimens small opper winther use mean field idea deriv approxim leav one test error estim quickli evalu suffer typic noisi cross valid score kwok appli evid framework support vector machin techniqu seem restrict kernel finit eigenfunct expans see detail interest compar variat method laplac method variat techniqu let differenti suppos given restrict approxim replac ti yi expans og ti yi ti yi ti pi yy oy fi posterior mean chang criterion fappro say easi show gaussian approxim posterior employ laplac method name fi diag minim fappro full covari use plug minim pp end evid approxim maxim laplac method latter variat techniqu sinc approxim loss function upper bound work differenti loss function upper bound loss function quadrat polynomi add variat paramet bound paramet method becom broadli similar lower bound algorithm inde sinc fix variat paramet polynomi easili solv mean covari former paramet essenti one howev quadrat upper bound poor function like svc loss case bound expect tighter experi test variat algorithm number dataset uci machin learn repositori delv archiv univers toronto leptograpsu crab pima indian diabet wisconsin breast cancer ringnorm twonorm waveform class descript may found web case normal whole set zero mean unit varianc input column pick train set random use rest test chose ir well known squar exponenti kernel see wd paramet constrain posit chose represent oi use prior see comment end section comparison train gaussian process classifi laplac method also without hyperprior support vector machin use fold cross valid select free paramet latter case constrain scale paramet wi equal infeas adapt hyperparamet data use crossvalid drop paramet allow bia paramet mention within variat method use posterior mode ssee http cs utoronto ca delv http ic uci edu mlearn lrepositori html seeger name train test var gp gp var svm svm lin size size lapl cv discr crab pima wdbc twonorm ringnorm waveform tabl number test error variou method well mean predict test method error bar comput baselin method linear discrimin train minim squar error tabl show test error differ method attain result show new algorithm perform equal well method consid cours regard combin much effort necessari produc took us almost whole day lot user interact cross valid model select rule thumb lot support vector upper bound indic larg paramet fail least two set start coars grid sweep sever stage refin effect known automat relev determin ard see nice observ dataset monitor length scale paramet wi inde variat svc algorithm almost complet ignor drive length scale small valu dimens crab pima waveform wdbc detect dimens particularli import regard separ harmoni gp laplac method thu sensibl parameter kernel famili togeth method bayesian kind allow us gain addit import inform dataset might use improv experiment design result experi method test hyperprior well detail analysi experi found discuss shown perform model select support vector machin use approxim bayesian variat techniqu method applic wide rang loss function abl adapt larg number hyperparamet given data allow use sophist kernel bayesian techniqu like automat relev determin see possibl use common model select criteria like cross valid sinc method fulli automat easi non expert use evid comput train set train data sacrif valid refer topic paper investig much greater detail press issu unfortun scale method train set asid open possibl compar svm fullyautomat method within delv project see section bayesian model select support vector machin size current current explorin applic power approxim might bring us much closer desir scale see also anoth interest issu would connect method work use generar model deriv kernel situat standard kernel applic reason acknowledg thank chri william amo storkey peter sollich carl rasmussen help inspir discuss work partial fund scholarship dr erich milllet foundat grate divis informat support visit edinburgh chri william make possibl refer david barber christoph bishop ensembl learn multi layer network advanc nip number page mit press mark gibb bayesian gaussian process regress classif phd thesi univers cambridg geoffrey hinton van camp keep neural network simpl minim descript length weight proceed th annual confer comput learn theori page tommi jaakkola marina meila toni jebara maximum entropi discrimin advanc nip number mit press tommi jaakkola david haussler exploit generar model discriminar classifi advanc nip number jame tin tau kwok integr evid framework support vector machin submit esann radford neal mont carlo implement gaussian process model bayesian classif regress technic report depart statist univers toronto januari manfr opper ole winther gp classif svm mean field result leav one estim advanc larg margin classifi mit press matthia seeger bayesian method support vector machin gaussian process master thesi univers karlsruh germani avail http www dai ed ac uk seeger john skill maximum entropi bayesian method cambridg univers press peter sollich probabilist method support vector machin advanc nip number mit press vladimir vapnik statist learn theori wiley grace wahba spline model observ data cbm nsf region confer seri siam grace wahba support vector machin reproduc kernel hilbert space random gacv technic report univers wisconsin christoph william predict gaussian process linear regress linear predict beyond jordan editor learn graphic model kluwer christoph william david barber bayesian classif gaussian process ieee tran pami rath run time essenti laplac method thu compar fastest known bayesian gp algorithm
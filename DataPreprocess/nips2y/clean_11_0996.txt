abstract paper address two issu long stand interest reinforc learn literatur first kind perform guarante made learn finit number action second quantit comparison made learn model base indirect approach use experi estim next state distribut line valu iter first show learn indirect approach enjoy rather rapid converg optim polici function number state transit observ particular order nlog log loglog transit suffici algorithm come within optim polici ideal model assum observ transit well mix throughout state mdp thu two approach roughli sampl complex perhap surprisingli sampl complex far less requir model base approach actual construct good approxim next state distribut result also show amount memori requir model base approach closer either approach remov assumpt observ transit well mix consid model transit determin fix arbitrari explor polici bound number transit requir order achiev desir level perform relat stationari distribut mix time polici introduct least two differ approach learn markov decis process indirect approach use control experi observ transit payoff estim model appli dynam program comput polici estim model direct approach learn use control converg rate learn indirect algorithm experi directli learn polici valu function without ever explicitli estim model known converg asymptot optim polici howev littl known perform two approach finit amount experi common argument offer propon direct method may requir much experi learn accur model simpli learn good polici argument predic seemingli reason assumpt indirect method must first learn accur model order comput good polici hand propon indirect method argu method unlimit line comput estim model may give advantag direct method least model accur learn good model may also use across task permit comput good polici multipl reward function date argument lack formal framework analysi verif paper provid framework use deriv first finit time converg rate sampl size bound learn standard indirect algorithm import aspect analysi separ qualiti polici gener experi qualiti two learn algorithm addit demonstr method enjoy rather rapid converg optim polici function amount control experi converg rate number specif perhap surpris implic hypothet differ two approach outlin implic well rate converg deriv briefli mention abstract interest breviti repeat instead proceed directli technic materi mdp basic let unknown state mdp action use ij denot probabl go state given state execut action denot reward receiv execut assum fix bound without loss gener polici assign action state valu state polici expect discount sum reward receiv upon start state execut forev rt reward receiv time step random walk govern start state discount factor also conveni defin valu state action pair ij goal learn approxim optim polici maxim valu everi state optim valu function denot given comput optim polici argmaxa given valu iter use comput good approxim optim valu function set initi guess qo iter follow qt ij defin max qt shown iter given approxim comput greedi approxim optim polici argmax kearn singh parallel sampl model reinforc learn transit probabl ij given good polici must learn basi observ experi transit classic converg result fc algorithm learn implicitli assum observ experi gener arbitrari explor polici proceed prove converg optim polici meet certain minim condit name must tri everi state action pair infinit often probabl approach conflar two distinct issu qualiti explor polici qualiti reinforc learn algorithm use experi gener rr contrast choos separ issu explor polici never rare visit state action pair would like reflect factor bound depend rr separ factor depend learn algorithm turn reflect effici particular learn algorithm use experi gener rr thu fix learn algorithm place equal foot directli compar probabl variou way separ accomplish introduc one particularli clean simpl would like model ideal explor polici one produc experi well mix sens everi state action pair tri equal frequenc thu let us defin parallel sampl subroutin ps behav follow singl call ps return everi state action pair random next state distribut accord ij thu everi state action pair execut simultan result next state report singl call ps therefor realli simul transit must care multipli number call ps factor wish count total number transit wit ps model model ideal explor polici manag visit everi state action pair success without duplic without fail intuit obviou explor polici would optim viewpoint gather experi everywher rapidli possibl shall first provid analysi section direct indirect reinforc learn algorithm set observ experi gener call ps cours given mdp may explor polici meet ideal captur ps instanc may simpli state difficult polici reach thu experi gener polici certainli equal mix around entir mdp inde call ps typic return set transit even correspond trajectori furthermor even ps could simul explor polici would like provid gener result express amount experi requir reinforc learn algorithm explor polici amount experi cours depend properti explor polici thu section sketch one bound amount experi requir order simul call ps detail provid longer version paper bound depend natur properti stationari distribut mix time combin result section get desir two factor bound discuss direct indirect approach bound total number transit requir consist one factor depend algorithm anoth factor depend explor polici converg rate learn indirect algorithm learn algorithm explicitli state two reinforc learn algorithm shall analyz compar keep separ algorithm explor polici alreadi discuss phrase algorithm parallel sampl framework section indic gener case arbitrari explor polici begin direct approach rather directli studi standard learn instead examin variant slightli easier analyz call phase learn howev emphas result gener appli standard learn learn rate number trial far basic rather updat valu function everi observ transit phase learn estim expect valu next state basi mani transit make updat memori requir phase learn essenti standard learn direct algorithm phase learn suggest name algorithm oper phase phase algorithm make mr call ps mr determin analysi thu gather mr trial everi state action pair fth phase algorithm updat estim valu function follow everi rn next state observ call ps eth phase polici comput algorithm gre edi polici determin final valu function note phase learn quit like standard learn except gather statist summat equat make updat proceed describ standard indirect approach indirect algorithm algorithm first make mi call ps obtain mi next state sampl build empir model transit probabl follow ij number time state reach mr trial algorithm valu iter describ section fix model ij ei phase polici comput algorithm greedi polici dictat final valu function thu phase learn algorithm run number phase phase requir call ps total number transit ez mr direct algorithm first make call ps run ei phase valu iter requir addit data total number transit mix question address larg must mi probabl least result polici expect return within optim polici answer give yield perhap surprisingli similar bound total number transit requir two approach parallel sampl model bound number transit state main result kearn singh theorem mdp appropri choic paramet mr total number call ps requir indirect algorithm order ensur probabl least expect return result polici within optim polici log loglog appropri choic paramet md total number call ps requir phase learn order ensur probabl least expect return result polici within optim polici log log loglog bound phase learn thu log larger indirect algorithm bound total number transit wit either case obtain multipli given bound sketch idea behind proof result first discuss implic debat direct versu indirect approach first approach converg rather fast total number transit order log fix simplic near optim polici obtain repres consider advanc classic asymptot result instead say infinit number visit everi state action pair requir converg optim polici claim rather small number visit requir get close optim polici second analysi two approach similar complex number transit requir differ log factor favor indirect algorithm third perhap surprisingli note sinc log call made ps fix sinc number trial per state action pair exactli number call ps total number non zero entri model ij built indirect approach fact log word ij extrem spars thu terribl approxim true transit probabl yet still good enough deriv near optim polici clever represent ij thu result total memori requir log rather fourth although space provid detail instead singl reward function provid reward function reward function given ad vanc observ experi algorithm number transit requir comput near optim polici reward function simultan factor log greater bound given view result implic algorithm enjoy rapid converg optim polici function amount experi gener neither approach enjoy signific advantag converg rate memori requir handl multipl reward function quit effici count space provid detail proof theorem instead provid highlight main idea proof indirect algorithm phase learn actual quit similar heart two slightli converg rate learn indirect algorithm differ uniform converg lemma phase learn possibl show bound number phase execut choos md red ri hold simultan everi everi phase word end everi phase empir estim expect next state valu everi close true expect expect respect current estim valu function indirect algorithm slightli subtl uniform converg argument requir show possibl choos bound number iter valu iter execut ij valu mr everi everi phase valu function result perform true valu iter ij equat essenti say expect true valu function quit similar either true estim model even though indirect algorithm never access true valu function either case uniform converg result allow us argu correspond algorithm still achiev success contract classic proof valu iter instanc case phase learn defin max qt deriv recurr relat follow qe max pi made use equat sinc ao qo qo recurr give hard show shown regret expect return suffer polici comput phase learn phase proof proce set regret smaller desir solv obtain result bound mr deriv bound indirect algorithm similar handl gener explor polici promis conclud technic result briefli sketch translat bound obtain section ideal parallel sampl model kearn singh bound applic fix polici guid explor bound must cours depend properti due space limit outlin main idea formal statement proof defer longer version paper let us assum simplic may stochast polici defin er odic markov process mdp thu induc uniqu stationari distribut pm state action pair intuit pm frequenc execut action state infinit random walk accord furthermor introduc standard notion mixin time stationari distribut inform number step requir distribut induc state action pair step walk accord close pm final let us defin min pm arm notion difficult show number step must take order simul high probabl call oracl ps polynomi quantiti intuit straightforward everi step obtain almost independ draw pm independ draw least probabl draw particular pair sampl everi pair simul call ps formal intuit lead version theorem applic bound multipli factor polynomi desir howev better result possibl case may small even would occur simpli ever execut action state factor larg infinit bound becom weak vacuou case better defin sub mdp obtain simpli delet pm paramet choos construct may obtain converg rate optim polici learn indirect approach like given theorem multipli factor polynomi technic must slightli alter algorithm initi phase detect elimin small probabl state action pair minor detail allow becom smaller amount experi receiv grow obtain anytim result sinc sub mdp approach full mdp refer jaakkola jordan singh converg stochast iter dynam program algorithm neural comput watkin learn delay reward ph thesi cambridg univers sutton barto reinforc learn introductior mit press mahadevan enhanc transfer reinforc learn build stochast model robot action machin learn proceed ninth intern confer formal degre close measur distanc transient stationari distribut breviti simpli assum paramet set small constant valu
abstract discuss applic tap mean field method known statist mechan disord system bayesian classif model gaussian process contrast previou approach knowledg distribut input need simul result sonar data set given introduct bayesian model base gaussian prior distribut function space promis non parametr statist tool recent introduc neural comput commun neal william rasmussen mackay give basic definit assum likelihood output target variabl given input written form rlh priori assum gaussian random field assum field zero prior mean statist entir defin second order correl denot expect opper qnther respect prior interest exampl st arcsin eiwi isti wi si exp choic motiv limit two layer neural network infinit mani hidden unit factoriz input hidden weight prior william wi hyperparamet determin relev prior lengthscal simplest choic wisi correspond singl layer perceptron independ gaussian weight prior bayesian framework one make predict novel input receiv set dm train exampl su use posterior distribut field test point given condit gaussian distribut posterior distribut field variabl train point normal partit function ht der prior distribut field train point introduc abbrevi major technic problem approach come difficulti perform high dimension integr non gaussian likelihood treat approxim mont carlo sampl neal laplac integr barber william bound likelihood gibb mackay use far paper introduc approach base mean field method known statist physic disord system zard parisi virasoro special case binari classif problem binari class label must predict use train set corrupt label nois likelihood problem taken probabl true classif label corrupt flip step function defin otherwis case expect non smooth model laplac method bound introduc gibb mackay directli applic mean field method classif gaussian process exact posterior averag order make predict input ideal label maximum posterior probabl chosen bay argmaxr ridm predict probabl given rldm dhp rlh hldm binari case bay classifi becom bay sign signh throughout paper let bracket denot posterior averag use somewhat simpler approach use predict sign would reduc ideal predict posterior distribut symmetr around mean goal mean field approach provid set equat approxim determin start point analysi partit function dx dh new auxiliari variabl integr along imaginari axi introduc order get rid hard show posterior averag field train input new test point given thu reduc problem calcul microscop orderparamet averag statist physic calcul deriv respect small extern field set zero equival formul use legendr transform function expect case given xu xu xu au xu ri addit averag xu introduc dynam variabl unlik ise spin fix length extern field must elimin true expect valu oa xu satisfi og og naiv mean field theori far descript give anyth new usual cannot calcul exactli non gaussian likelihood model interest nevertheless base mean field theori mft possibl guess approxim form although integr imaginari axi expect come posit due fact integr measur complex well opper winther mean field method found interest applic neural comput within framework ensembl learn exact posterior distribut approxim simpler one use product distribut variat treatment standard mean field method posterior case gaussian process classif prepar discuss somewher els paper suggest differ rout introduc nontrivi correct simpl naiv mft variabl besid variat method would pure formal distribut complex defin probabl way defin simpl mft truncat perturb expans respect interact cu first order plefka approach yield result gnaiv go tt go contribut model without interact legendr transform zo tt error function simpl model statist physic interact posit equal easi show aiv becom exact limit infinit number variabl henc system larg number nonzero interact order magnitud one may expect approxim bad tap approach nevertheless interact cu posit neg one would expect input zero mean even thermodynam limit nice distribut input addit contribut ag must ad naiv mean field theori correct often call onsag reaction term introduc spin glass model thouless anderson palmer tap later appli statist mechan singl layer perceptron zard gener bayesian framework opper winther applic multilay network see wong thermodynam limit infinit larg dimens input space nice input distribut result shown coincid result replica framework drawback previou deriv tap mft neural network fact special assumpt input distribut made certain fluctuat term replac averag distribut random data practic would avail paper use approach parisi potter allow circumv problem conclud appli case spin model random interact specif type function form ag depend type singl particl contribut go henc one may use model go calcul exactli gaussian regress model subtract naiv mean field contribut obtain mean fieldmethod classif gaussian process desir ag sake simplic chosen even simpler model rulh without chang final result lengthi straightforward calcul problem lead result ag lndet zln rt must elimin use og lead equat note choic tap mean field theori becom exact gaussian likelihood standard regress problem final set deriv gtap gnaiv kg respect variabl xu xg au equal zero obtain equat vr gaussian measur eq solv numer togeth contrast naiv mft simpler result au cuu found simul solv nonlinear system equat iter turn quit straightforward data set get converg one add diagon term covari matrix cij cij sijlj may shown term orrespond learn gaussian nois varianc ad gaussian random field present simul result singl data set sonar mine versu rock use train test set split origin studi gorman sejnowski input data pre process linear rescal train set input variabl zero mean unit varianc case mean field equat fail converg use raw data import featur tap mft fact method also give approxim leav one estim gener error oo express term solut mean field equat see opper winther detail also possibl deriv leav one estim naiv mft opper winther publish sinc far dealt problem automat estim hyperparamet number drastic reduc set wi covari remain hyperparamet chosen opper qnther tabl result sonar data exact algorithm covari function crest cloo oo tap mean field naiv mean field back prop simpl perceptron best layer hidden minim ioo turn lowest ioo found model without nois simul result shown tabl comparison back propag taken gorman sejnowski solut found algorithm turn uniqu differ order present exampl differ initi valu xu converg solut tabl also compar estim given algorithm exact leav one estim exact obtain go train set cloo keep exampl test run mean field algorithm rest estim exact valu complet agreement compar test error see train set hard test set easi small differ test error naiv full mean field algorithm also indic mean field scheme quit robust respect choic discuss work done make tap approach practic tool bayesian model one find better method solv equat convers direct minim problem free energi mayb help achiev one may probabl work real field variabl instead imaginari problem determin hyperparamet covari function two way seem interest one may use approxim free energi essenti neg logarithm bayesian evid estim probabl valu hyperparamet howev estim error made tap approach would necessari second one may use built leav one estim estim gener error estim valid approxim necessari interest appli way deriv tap equat model boltzmann machin belief net combinatori optim roblem standard mean field theori appli success acknowledg research support swedish foundat strateg research danish research council natur technic scienc danish comput neural network center connect mean fiem method classif gaussian process refer barber william gaussian process bayesian classif via hybrid mont carlo neural inform process system mozer jordan petsch ed mit press gibb mackay variat gaussian process classifi preprint cambridg univers gorman sejnowski analysi hidden unit layer network train classifi sonar target neural network mackay gaussian process replac neural network nip tutori may obtain http wol ra phi cam ac uk pub mackay zard space interact neural network gardner comput caviti method phi zard parisi virasoro spin glass theori beyond lectur note physic world scientif neal bayesian learn neural network lectur note statist springer neal mont carlo implement gaussian process model bayesian regress classif technic report crg tr dept comput scienc univers toronto opper winther mean field approach bay learn feedforward neural network phi rev lett opper winther mean field algorithm bay learn larg feed forward neural network neural inform process system mozer jordan petsch ed mit press parisi potter mean field equat spin model orthogon interact matric phi math gen plefka converg condit tap equat infinit rang ise spin glass phi thouless anderson palmer solut solvabl model spin glass phil mag william comput infinit network neural inform process system mozer jordan petsch ed mit press william rasmussen gaussian process regress neural inform process system touretzki mozer hasselmo ed mit press wong microscop equat stabil condit optim neural network europhi lett
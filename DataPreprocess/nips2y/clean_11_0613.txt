abstract gaussian process regress covari output input locat usual assum depend distanc posit definit matrix often taken diagon allow gener posit definit matrix tune basi train data eigen analysi show effect creat hidden featur dimension hidden featur space determin data demonstr superior predict use gener matrix base diagon matrix two test problem introduct last year bayesian approach predict neural network come fore follow argument neal concern equival infinit neural network certain gaussian process gaussian process gp predict also becom popular rasmussen demonstr good perform gp predictor number task gaussian process predict appli rasmussen william rasmussen other covari output locat usual assum depend distanc tw posit definit diagon matrix mean differ dimens input space differ relev predict problem mackay neal idea automat relev determin neal howev reason success neural network method project pursuit regress suggest discov relev direct featur space import clearli ard model special case direct varelli william parallel axe input featur space paper allow gener posit semidefinit matrix defin mahalanobi distanc input space therebi allow gener direct input space select compar perform gp predictor use diagon full distanc matric regress problem structur paper follow gp regress introduc section also explain rsle play distanc matrix criterion use compar generalis perform diagon gener distanc matric two method compar two regress task result experi shown section summari work done open question present section gaussian process predict paper use gaussian process model predictor consid stochast process input observ belong input space gaussian process subset stochast process defin specifi mean covari function cp respect work shall set although gp formul provid prior function purpos suffic note valu xl correspond xvalu multivari gaussian distribut kp kp ij cp xi xj specif form covari function shall use exp diagon matrix entri wii invers squar correl length scale process along direct particular note model close relat automat relev determin method mackay neal neal small lengthscal along certain direct space highlight relev correspond input featur assum input normalis predict problem let us suppos data point xl tl output valu correspond input assum gener true valu ad given assumpt gaussian process prior gaussian nois varianc function standard result whittl predict distribut correspond new input af mean varianc cy cp cy lk kp cp cp cp method predict assum process model realli function observ howev often case real world problem actual function set hidden featur aris combin manifest variabl particular wish studi problem hidden featur linear combin observ discov hidden featur gaussian process regress coordin matrix mx case covari function specifi equat turn depend upon estim distanc hidden featur sincez mx andw mt gp model depend paramet describ covari function element train gp carri crv either estim paramet covari function exampl use maximum likelihood method use bayesian approach sampl posterior distribut paramet william rasmussen follow first approach maximis logarithm likelihood logp log det ttk log depend upon vector paramet covari function number free paramet depend number non zero element matrix usual chosen diagon number free paramet cri notic parametris diagon element ap allow discoveri relev direct observ space lead estim gener map onto featur space relev direct parallel axe input manifest space known advanc prefer use gener symmetr posit semidefinit matrix parametris matrix follow choleski decomposit utu upper triangular matrix posit entri diagon william henc factoris turn exp ul ul exp ua exp element diagon posit exponenti symmetr independ entri thu total number free paramet gp model note full distanc matrix iv allow estim matrix eigenvalu decomposit vav diagon matrix eigenvalu matrix eigenvector dimens hidden featur space infer number relev eigenvalu matrix invers squar correl length process along direct hidden space direct hidden featur space defin eigenvector correspond relev eigenvalu particular matrix compos eigenvector give estim map follow diagon gener full correl matric design wd import observ predictor obtain use wf equival addit model hasti tibshirani predictor multivari function rather addit function compon howev would possibl produc addit function gp context use covari function sum one dimension covari function base project varelli william generalis error consid predict valu function predictor commonlyus measur generalis error given dataset dn averag squar error dx averag generalis error dataset size obtain averag choic train dataset eg eg sometim evalu analyt numer integr usual necessari use sampl perform averag train dataset order investig generalis capabl gp use diagon full distanc matric wd wf train gp predictor regress task generalis error compar look rel error generalis error report use diagon full distanc matrix respect ratio allow us perform fair comparison pairwis differ generalis error dataset actual valu expect valu averag sampl train data result conduct experi compar generalis capabl gp predictor full diagon distanc matric section illustr result obtain train gp two regress task regress trigonometr function section regress high interact surfac section regress trigonometr function first experi gp train observ drawn function sin rz corrupt gaussian nois mean zero varianc hidden featur gener observ variabl lt transform mtx iv li wish infer process actual function one dimension featur use gp manifest space evalu expect generalis error equat gaussian quadratur press et al estim expect rel error averag differ sampl train set paramet covari function optimis train dataset maximis likelihood see equat conjug gradient algorithm press et al wd wf iter largest train set data figur report valu vertic axi function amount train data axi varianc nois set figur figur discov hidden featur gaussian process regress figur figur report axi graph see equat function amount train data axi nois level set figur figur error bar gener minimum maximum valu occur train dataset plot show use wf significantli improv generalis perform respect diagon matrix rel error lie well zero within confid interv particularli highlight figur dataset larger data larger notic small dataset close zero distribut valu spread around zero wide confid interv due fact small amount data possibl train gp properli particular number free paramet wf larger wd former need larger dataset train latter order avoid overfit fulli bayesian treatment train gp see section would serious affect problem sinc predict gp would marginalis posterior distribut paramet larg dataset rel error declin reach maximum valu agre intuit larg amount data method becom good predictor similar remark although notic rel appli also figur rrv error assum lower valu due higher nois varianc better perrom wi respect wd explain eigenanalysi two distanc matric sinc one eigenvalu wi much larger vs full rank distanc matrix abl discov relev true dimens process eigenvector correspond larger eigenvalu repres oper map space observ onto hidden featur space wa fail find effect dimens problem characteris two eigenvalu similar magnitud high interact surfac also test method exampl taken breiman concern regress problem surfac high dimension space target function rr rr cr rr sigmoid function exp exp hidden featur zl deriv transform zi li li normalis inner product tx observ variabl uniformli distribut varelli william figur figur report axi graph ofp see equat function amount train data axi error bar gener minimum maximum valu occur train dataset figur show graph ten eigenvalu wd wf distanc matric obtain use one train set data lower valu reach train set data respect three vector mi valu true function also corrupt gaussian nois mean zero varianc nois ratio standard deviat signal nois breiman run experi train gp diagon full distanc matric data set size work breiman use train set datapoint gp paramet optimis train dataset maximis likelihood see equat conjug gradient algorithm press et al generalis error wf estim use test data point rel generalis error equat shown figur observ dataset size use wf significantli reduc rel error respect diagon matrix model train smaller train set good generalis perform larger number paramet overfit data eigenvalu decomposit distanc matric show abl discov underli structur process figur display eigenvalu optimis one train set data characteris three larg eigenvalu whose eigenvector indic three main direct featur space thu full matrix abl find three ten direct respons variat function convers wd fail discov hidden featur data sinc eigenvalu almost magnitud input dimens observ variabl equal relev train gp eigenvector wf defin basi space gener subspac featur order verifi subspac span actual overlap hidden featur space tri express former set vector linear combin latter thu comput singular valu press et al matrix compos normalis vector mi basi discov hidden featur gaussian process regress three six singular valu neglig respect other vs origin hidden transform well approxim linear combin new basi eigenvector show eigenspac wf good approxim hidden featur space discuss paper shown discov hidden featur gp regress also note techniqu could appli problem gaussian process predictor use classif problem attract featur method allow appropri dimension space discov wish restrict maximum dimension one could use distanc matrix rank idea allow gener transform input space mention literatur exampl girosi et al howev girosi et al suggest set paramet wf cross valid believ practic high dimension space result obtain show use full distanc matrix reduc significantli rel error respect use diagon distanc matrix train gp carri maximis logarithm likelihood effect particularli evid larger amount data use problem reduc full bayesian approach gp regress use current investig input dimension affect gp regress gener distanc matrix fix dimension acknowledg research form part valid verif neural network system project fund jointli epsrc gr british aerospac thank dr andi wright bae help discuss refer breiman hing hyperplan regress classif function approxim ieee tran inform theori girosi jone poggio regular theori neural network architectur neural comput hasti tibshirani gener addit model chapman hall london neal bayesian learn neural network springer lectur note statist press teukolski vetterl flanneri numer recip art scientif comput cambridg univers press second edit rasmussen evalu gaussian process method nonlinear regress phd thesi dept comput scienc univers toronto whittl predict regul linear least squar method english univers press william rasmussen gaussian process regress touretzki mozer hasselmo editor advanc neural inform process system page mit press william condit multivari densiti neural comput
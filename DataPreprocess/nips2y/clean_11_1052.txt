abstract articl propos new reinforc learn rl method base actor critic architectur actor critic approxim normal gaussian network ngnet network local linear regress unit ngnet train line em algorithm propos previou paper appli rl method task swing stabil singl pendulum task balanc doubl pendulum near upright posit experiment result show rl method appli optim control problem continu state action space method achiev good control small number trial error introduct reinforc learn rl method barto et al success appli variou markov decis problem finit state action space backgammon game tesauro complex task dynam environ lin hand applic continu state action problem werbo doya sofg white much difficult finit state action case good function approxim method fast learn algorithm crucial success applic articl propos new rl method mention two featur method base actor critic architectur barto et al although detail implement actor critic quit differreinforc learn base line em algorithm ent origin actor critic model actor critic method estim polici function respect approxim normal gaussian network ngnet moodi darken ngnet network local linear regress unit model softli partit input space use normal gaussian function local unit linearli approxim output within partit point sutton local model ngnet suitabl global model multi layer perceptron avoid seriou learn interfer line rl process ngnet train line em algorithm propos previou paper sato ishii shown line em algorithm faster gradient descent algorithm line em algorithm posit local unit adjust accord input output data distribut moreov unit creation unit delet perform accord data distribut therefor model adapt dynam environ input output data distribut chang time sato ishii appli new rl method optim control problem determinist nonlinear dynam system first experi task swing stabil singl pendulum limit torqu doya second experi task balanc doubl pendulum torqu appli first pendulum rl method base line em algorithm demonstr good perform experi ngnet line em algorithm section review line em algorithm ngnet propos previou paper sato ishii ngnet moodi darken transform dimension input vector dimension output vector defin follow equat gi denot number unit prime denot transpos gi dimension gaussian function dimension center dimension covari matrix lvi bi dimension linear regress matrix dimension bia vector respect subsequ use notat ngnet interpret stochast model pair input output stochast event event unit index assum select regard hidden variabl stochast model defin probabl distribut triplet call cmnplete event ilo exp rr set model paramet easili prove expect valu output given input ylx sato lshii yp ylx dy ident equat name probabl distribut provid stochast model ngnet set event observ data model paramet stochast model determin maximum likelihood estim method particular em algorithm dempster et al em algorithm repeat follow step estim step let present estim use posterior probabl th unit select given ilx il jlo maximizatipn step use posterior probabl expect loglikelihood oio complet event defin ilx logp ilo sinc increas impli increas log likelihood observ data dempster et al oio maxim respect solut necess condit ol given xu et al sa denot weight mean respect posterior probabl defin ilx em algorithm introduc base batch learn xu et al name paramet updat see observ data introduc line version sato ishii em algorithm let estim th observ data line em algorithm weight mean replac rl ilx paramet discount factor introduc forget effect earlier inaccur estim normal coeffici iter calcul rl modifi weight mean obtain step wise equat rl pi reinforc learn base line em algorithm pi ilx use modifi weight mean new paramet obtain follow equat pi pi tr obtain follow relat fki es prove line em algorithm equival stochast approxim find maximum likelihood estim time cours discount factor given constant sato ishii also employ dynam unit manipul mechan order effici alloc unit sato ishii probabl ilo indic probabl th unit produc datum present paramet probabl everi unit less threshold valu new unit produc account new datum weight mean indic much th unit use account data mean becom less threshold valu unit delet order deal singular input distribut regular introduc follow xx lfi ai ix dimension ident matrix small constant correspond calcul line manner use similar equat sato ishii reinforc learn section propos new rl method base line em algorithm describ previou section follow consid optim control problem determinist nonlinear dynam system continu state action space assum knowledg control system actor critic architectur barto et use learn system origin actor critic model actor critic approxim probabl action valu function respect train use td error actor critic rl method differ origin model explain later sato ishii current state xc control system actor output control signal action given polici function xc control system chang state xc receiv control signal subsequ reward xc given learn system object learn system find optim polici function maxim discount futur return defin xc tr xc xc xc xc discount factor xc call valu function defin current polici function employ actor function defin xc xc xc xc xc assum valu function obtain function xc function satisfi consist condit xc rl method polici function function approxim ngnet call actor network critic network respect learn phase stochast actor necessari order explor better polici purpos employ stochast model defin correspond actor network stochast action gener follow way unit index select randomli accord condit probabl ilx given state subsequ action gener randomli accord condit probabl ulxc given select valu function defin either stochast polici determinist polici sinc control system determinist use valu function defin determinist polici given actor network learn process proce follow current state xc stochast action gener stochast model correspond current actornetwork next time step learn system get next state xc reward xc critic network train line em algorithm input critic network target output given right hand side function determinist polici function calcul use current critic network current actor network respect actor network also train line em algorithm input actor network xc target output given use gradient critic network sofg white utahget xc xe function determinist polici function calcul use modifi critic network current actor network respect small constant target output give better action increas function valu current state current determinist action learn scheme critic network actor network updat concurr one consid anoth learn scheme scheme learn system tri control control system given period time use fix actor network period critic network train estim reinforc learn base line em algorithm function fix actor network state trajectori period save next stage actor network train along save trajectori use critic network modifi first stage experi first experi task swing stabil singl pendulum limit torqu doya state pendulum repres xc ds denot angl upright posit angular veloc pendulum respect reward xc assum given exp vl constant reward encourag pendulum stay high releas pendulum vicin upright posit control learn process actor critic network conduct second singl episod reinforc learn done repeat episod episod system abl make pendulum achiev upright posit almost everi initi state even low initi posit system swing pendulum sever time stabil upright posit figur show control process stroboscop time seri pendulum use determinist polici train accord previou experi actor critic network ngnet fix center train gradient descent algorithm good control obtain episod therefor new rl method abl obtain good control much faster base gradient descent algorithm second experi task balanc doubl pendulum near upright posit torqu appli first pendulum state pendulum repres first pendulum angl upright direct second pendulum angl first pendulum direct respect angular veloc first second pendulum reward given height second pendulum end lowest posit episod system abl stabil doubl pendulum figur show control process use determinist polici train upper two figur show stroboscop time seri pendulum dash dot solid line bottom figur denot rr control signal produc actor network respect transient period pendulum success control stay near upright posit number unit actor critic network train singl doubl pendulum case respect rl method use center fix ngnet train gradient descent algorithm employ actor unit critic unit singl pendulum task doubl pendulum task scheme work even actor unit critic unit prepar number unit ngnet train line em algorithm scale moder input dimens increas conclus articl propos new rl method base line em algorithm show rl method appli task swing sato ishii stabil singl pendulum task balanc doubl pendulum near upright posit number trial error need achiev good control found small two task order appli rl method continu state action problem good function approxim method fast learn algorithm crucial experiment result show rl method featur refer barto sutton anderson ieee transact system man cybernet barto sutton watkin learn comput neurosci foundat adapt network pp mit press dempster laird rubin journal royal statist societi doya advanc neural inform process system pp mit press lin machin learn moodi darken neural comput sato ishii atr technic report tr atr sofg white handbook intellig control pp van nostrand reinhold sutton advanc neural inform process system pp mit press tesauro machin learn werbo neural network control pp mit press xu jordan hinton advanc neural inform process system pp mit press time sequenc invert pendulum time sec time sec figur figur
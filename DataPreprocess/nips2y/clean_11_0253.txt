abstract follow recent result show import fatshatt dimens explain benefici effect larg margin gener perform current paper investig implic result case imbalanc dataset develop two approach set threshold approach incorpor thetaboost boost algorithm deal unequ loss function perform thetaboost two approach test experiment keyword comput learn theori gener fat shatter larg margin pac estim unequ loss imbalanc dataset introduct shaw taylor demonstr output margin also use estim confid particular classif made word new exampl output valu well clear threshold confid associ classif output valu closer threshold current paper appli result case differ loss associ fals posit fals neg signific number data point misclassifi use criterion minimis empir loss howev data correctli classifi empir loss zero correctli separ hyperplan case approach provid insight choos hyperplan threshold summari paper suggest way hyperplan optimis irabalanc dataset loss associ misclassifi less preval class higher karakoula shaw taylor background analysi definit let set real valu function say set point shatter real number rx index binari vector index function realis dichotomi margin fat shatter dimens fati set function posit real number integ map valu size largest shatter set finit infin otherwis gener concern classif obtain threshold real valu function classif valu instead usual order simplifi express henc typic consid set function map input space real sake simplifi present result assum threshold use classif result extend threshold without difficulti henc implicitli use classif function function threshold say margin train set xi yi rn minl yif xi note posit margin impli consist definit given real valu function use classif threshold probabl distribut use erp denot follow probabl erp yf suppos ri use erp flr denot probabl erp fl yf probabl erp flr probabl misclassif randomli chosen exampl given margin consid follow restrict set real valu function definit stant real valu function class close addit connot linear function threshold weight use perceptron satisfi properti neural network linear output unit henc properti appli support vector machin neural network exampl quot result theorem let class real valu function close addit constant fat shatter dimens bound fat continu right probabl least choic random rn sampl xi yi drawn accord follow hold suppos yif xi ri xi yi sampl yif xi ri dln rn log ern ln rn let fat probabl new exampl margin rl misclassifi bound dlog og em log optim classif irabalanc train set unequ loss function consid situat loss associ exampl differ misclassif posit neg exampl let lb loss associ classif function exampl analysi consid loss function taken lh ih yl point misclassifi otherwis also known discret loss paper consid differ loss function classif function definit toss function defin lb otherwis first consid classic approach minim empir loss loss train set sinc loss function longer binari standard theoret result appli much weaker binari case algorithm implic howev investig assumpt use hyperplan parallel maxim margin hyperplan empir risk given er im mfi xi yi train set xi yi rn assum train set correctli classifi hypothesi class criterion abl distinguish consist hypothes henc give reason choos standard maxim margin choic howev natur way introduc differ loss maxim margin quadrat program procedur constraint given specifi yi zi rn order forc hyperplan away posit point incur greater loss natur heurist set yi neg exampl yi posit point henc make decis boundari case consist classif possibl effect move hyperplan parallel margin posit side time neg side henc solv problem simpli use standard maxim margin algorithm replac threshold one closest posit neg point altern approach wish employ consid movement hyperplan parallel retain consist let margin maxim margin hyperplan consid consist hyperplan margin posit exampl neg exampl basic analyt tool theorem appli posit exampl neg exampl note classif set theorem let ho maxim margin hyperplan margin set probabl least choic random rn sampl ci yi drawn accord follow hold suppos ho yiho xi rn dln rn log em ln let fat fat bound expect loss max log log em log karakoula shaw taylor proof use theorem bound probabl error given correct classif posit term express fat shatter dimens neg exampl bound probabl error term express fat shatter dimens henc expect loss bound take maximum second bound place togeth factor front second log term first bound multipli fl bound obtain suggest way optimis choic name minimis express fat shatter dimens linear function solv term fl give choic gener agre suggest choic threshold previou section later section report initi experi investig perform differ choic thetaboost algorithm idea adjust margin case unequ loss function also appli adaboost algorithm shown maximis margin train exampl henc gener bound term margin fat shatter dimens function produc algorithm first develop boost algorithm unequ loss function extend adjust margin specif assum set train exampl xi ii weak learner output hypothes iii unequ loss function definit assign initi weight posit exampl wto neg exampl valu set adjust use valid set gener adaboost case unequ loss function given adauboost algorithm figur adapt theorem algorithm theorem assum notat algorithm figur follow bound hold train error xi yi xi yi jt choic forc uneven probabl misclassif train set ensur weak learner concentr misclassifi posit exampl defin suppress subscript exp fiiyih xi thu minim train error seek minim respect vote coeffici iter boost follow introduc notat optim classif irabalanc train set equat zero first deriv respect use exp exp exp exp let exp get polynomi root polynomi found numer sinc one zero give uniqu minimum solut use take distanc train exampl standard threshold iter adauboost algorithm figur well combin weak learner thetaboost algorithm search posit neg support vector sv point hyperplan separ largest margin sv point found appli formula section respect comput valu adjust threshold see figur complet algorithm algorithm adauboost initi dl describ train weak learner use distribut dr get weak hypothesi hi choos updat dt dt exp lih zi zt yi otherwis zt normal factor output final hypothesi sgn algorithm thetaboost adauboost remov train dataset fals posit borderlin point find smallest mark remov neg point valu greater find first neg point next rank mark sv comput margin sum distanc standard threshold check candid near current one chang margin least use comput theta threshold eqn output final hypothesi sgn tr atht figur adauboost theta boost algorithm karakoula shaw taylor experi purpos experi report section two fold compar gener perform adauboost standard adaboost imbalanc dataset ii examin two formula choos threshold thetaboost evalu effect gener perform evalu ii use two perform measur averag geometr mean accuraci mean latter defin precis recal posit correct posit correct precis posit predict recal true posit mean recent propos perform measur contrast accuraci captur specif trade fals posit true posit imbalanc dataset also independ distribut exampl class initi experi use satimag dataset uci repositori use uniform dataset classifi neigborhood pixel satelit imag continu attribut class pick class goal class sinc less preval one dataset dataset come train exampl test exampl set tabl show perform test set adauboost adaboost differ valu beta paramet point latter two algorithm minim total error assum equal loss function case equal loss adauboost simpli reduc adaboost observ tabl higher loss paramet bigger improv adauboost two algorithm particularli appar valu mean adauboost adaboost valu avgloss mean avgloss mean avgloss mean tabl gener perform satimag dataset figur show gener perform thetaboost term averag loss differ valu threshold latter rang largest margin neg exampl correspond smallest margin posit exampl correspond rang includ valu given formula experi set depict figur margin defin achiev better gener perform margin defin particular closer valu give minimum loss test set addit thetaboost perform better adauboost test set emphasis howev differ signific extens experi requir two approach rank reliabl optim classif irabalanc train set emo threshold figur averag loss test set function discuss built theoret framework optim set margin given unequ loss function appli framework boost develop adauboost thetaboost gener adaboost well known boost algorithm take account unequ loss function adjust margin imbalanc dataset initi experi shown factor improv gener perform boost classifi refer corinna cort vladimir vapnik machin learn yoav freund robert schapir page proceed intern confer machin learn icml michael kearn robert schapir page proceed ist symposium foundat comput scienc foc kubat holt matwin machin learn metz murphi uci repositori machin learn databas http www ic uci edu mlearn mlrepositori html schapir freund bartlett sun lee page proceed intern confer machin learn icml robert schapir yoram singer proceed eleventh annual confer comput learn theori colt john shaw taylor algorithmica john shaw taylor peter bartlett robert williamson martin anthoni ieee tran inf theori
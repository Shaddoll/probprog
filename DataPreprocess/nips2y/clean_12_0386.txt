abstract present new techniqu time seri analysi base dynam probabilist network approach observ data model term unobserv mutual independ factor recent introduc techniqu independ factor analysi ifa howev unlik ifa factor factor tempor statist characterist deriv famili em algorithm learn structur underli factor relat data algorithm perform sourc separ nois reduct integr manner demonstr superior perform compar ifa introduct techniqu independ factor analysi ifa introduc provid tool model dim data term unobserv factor factor mutual independ combin linearli ad nois produc observ data mathemat model defin yt hxt ut xt vector factor activ time yt data vector mix matrix ut nois origin ifa lie appli statist one hand signal process hand statist ancestor ordinari factor analysi fa assum gaussian factor contrast ifa allow factor arbitrari distribut model semi parametr dim mixtur gaussian mog mog paramet well mix matrix nois covari matrix learn observ data expect maxim em algorithm deriv signal process ancestor ifa independ compon analysi ica method blind sourc separ ica factor term sourc task blind sourc separ recov observ data knowledg mix process sourc ica non gaussian distribut unlik ifa distribut usual fix prior knowledg quit limit adapt signific restrict dynam independ factor analysi number set data dimension squar mix mix matrix assum invert data assum nois free ut contrast ifa allow includ sourc sensor well non zero nois unknown covari addit use flexibl mog model often prove crucial achiev success separ therefor ifa gener unifi fa ica tl model learn use classif fit ifa model class complet miss data context blind separ optim reconstruct sourc xt data obtain use map estim howev ifa ancestor suffer follow shortcom oblivi tempor inform sinc attempt model tempor statist data see squar nois free mix word model learn would affect permut time indic yt unfortun sinc model data time seri would facilit filter forecast well accur classif moreov sourc separ applic learn tempor statist would provid addit inform sourc lead cleaner sourc reconstruct see one may think problem blind separ noisi data term two compon sourc separ nois reduct possibl approach might follow two stage procedur first perform nois reduct use wiener filter second perform sourc separ clean data use ica algorithm notic procedur directli exploit tempor second order statist data first stage achiev stronger nois reduct altern approach would exploit tempor structur data indirectli use tempor sourc model result singl stage algorithm oper sourc separ nois reduct coupl approach taken present paper follow present new approach independ factor problem base dynam probabilist network order captur tempor statist properti observ data describ sourc hidden markov model hmm result dynam model describ multivari time seri term sever independ sourc tempor characterist section present em learn algorithm zero nois case section present algorithm case isotrop nois case non isotrop nois turn comput intract section provid approxim em algorithm base variat approach notat multivari gaussian densiti denot ry exp zt wo rk point time block denot xt ith coordin xt xl function denot averag ensembl block zero nois mog sourc model employ ifa advantag capabl approxim arbitrari densiti ii learn effici data em gaussian correspond hidden state sourc label assum time sourc state sl signal xl gener order sampl gaussian distribut mean ui varianc captur tempor statist data endow sourc tempor structur introduc transit matrix state focus attia time block result probabilist model defin rs st st yl det xl xi joint densiti sourc time point last equat follow xt gyt unmix matrix usual nois free scenario see section assum mix matrix squar invert graphic model observ densiti defin parametr gij es rs model describ sourc first order hmm reduc time independ model wherea tempor structur describ mean move averag autoregress model hmm advantag sinc model high order tempor statist facilit em learn omit deriv maxim respect gij result increment updat rule xt xttg es es natur gradient use appropri chosen learn rate sourc paramet obtain updat rule et es et et use standard hmm initi probabl updat via notat posterior densiti comput step sourc given term data via gijyt use forward backward procedur algorithm may use sever possibl gener em scheme effici one given follow two phase procedur freez sourc paramet learn separ matrix use ii freez learn sourc paramet use go back repeat notic rule similar natur gradient version bell sejnowski ica rule fact two coincid time independ sourc ck xi ogp xi oxi also recogn baum welch method henc phase algorithm separ sourc use gener ica rule wherea phase ii learn hmm sourc remark often one would like model given variabl time seri term smaller number factor framework nois free model yt hxt achiev appli algorithm largest princip compon data notic data inde gener factor remain princip compon would vanish equival one may appli algorithm data directli use non squar unmix matrix result figur demonstr perform method mixtur speech signal pass non linear function modifi distribut mixtur insepar ica sourc model use latter fit actual sourc densiti see discuss also appli dynam network mixtur speech signal whose distribut dynam independ factor analysi hmm ica ica figur left two four sourc distribut middl output em algorithm nearli independ right output ica correl made gaussian appropri non linear transform sinc tempor inform crucial separ case see mixtur insepar ica ifa howev algorithm accomplish separ success isotrop nois turn case non zero nois ut assum nois white zero mean gaussian distribut covari matrix gener case comput intract see section reason estep requir comput posterior distribut sourc state zero nois case also sourc signal posterior quit complic structur show assum isotrop nois aij sij well squar invert mix posterior simplifi consider make learn infer tractabl done adapt idea suggest dynam probabilist network start pre process data use linear transform make covari matrix uniti ytyt sphere denot averag point time block follow hsh xtxt diagon covari matrix sourc squar invert impli hth diagon fact sinc unobserv sourc determin within scale factor set varianc sourc uniti obtain thogon properti shown sourc posterior factor product individu sourc xx yx hi ixi lyl yl vtp vop mean varianc time well quantiti depend data yt state particular hjiyt vs av ys use express omit transit probabl henc posterior distribut effect defin new hmm sourc yt depend emiss transit probabl deriv learn rule first comput condit mean sourc signal time given data done recurs use forward backward procedur obtain czc eytit attia fraction form result impos orthogon constraint hth use lagrang multipli comput via diagon procedur sourc paramet comput use learn rule omit similar nois free rule easi deriv learn rule nois level well fact ordinari fa rule would suffic point algorithm deriv case perfectli well defin though sub optimah see non isotrop nois gener case non isotrop nois non squar mix comput intract exact step requir sum possibl sourc configur st time tl intract tl problem stem fact sourc independ sourc condit data vector correl result larg number hidden configur problem aris nois free case avoid case isotrop nois squar mix use orthogon properti case exact posterior sourc factor em algorithm deriv base variat approach approach introduc context sigmoid belief network constitut gener framework ml learn intract probabilist network use hmm context idea use approxim tractabl posterior place lower bound likelihood optim paramet maxim bound start point deriv bound likelihood neal hinton ill formul em algorithm ogp eqlogp yt ixt eqlogp eqlogq eq denot averag respect arbitrari posterior densiti hidden variabl given observ data xl yl exact em shown obtain maxim bound respect posterior correspond step model paramet mstep howev result true intract posterior contrast variat em choos differ true posterior facilit tractabl step step use sb li htq parametr sl st xt xt pt thu variat transit probabl describ multipli paramet subject normal constraint origin one sourc signal time jointli gaussian mean pt covari mean covari transit probabl time datadepend pt etc parametr scheme motiv becom form posterior notic quantiti vs variat paramet relat scheme use differ context sinc paramet adapt independ model paramet non isotrop algorithm expect give superior result compar isotrop one dynam independ factor analysi lo mix reconstruct snr db snr db right qualiti sourc figur left qualiti model paramet estim reconstruct see text cours true posterior xt correl tempor among st latter factor best approxim variat paramet optim maxim bound equival minim kl distanc true posterior requir lead fix point equat pt hta bt hta lyt bt hta bt exp logv es ij es fit ensur factor maliz hmm quantiti comput forward backward procedur use variat transit probabl variat paramet determin solv eq iter block yl practic found less iter usual requir converg step updat rule given mix paramet sourc paramet comput use variat transit probabl notic learn rule sourc paramet baum welch form spite correl condit sourc variat approach correl hidden manifest fact fix point equat coupl paramet across time point sinc depend ks sourc sourc reconstruct xt yl observ map sourc estim given pt yl depend result algorithm demonstr sourc separ task figur use speech signal transform non linear arbitrari one point densiti mix random matrix differ signalto nois snr level use error estim left solid line quantifi size non diagon element ho rel ytpt ptpt et ytyt ytpt attia diagon result obtain ifa use tempor inform plot refer dot line mean squar error reconstruct sourc right solid line correspond ifa result right dash line also shown estim reconstruct error algorithm consist smaller ifa reflect advantag exploit tempor structur data addit experi differ number sourc sensor gave similar result notic algorithm unlik previou two allow also consid situat number sensor smaller number sourc separ qualiti good although expect less opposit case conclus import issu address model select appli algorithm arbitrari dataset number factor hmm state factor determin wherea could done principl use cross valid requir comput effort would fairli larg howev recent paper develop new framework bayesian model select well model averag probabilist network framework term variat bay propos em like algorithm approxim full posterior distribut hidden variabl also paramet model structur well predict quantiti analyt manner current appli algorithm present good preliminari result one field approach may find import applic speech technolog suggest build econom signal model base combin independ low dimension hmm rather fit singl complex hmm may also contribut toward improv recognit perform noisi multispeak reverber condit character real world auditori scene refer attia independ factor analysi neut comp bell sejnowski inform maxim approach blind separ blind aleconvolut neut comp amari cichocki yang new learn algorithm blind signal separ adv neut info roc sy ed touretzki et al mit press cambridg pearlmutt parra maximum likelihood blind sourc separ context sensit gener ica adv neut info proc sy ed mozer et al mit press cambridg hyv inen oja fast fix point algorithm independ compon analysi neut comp attia schreiner blind sourc separ aleconvolut dynam compon analysi algorithm neut comp rabin juang fundament speech recognit prentic hall englewood cliff nj lee sompolinski unpublish lee person commun saul jaakkola jordan mean field theori sigmoid belief network art int re ghahramani jordan factori hidden markov model mach learn neal hinton view em algorithm justifi increment spars variant learn graphic model ed jordan kluwer academ press attia variat bayesian framework graphic model adv neut info proc sy ed leen et al mit press cambridg
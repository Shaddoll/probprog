abstract stochast meta descent smd new techniqu onlin adapt local learn rate arbitrari twice differenti system like matrix momentum use full second order inform retain comput complex exploit effici comput hessian vector product appli smd independ compon analysi employ result algorithm blind separ time vari mixtur match individu learn rate rate chang sourc signal mixtur coeffici techniqu capabl simultan track sourc move differ priori unknown speed introduct independ compon analysi ica method typic run batch mode order keep stochast empir gradient low often combin global learn rate anneal scheme negoti tradeoff fast converg good asymptot perform time vari mixtur must replac learn rate adapt scheme adapt singl global learn rate howev facilit track sourc whose mix coeffici chang compar rate resp switch time case sourc move much faster other switch differ time individu weight unmix matrix must adapt differ rate order achiev good perform appli stochast meta descent smd new onlin adapt method local learn rate extend bell sejnowski ica algorithm natur gradient kurtosi estim modif result algorithm capabl separ track time vari mixtur sourc whose unknown mix coeffici chang differ rate schraudolph giannakopoulo smd algorithm given sequenc data point minim expect valu twice differenti loss function respect paramet stochast gradient descent denot compon wise multipl local learn rate fi best adapt exponenti gradient descent cover wide dynam rang stay strictli posit gt lntyt ln olnti tyt fft exp yt nil global meta learn rate approach rest assumpt element fi affect correspond element consider variat form basi local rate adapt method found literatur order avoid expens exponenti weight updat typic use linear valid small lu give fft max yt constrain multipli least typic safeguard unreason small neg valu meta level gradient descent stabl must case chosen multipli fido stray far uniti condit find linear approxim quit suffici definit gradient trace accur measur effect chang local learn rate correspond weight tempt consid immedi effect chang fit declar independ fit one quickli arriv nyt yt fft howev common approach fail take account increment natur gradient descent chang fi affect current updat also futur one author account set exponenti averag past gradient found empir method alineida et al inde improv approach averag serv reduc stochast product impli averag remain one immedi singl step effect contrast sutton model long term effect fi futur weight updat linear system carri relev partial forward time done real time recurr learn result iter updat rule extend nonlinear system defin onlin ica local rate adapt exponenti averag effect past chang ff current weight ai olnp forget factor free paramet algorithm insert olni xt yt gt give ht denot instantan hessian time approxim assum vi ofit ofit signifi certain depend appropri choic meta learn rate note effici algorithm calcul htyt without ever comput store matrix ht shall elabor techniqu case independ compon analysi meta level condit gradient descent ff meta level may cours suffer ill condit like descent main level meta descent fact squar condit number defin previou gradient exponenti averag past gradient special measur improv condit thu requir make meta descent work non trivial system mani research use sign function radic normal updat unfortun nonlinear preserv zero mean properti character stochast gradient equilibrium particular translat skew equilibrium distribut non zero mean chang caus converg non optim step size render method unsuit onlin learn notabl almeida et al avoid pitfal use run estim gradient stochast varianc meta normal addit model long term effect chang local learn rate iter gradient trace serv highli effect condition meta descent fixpoint given aht diag ff modifi newton step typic valu close scale invers gradient consequ expect product yt well condit quantiti experi feedforward multi layer perceptron confirm smd requir explicit meta level normal converg faster altern method applic ica appli smd techniqu independ compon analysi use bell sejnowski algorithm base method goal find unmix schraudolph giannakopoulo matrix wt scale permut provid good linear estim wt independ sourc gt present given mixtur signal tthe mixtur gener linearli accord gt unknown unobserv full rank matrix includ well known natur gradient kurtosi estim modif basic algorithm well matrix pt local learn rate result onlin updat weight matrix wt wt wt pt dr gradient dt given fw ot owt fit tanh wt sign compon tanh term depend current kurtosi estim follow pearlmutt defin differenti oper wt og wt rvt describ effect perturb weight direct vt use effici calcul hessian vector product ht vt vec ht vec vt dt vec oper concaten column matrix singl column vector sinc linear oper wt vt zvt zvt wt vt tg tanh fft diag tanh vt forth cf start appli oper obtain ht vt conjunct matrix version learn rate updat pt pt max ao dt vt gradient trace vt vt pt dt kht vt constitut smd ica algorithm onlin ic qth local rate adapt experi algorithm test artifici problem sourc follow ellipt trajectori accord abas sin co abas normal distribut mix matrix well whose column repres axe ellips sourc travel veloc normal distribut around mean one revolut everi data sampl sourc supergaussian ica smd algorithm implement onlin access data includ line whiten whenev condit number estim whiten matrix exceed larg threshold set updat disabl prevent algorithm diverg paramet set result separ sourc without ambigu discard figur show perform index lower better zero ideal case along condit number mix matrix show algorithm robust temporari confus separ ordin repres data sampl divid mini batch effici figur show match actual mix column estim subspac span ellipt trajectori singular occur halfway damag perform global algorithm remain stabl long degener input handl correctli conclus smd ica found separ solut find possibl simultan track ten sourc move independ differ priori unknown error index cond figur global view qualiti separ schraudolph giannakopoulo estim error figur project column mix matrix arrow link exact point estim trajectori proce lower right upper left speed continu track extend period necessari handl momentari singular onlin estim number sourc heurist solut smd adapt local learn rate facilit continu onlin use ica rapidli chang environ acknowledg work support swiss nation scienc foundat grant number refer karhunen pajunen blind sourc separ track use nonlinear pca criterion least squar approach proc ieee int conf neural network houston texa pp murata mfiller zieh amari adapt line learn chang environ advanc neural inform process system mozer jordan petsch edso vol pp mit press cambridg schraudolph local gain adapt stochast gradient descent proceed th intern confer artifici neural network edinburgh scotland pp iee london tp tp pub nic smd ps gz schraudolph onlin learn adapt local step size neural net wirn vietri proceed th italian workshop neural net marinaro tagliaferri ed vietri sul mare salerno itali perspect neural comput pp springer verlag berlin onlin ica local rate adapt bell sejnowski inform maxim approach blind separ blind deconvolut neural comput amari cichocki yang new learn algorithm blind signal separ advanc neural inform process system touretzki mozer hasselmo ed vol pp mit press cambridg girolami fyfe generalis independ compon analysi unsupervis learn emerg bussgang properti proc ieee int conf neural network houston texa pp kivinen warmuth exponenti gradient versu gradient descent linear predictor tech rep ucsc crl univers california santa cruz june kivinen warmuth addit versu exponenti gradient updat linear predict proc th annual acm symposium theori comput new york ny may pp associ comput machineri schraudolph fast compact approxim exponenti function neural comput jacob increas rate converg learn rate adapt neural network tollenaer supersab fast adapt back propag good scale properti neural network silva almeida speed back propag advanc neural comput eckmil ed amsterdam pp elsevi riedmil braun direct adapt method faster backpropag learn rprop algorithm proc intern confer neural network san francisco ca pp ieee new york almeida langloi amar plakhov paramet adapt stochast optim line learn neural network saad ed public newton institut chapter cambridg univers press ftp pub lba paper adstep gz harmon baird iii multi player residu advantag learn gener function approxim tech rep wl tr wright laboratori wl aacf avion circl wright patterson air forc base oh http leemon com paper sim tech sim tech ps gz sutton adapt bia gradient descent increment version delta bar delta proc loth nation confer artifici intellig pp mit press cambridg ftp ftp cs amass edu pub pub sutt sutt ps gz sutton gain adapt beat least squar proc th yale workshop adapt learn system pp ftp ftp cs amass edu pub pub sutt sutt ps gz william zipser learn algorithm continu run fulli recurr neural network neural comput pearlmutt fast exact multipl hessian neural comput karhunen oja wang vigario joutsensalo class neural network independ compon analysi ieee tran neural network
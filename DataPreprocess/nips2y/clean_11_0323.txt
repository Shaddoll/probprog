abstract ws log log dqh ws log log dqh upper bound vc dimens set neural network unit piecewis polynomi activ function depth network number hidden unit number adjust paramet maximum number polynomi segment activ function maximum degre polynomi also fl wslog dqh lower bound vc dimens network set tight case constant special case vc dimens ws log introduct spite import unabl obtain vc dimens valu practic type network fairli tight upper lower bound obtain linear threshold element network element perform threshold function weight sum input roughli lower bound network log upper bound log number hidden element number connect weight one hidden layer case nh input dimens network mani applic though sigmoid function specif typic sigmoid function exp piecewis linear function economi calcul use instead threshold function mainli differenti function need perform backpropag learn algorithm unfortun explicit bound obtain far vc dimens sigmoid network exhibit larg gap wlog bound depth sakurai wh unbound depth hard improv piecewis linear case maass obtain result vg dimens log number linear piec function recent koiran sontag prove lower bound piecewis polynomi case claim open problem maass pose match lower bound type network solv still someth sinc show case number hidden layer unbound also bound room improv paper improv bound obtain maass koiran sontag consequ show role polynomi play linear function role constant function could appear piecewis polynomi case cannot play polynomi function submiss draft found bartlett maiorov meir haa obtain similar result prior also proceed advantag clarifi role play degre number segment concern bound terminolog notat log stand logarithm base throughout paper depth network length longest path extern input extern output length number unit path likewis assign depth unit network length longest path extern input output unit hidden layer set unit depth depth network therefor depth network hidden layer mani case stand vector compos connect weight network includ threshold valu threshold unit length number unit network exclud input unit denot word number hidden unit plu one sometim number hidden unit function whose rang set call boolean valu function upper bound obtain upper bound vc dimens use region count argument develop goldberg jerrum vc dimens network vc dimens function set fg iwer upper bound nc number connect compon af set follow two theorem conveni refer first theorem lemma follow easili proven theorem let xj real polynomi degre less number connect compon set gi wlfo xl bound length ofw tight bound vc dimens piecewis polynomi network lemn ta log loglog first let us consid polynomi activ function case theorem suppos activ function polynomi degre wslog upper bound vu dimens network depth en bound whlogd precis ws logd log loud upper bound note allow polynomi input function replac maximum degre input function activ function theorem clear ct network function fa polynomi degre theorem lemma piecewis linear case two type bound first one suitabl bound depth case depth second one unbound depth case theorem suppos activ function piecewis polynomi segment polynomi degre ws slogd log dqh ws log log dqh upper bound vu dimens depth network precis ws logd log qh ws log log asymptot upper bound note allow polynomi input function replac maximum degre input function activ function proof two differ way calcul bound first ii enqd os hi number hidden unit th layer oper form new vector concaten two get asymptot upper bound ws log log qh vc dimens secondli ncc jv qh enqad get asymptot upper bound ws logq loud vcdimens combin two bound get result note log dqh introduc elimin unduli larg term emerg lower bound polynomi network theorem let us consid case activ function polynomi degre wslogd lower bound vu dimens network depth bound whlogd precis salatrai log asymptot lower bound degre activ function power two restrict input dimens proof consist sever lemma network construct two part encod decod deliber fix input point decod part fix underli architectur also fix connect weight wherea encod part variabl weight given binari output input point decod could output specifi valu code output valu encod encod first consid decod two real input one real output one two input hold code binari sequenc bx bm hold code binari sequenc cx cm element latter sequenc except cj cj order decod output bj consequ network show two type network one activ function degre two vc dimens activ function degre power two vc dimens log use conveni two function otherwis undefin otherwis throughout section use simpl logist function follow properti lemma binari sequenc bin exist interv bi pi pi xx next lemma easili proven lemma binari sequenc cx cm except cj exist xo ci tg xo specif take xo pz invers pj xo pi xo pj xo posit proof clear fact lemma binari sequenc bx bm take tl pi pi pz tl ei pi xo pi bj tlo ei pi xo pi proof bj ei pi xo pi ei pi xo pi pj ei pj bj ei pi xo pi pi xo pj lemma network figur left follow function suppos binari sequenc bx bm integ given present depend bx bm depend bj output decod note use xy realiz multipl unit case degre higher two construct bit complic one use anoth simpl logist function need next lemma lemma take invers tff xo xo tight bound vc dimens piecewis polynomi network figur network architectur consist polynomi order two left order power two right proof clear fact lemma binari sequenc bx bk bk bm take bi tx pi pi moreov take xo xz im pik ik xl lemma ifo pi anyo take te pt ls proof four case depend whether pt uphil downhil whether uphil downhil pt proof done induct first suppos two uphil pt pt pt xe pt te secondli suppos pt uphil downhil pt pt pt xe pt te two case similar proof lemma show differ pi xo suffici small clearli xik xx pi ei xx pi pj pj pi uphil pt use lemma get tzi xo xo pt pj pi note downhil pt use lemma get pi tzi xo tzi xo pt pjk next show encod scheme adopt show case sinc case gener easili obtain theorem network input hidden unit weight sakurai set input valu xx xh set valu yx yh chose satisfi yi xi proof extens util fact monomi obtain choos variabl variabl repetit allow say linearli independ note number monomi thu form mm suppos simplic input main hidden unit hidden unit use multipl unit fact composit two squar unit output suppos sum figur form linearli independ monomi compos variabl xl use multipl unit nomin unit way form linearli independ monomi compos variabl let us denot monomi ux ua vl form subnetwork calcul wi jui vj use multipl unit clearli calcul result weight sum monomi describ weight wi sinc linear combin linearli independ term choos appropri set valu xx xh xx assign valu yx ya set weight yi xl proof theorem whole network consist decod encod input point cartesian product xx defin lemma number bit encod mean point shatter let number hidden layer decod number unit use decod degre case decod bit degre case decod bit number unit use encod less though constraint domin depth network domin number unit network roughli log satisfi let us chose log better chois result use unit layer shatter log point asymptot use unit layer shatter log point piecewis polynomi case theorem let us consid set network unit linear input function piecewis polynomi polynomi segment activ function wslog dqh lower bound vc dimens depth network maximum degre activ function precis log log log asymptot lower bound scarciti space give outlin proof proof base polynomi network use unit activ function polynomi segment degre place pk unit decod give abil decod log dqh bit one layer log dqh bit total sh unit total design total number unit tight bound vc dimens piecewis polynomi network number decod bit repres log dqh follow simplic suppos dqh power let pk composit usual pk pi let pl ga pl ga otherwis way polynomi segment unit polynomi case replac array og og iog unit defin follow oi ga gq array two unit one loga loge otherwis plog og otherwis ii pl ga ge array unit one function pl ga composit note three linear segment one linear other constant sum possibl combin equal function lemma similar one polynomi case follow refer anthoni classif polynomi surfac neuro colt technic report seri nc tr goldberg jetrum bound vapnik chervonenki dimens concept class parameter real number proc sixth annual acm confer comput learn theori karpinski macintyr polynomi bound vc dimens sigmoid neural network proc th cm symposium theori comput koiran sontag neural network quadrat vc dimens journ comp syst sci maass bound comput power learn complex analog neural net proc th annual symposium theori comput maass neural net superlinear vc dimens neural comput milnor betti number real varieti proc am sakurai tighter bound vc dimens three layer network proc wcnn iii sakurai vc dimens depth four threshold circuit complex boolean valu function proc alt lnai refin version theoret comput scienc sakurai vc dimens neural network larg number hidden layer proc nolta ieic warren lower bound approxim nonlinear manifold tran am
abstract mani belief network propos compos binari unit howev task object speech recognit produc real valu data binari network model usual inadequ independ compon analysi ica learn model real data descript power model severli limit begin describ independ factor analysi ifa techniqu overcom limit ica creat multilay network cascad singlelay ifa model level ifa network extract realvalu latent variabl non linear function input data highli adapt function form result hierarch distribut represent data wherea exact maximum likelihood learn network intract deriv algorithm maxim lower bound likelihood base variat approach introduct intrigu hypothesi brain repres incom sensori inform hold construct hierarch probabilist model observ data model paramet learn unsupervis manner maxim likelihood data gener model multilay belief network realiz model mani belief network propos compos binari unit hidden unit network repres latent variabl explain differ featur data whose relat current address gatsbi comput neurosci unit univers colleg london queen squar london wcin ar attia data highli non linear howev task object speech recognit produc real valu data model provid binari network often inadequ independ compon analysi ica learn generar model real data extract real valu latent variabl mutual statist independ unfortun model restrict singl layer latent variabl simpl linear function data henc underli degre freedom non linear cannot extract ica addit requir equal number hidden observ variabl assumpt noiseless data render ica model inappropri paper begin introduc independ factor analysi ifa techniqu ifa extens ica allow differ number latent observ variabl handl noisi data paper proce creat multilay network cascad singl layer ifa model result generar model produc hierarch distribut represent input data latent variabl extract level non linear function data highli adapt function form wherea exact maximum likelihood ml learn network intract due difficulti comput posterior densiti hidden layer present algorithm maxim lower bound likelihood algorithm base gener variat approach develop ifa network independ compon independ factor analysi although concept ica origin field signal process actual densiti estim problem given observ data vector task explain term vector unobserv sourc mutual statist independ relat two assum linear hx mix matrix nois vector usual assum zero mean gaussian covari matrix context blind sourc separ sourc signal recov mix noisi signal knowledg sourc densiti xi henc term blind densiti estim approach one regard probabilist gener model observ mix matrix nois covari sourc densiti serv model paramet principl paramet learn ml follow infer sourc via map estim gaussian sourc factor analysi model em algorithm exist map estim linear problem becom interest difficult non gaussian sourc ica algorithm focu squar noiseless hx mix fix xi use prior knowledg see case noisi mix fix laplacian sourc prior learn occur via gradient ascent maxim likelihood sourc densiti paramet also adapt way result gradient ascent learn rather slow state affair present problem ica algorithm sinc abil learn arbitrari sourc densiti known advanc crucial use inaccur xi often lead bad estim fail separ problem recent solv introduc ifa techniqu ifa employ semi parametr model sourc densiti allow learn well mix matrix use expect maxim em specif xi describ mixtur gaussian mog mixtur hierarch belief network compon label hi mean varianc xi si xi lui mix proport parametr use softmax form si exp ai exp ai beyond noiseless ica em algorithm noisi case also deriv use mog descript algorithm learn probabilist model observ data parametr ai lui graphic represent model provid fig set yj vj hierarch independ factor analysi follow develop multilay gener ifa cascad duplic gener model introduc layer compos two sublay sourc sublay consist unit output sublay consist yy two linearli relat via gaussian nois vector covari nth layer sourc describ mog densiti model paramet analog ifa sourc import step determin layer depend previou layer choos introduc depend ith sourc layer ith output layer notic match ln requir depend implement make mean mixtur proport gaussian compos depend specif make replac result joint densiti yn li li bti sy layer condit layer sn yi xi yi paramet layer exp yi exp bi yi pi syi ti full model joint densiti given product set graphic represent layer hierarch ifa network given fig unit hidden except gain insight network examin relat nth layer sourc lth layer output yi relat probabilist determin condit densiti yi si yi yi notic mog densiti depend mean given yi lin bti syi throughout paper exp xt howev mani sourc step becom intract sinc number li ni sourc state configur depend exponenti case treat use variat approxim attia figur layer hierarch ica gener model non linear function due softmax form adjust paramet function assum wide rang form suppos state set signific small continu rang valu differ rang associ differ rang domin linear term henc desir fi produc place orient line segment appropri point axi smoothli join togeth use algorithm optim form fi learn data therefor model describ data potenti highli complex function top layer sourc produc repeat applic linear mix follow non linear nois allow stage learn infer variat em need sum exponenti larg number sourc state configur integr softmax function make exact learn intract network thu approxim must made follow develop variat approach spirit hierarch ifa begin follow approach em bound loglikelihood logp elogp xn elogp yi log denot averag hidden layer use arbitrari posterior yx exact em iter true posterior parametr previou iter variat em chosen form make learn tractabl parametr separ set paramet optim bring close true posterior possibl hierarch ifa belief network step use variat posterior factor across layer within layer form sn xn yn vn vin sl zn pn en xn yn vj variat paramet vi depend data full layer posterior simpli product henc given data nth layer sourc output jointli gaussian wherea state independ lower even variat posterior term elogp bound cannot calcul analyt sinc involv integr softmax function instead calcul yet lower bound term let bi sy drop unit layer indic logp log ec borrow idea multipli divid logarithm sign use jensen inequ get elogp sec loge ec result bound calcul close form elogp si ef yi log py eyi eyi subscript omit also defin similarli yy exi subblock sinc hold arbitrari latter treat addit variat paramet optim tighten bound optim variat paramet equat gradient lower bound zero obtain hta hta hta hta vi vi ij bi vi pi vi paramet within belong layer contain correspond deriv sum state posterior exp easi introduc structur allow mean depend thu make approxim covari ei depend accur complex maintain tractabl altern approach handl elogp yi approxim requir integr maximum valu integrand possibl includ gaussian correct result approxim simpler howev longer guarante bound log likelihood ttia unit subscript omit exnx ii jn set simpl modif equat requir layer optim obtain solv fix point equat iter data vector keep gener paramet fix notic equat coupl layer layer addit paramet adjust use gradient ascent learn complet infer problem solv sinc map estim hidden unit valu given data readili avail step term variat paramet obtain step new gener paramet given pypx eyx pxp pypi yy vs vs px vs px py omit subscript slightli modifi layer batch mode averag data impli cancel final softmax paramet adapt gradient ascent bound discuss hierarch ifa network present constitut quit gener framework learn infer use real valu probabilist model strongli non linear highli adapt notic network includ continu yi binari unit thu extract type latent variabl may repres class label classif particular uppermost unit task model propos view special case prescrib determinist function rectifi previou output yy ifa network determinist still adapt depend obtain case assum set varianc note sourc valu thu correspond discret latent variabl learn infer algorithm present base variat approach unlik variat approxim belief network use complet factor approxim structur hierarch ifa network facilit use variat posterior allow correl among hidden unit occupi layer thu provid accur descript true posterior would interest compar perform variat algorithm belief propag algorithm adapt dens connect ifa network would also approxim markov chain mont carlo method includ recent slice sampl procedur use would becom slow network size increas possibl consid gener non linear network along line hierarch ifa notic given previou layer output yn mean output next layer yi rrn ijfj yj see linear mix preced non linear function oper output compon separ howev elimin sourc xy replac individu sourc hierarch ifa belief network state sy collect state allow linear transform depend arriv follow model yn exp asn tyn ep sli gener non linear yn final block fig altern block sn describ connect vertic paper horizont creat layer multipl block direct acycl graph architectur variat em algorithm extend accordingli acknowledg thank de sa help discuss support offic naval research nidcd sloan foundat refer bell sejnowski inform maxim approach blind separ blind deconvolut neural comput cardoso infomax maximum likelihood sourc separ ieee signal process letter pearlmutt parra maximum likelihood blind sourc separ context sensit gener ica advanc neural inform process system ed mozer et al mit press attia schreiner blind sourc separ deconvolut dynam compon analysi algorithm neural comput lewicki sejnowski learn nonlinear overcomplet represent effici code advanc neural inform process system ed jordan et al mit press attia independ factor analysi neural comput press neal hinton view em algorithm justifi increment spars variant learn graphic model ed jordan kluwer academ press saul jaakkola jordan mean field theori sigmoid belief network journal artifici intellig research frey continu sigmoid belief network train use slice sampl advanc neural inform process system ed mozer et al mit press frey hinton variat learn non linear gaussian belief network neural comput press ghahramani hinton hierarch non linear factor analysi topograph map advanc neural inform process system ed jordan et al mit press pearl probabilist reason intellig system morgan kaufmann san mateo ca